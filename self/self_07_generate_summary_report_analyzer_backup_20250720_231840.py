import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import Font
from datetime import datetime
import gc

from self_00_02_utils import log_info
from self_00_04_excel_processor import (
    add_dataframe_to_excel_with_grouped_headers,
    format_excel_sheet
)
from self_08_create_http_lifecycle_visualization import create_http_lifecycle_visualization
from self_06_performance_stability_analyzer import (
    analyze_service_stability,
    apply_highlighting,
    add_summary_sheet,
)

# HTTPç”Ÿå‘½å‘¨æœŸå‚æ•°æ˜ å°„è¡¨ï¼ˆåŸºäºnginx_lifecycle_analysis.txtï¼‰
HTTP_LIFECYCLE_METRICS = {
    # åŸºç¡€æ—¶é—´å‚æ•°ï¼ˆ4ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼‰
    'request_time': 'è¯·æ±‚æ€»æ—¶é•¿',
    'upstream_response_time': 'åç«¯å“åº”æ—¶é•¿',
    'upstream_header_time': 'åç«¯å¤„ç†æ—¶é•¿',
    'upstream_connect_time': 'åç«¯è¿æ¥æ—¶é•¿',

    # æ ¸å¿ƒé˜¶æ®µå‚æ•°ï¼ˆ4ä¸ªå…³é”®é˜¶æ®µï¼‰
    'backend_connect_phase': 'åç«¯è¿æ¥é˜¶æ®µ',
    'backend_process_phase': 'åç«¯å¤„ç†é˜¶æ®µ',
    'backend_transfer_phase': 'åç«¯ä¼ è¾“é˜¶æ®µ',
    'nginx_transfer_phase': 'Nginxä¼ è¾“é˜¶æ®µ',

    # ç»„åˆåˆ†æå‚æ•°
    'backend_total_phase': 'åç«¯æ€»é˜¶æ®µ',
    'network_phase': 'ç½‘ç»œä¼ è¾“é˜¶æ®µ',
    'processing_phase': 'çº¯å¤„ç†é˜¶æ®µ',
    'transfer_phase': 'çº¯ä¼ è¾“é˜¶æ®µ',

    # æ€§èƒ½æ¯”ç‡å‚æ•°ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰
    'backend_efficiency': 'åç«¯å¤„ç†æ•ˆç‡(%)',
    'network_overhead': 'ç½‘ç»œå¼€é”€å æ¯”(%)',
    'transfer_ratio': 'ä¼ è¾“æ—¶é—´å æ¯”(%)',

    # æ•°æ®ä¼ è¾“ç›¸å…³ï¼ˆKBå•ä½ï¼‰
    'response_body_size_kb': 'å“åº”ä½“å¤§å°(KB)',
    'total_bytes_sent_kb': 'æ€»ä¼ è¾“é‡(KB)',

    # ä¼ è¾“é€Ÿåº¦ç›¸å…³
    'response_transfer_speed': 'å“åº”ä¼ è¾“é€Ÿåº¦(KB/s)',
    'total_transfer_speed': 'æ€»ä¼ è¾“é€Ÿåº¦(KB/s)',
    'nginx_transfer_speed': 'Nginxä¼ è¾“é€Ÿåº¦(KB/s)',

    # æ–°å¢æ€§èƒ½æŒ‡æ ‡
    'connection_cost_ratio': 'è¿æ¥æˆæœ¬æ¯”ä¾‹(%)',
    'processing_efficiency_index': 'å¤„ç†æ•ˆç‡æŒ‡æ•°'
}


def generate_summary_report(outputs, output_path):
    """ç”Ÿæˆä¼˜åŒ–åçš„ç»¼åˆæŠ¥å‘Š"""
    log_info("å¼€å§‹ç”Ÿæˆç»¼åˆæŠ¥å‘Š...", level="INFO")
    wb = Workbook()
    ws = wb['Sheet']
    ws.title = 'ç»¼åˆæŠ¥å‘Š'

    # æŠ¥å‘Šå¤´éƒ¨
    row = _add_report_header(ws)

    # é«˜çº§æ‘˜è¦ï¼ˆåŒ…å«HTTPç”Ÿå‘½å‘¨æœŸå¥åº·è¯„åˆ†ï¼‰
    row = _add_executive_summary_with_lifecycle(ws, outputs, row)

    # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    row = _add_basic_statistics(ws, outputs, row)

    # è¯·æ±‚å¤´åˆ†æ
    row = _add_header_analysis(ws, outputs, row)

    # è¯·æ±‚å¤´æ€§èƒ½å…³è”åˆ†æ
    row = _add_header_performance_analysis(ws, outputs, row)

    # HTTPç”Ÿå‘½å‘¨æœŸæ€§èƒ½åˆ†æ
    row = _add_http_lifecycle_analysis(ws, outputs, row)

    # èµ„æºå ç”¨åˆ†æï¼ˆä¼˜åŒ–KBå•ä½æ˜¾ç¤ºï¼‰
    row = _add_optimized_resource_analysis(ws, outputs, row)

    # æ—¶é—´è¶‹åŠ¿åˆ†æ
    row = _add_trend_analysis_section(ws, outputs, row)

    # è¿æ¥æ•ˆç‡åˆ†æ
    row = _add_connection_efficiency_analysis(ws, outputs, row)

    # æ€§èƒ½ç“¶é¢ˆè¯Šæ–­
    row = _add_performance_bottleneck_diagnosis(ws, outputs, row)

    # ä¼˜åŒ–å»ºè®®
    row = _add_optimization_suggestions_enhanced(ws, row)

    # è®¾ç½®åˆ—å®½å¹¶ä¿å­˜
    ws.column_dimensions['A'].width = 80
    create_http_lifecycle_visualization(wb)

    try:
        wb.save(output_path)
        log_info(f"ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}", level="INFO")
    except Exception as e:
        log_info(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}", level="ERROR")

    _cleanup_resources(wb, ws)


def _add_report_header(ws):
    """æ·»åŠ æŠ¥å‘Šå¤´éƒ¨"""
    ws.cell(row=1, column=1, value="Nginxæ—¥å¿—åˆ†æç»¼åˆæŠ¥å‘Š").font = Font(size=14, bold=True)
    ws.cell(row=3, column=1, value=f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    return 5


def _add_executive_summary_with_lifecycle(ws, outputs, start_row):
    """æ·»åŠ åŒ…å«HTTPç”Ÿå‘½å‘¨æœŸå¥åº·è¯„åˆ†çš„é«˜çº§æ‘˜è¦"""
    row = start_row
    ws.cell(row=row, column=1, value="ç³»ç»Ÿå¥åº·è¯„åˆ†").font = Font(size=12, bold=True)
    row += 1

    ws.cell(row=row, column=1, value="=" * 60)
    row += 1

    # è®¡ç®—ç»¼åˆå¥åº·è¯„åˆ†
    health_score = 100
    health_issues = []

    # é”™è¯¯ç‡è¯„ä¼°
    error_rate = _calculate_error_rate(outputs.get('status_stats'))
    if error_rate > 1.0:
        health_score -= min(30, error_rate * 10)
        health_issues.append(f"âš ï¸ 5xxé”™è¯¯ç‡: {error_rate:.2f}% (å»ºè®®<1%)")

    # HTTPç”Ÿå‘½å‘¨æœŸæ€§èƒ½è¯„ä¼°
    lifecycle_score = _evaluate_lifecycle_performance(outputs)
    health_score = min(health_score, lifecycle_score)

    # è¿æ¥æ•ˆç‡è¯„ä¼°
    conn_efficiency = _evaluate_connection_efficiency(outputs)
    if conn_efficiency < 70:
        health_score -= (70 - conn_efficiency) / 2
        health_issues.append(f"âš ï¸ è¿æ¥å¤ç”¨æ•ˆç‡ä½: {conn_efficiency:.1f}åˆ† (å»ºè®®>70åˆ†)")

    # æ˜¾ç¤ºå¥åº·é—®é¢˜
    for issue in health_issues:
        ws.cell(row=row, column=1, value=issue)
        row += 1

    # æ˜¾ç¤ºç»¼åˆè¯„åˆ†
    health_status = _get_health_status(health_score)
    score_cell = ws.cell(row=row, column=1, value=f"ç³»ç»Ÿå¥åº·è¯„åˆ†: {health_score:.1f}/100 - {health_status}")
    score_cell.font = Font(bold=True, size=11,
                           color=_get_score_color(health_score))
    row += 1

    ws.cell(row=row, column=1, value="=" * 60)
    return row + 2


def _add_basic_statistics(ws, outputs, start_row):
    """æ·»åŠ åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
    row = start_row

    # æ€»è¯·æ±‚æ•°
    if 'total_requests' in outputs:
        total_req = _safe_int_convert(outputs['total_requests'])
        ws.cell(row=row, column=1, value=f"æ€»è¯·æ±‚æ•°: {total_req:,}")
        row += 2

    # çŠ¶æ€ç åˆ†å¸ƒ
    row = _add_stats_table(ws, outputs, 'status_stats', "çŠ¶æ€ç åˆ†å¸ƒ:",
                           ['çŠ¶æ€ç ', 'è¯·æ±‚æ•°', 'ç™¾åˆ†æ¯”(%)'], row)

    # æ…¢APIç»Ÿè®¡
    row = _add_stats_table(ws, outputs, 'slow_apis', "å“åº”æ—¶é—´æœ€é•¿çš„API:",
                           ['è¯·æ±‚URI', 'æˆåŠŸè¯·æ±‚æ•°', 'å¹³å‡è¯·æ±‚æ—¶é•¿(ç§’)'], row)

    # æœåŠ¡åˆ†å¸ƒ
    row = _add_stats_table(ws, outputs, 'service_stats', "æœåŠ¡è¯·æ±‚åˆ†å¸ƒ:",
                           ['æœåŠ¡åç§°', 'æˆåŠŸè¯·æ±‚æ•°', 'å æ€»è¯·æ±‚æ¯”ä¾‹(%)'], row)

    return row


def _add_header_analysis(ws, outputs, start_row):
    """æ·»åŠ è¯·æ±‚å¤´åˆ†æ"""
    row = start_row
    ws.cell(row=row, column=1, value="è¯·æ±‚å¤´åˆ†æ:").font = Font(bold=True, size=12)
    row += 1
    
    header_analysis = outputs.get('header_analysis')
    if not header_analysis or not isinstance(header_analysis, dict):
        ws.cell(row=row, column=1, value="- æ— è¯·æ±‚å¤´åˆ†ææ•°æ®")
        return row + 2
    
    # åŸºç¡€ç»Ÿè®¡
    total_processed = header_analysis.get('total_processed', 0)
    unique_user_agents = header_analysis.get('unique_user_agents', 0)
    unique_referers = header_analysis.get('unique_referers', 0)
    
    ws.cell(row=row, column=1, value=f"- æ€»å¤„ç†è®°å½•æ•°: {total_processed:,}")
    row += 1
    ws.cell(row=row, column=1, value=f"- å”¯ä¸€User-Agent: {unique_user_agents:,} ä¸ª")
    row += 1
    ws.cell(row=row, column=1, value=f"- å”¯ä¸€Referer: {unique_referers:,} ä¸ª")
    row += 1
    
    # TOPæµè§ˆå™¨
    top_browsers = header_analysis.get('top_browsers', {})
    if top_browsers:
        ws.cell(row=row, column=1, value="TOPæµè§ˆå™¨:")
        row += 1
        for browser, count in list(top_browsers.items())[:5]:
            ws.cell(row=row, column=1, value=f"  - {browser}: {count:,} æ¬¡")
            row += 1
    
    # TOPæ¥æºåŸŸå
    top_domains = header_analysis.get('top_domains', {})
    if top_domains:
        ws.cell(row=row, column=1, value="TOPæ¥æºåŸŸå:")
        row += 1
        for domain, count in list(top_domains.items())[:5]:
            ws.cell(row=row, column=1, value=f"  - {domain}: {count:,} æ¬¡")
            row += 1
    
    return row + 1


def _add_header_performance_analysis(ws, outputs, start_row):
    """æ·»åŠ è¯·æ±‚å¤´æ€§èƒ½å…³è”åˆ†æ"""
    row = start_row
    ws.cell(row=row, column=1, value="è¯·æ±‚å¤´æ€§èƒ½å…³è”åˆ†æ:").font = Font(bold=True, size=12)
    row += 1
    
    header_performance = outputs.get('header_performance_analysis')
    if not header_performance or not isinstance(header_performance, dict):
        ws.cell(row=row, column=1, value="- æ— è¯·æ±‚å¤´æ€§èƒ½å…³è”æ•°æ®")
        return row + 2
    
    # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
    slow_rate_overall = header_performance.get('slow_rate_overall', 0)
    total_slow_requests = header_performance.get('total_slow_requests', 0)
    slow_threshold = header_performance.get('slow_threshold', 3)
    
    ws.cell(row=row, column=1, value=f"- æ•´ä½“æ…¢è¯·æ±‚ç‡: {slow_rate_overall}% (é˜ˆå€¼: {slow_threshold}ç§’)")
    row += 1
    ws.cell(row=row, column=1, value=f"- æ…¢è¯·æ±‚æ€»æ•°: {total_slow_requests:,}")
    row += 1
    
    # æ€§èƒ½æœ€å·®çš„æµè§ˆå™¨
    worst_browsers = header_performance.get('worst_browsers', [])
    if worst_browsers:
        ws.cell(row=row, column=1, value="æ€§èƒ½æœ€å·®æµè§ˆå™¨:")
        row += 1
        for browser_info in worst_browsers[:3]:
            ws.cell(row=row, column=1, value=f"  - {browser_info['browser']}: æ…¢è¯·æ±‚ç‡ {browser_info['slow_rate']}%, å¹³å‡å“åº”æ—¶é—´ {browser_info['avg_response_time']:.3f}ç§’")
            row += 1
    
    # æ€§èƒ½æœ€å·®çš„è®¾å¤‡ç±»å‹
    worst_devices = header_performance.get('worst_devices', [])
    if worst_devices:
        ws.cell(row=row, column=1, value="æ€§èƒ½æœ€å·®è®¾å¤‡ç±»å‹:")
        row += 1
        for device_info in worst_devices[:3]:
            ws.cell(row=row, column=1, value=f"  - {device_info['device']}: æ…¢è¯·æ±‚ç‡ {device_info['slow_rate']}%, å¹³å‡å“åº”æ—¶é—´ {device_info['avg_response_time']:.3f}ç§’")
            row += 1
    
    # æ€§èƒ½æœ€å·®çš„æ¥æºåŸŸå
    worst_domains = header_performance.get('worst_domains', [])
    if worst_domains:
        ws.cell(row=row, column=1, value="æ€§èƒ½æœ€å·®æ¥æºåŸŸå:")
        row += 1
        for domain_info in worst_domains[:3]:
            ws.cell(row=row, column=1, value=f"  - {domain_info['domain']}: æ…¢è¯·æ±‚ç‡ {domain_info['slow_rate']}%, å¹³å‡å“åº”æ—¶é—´ {domain_info['avg_response_time']:.3f}ç§’")
            row += 1
    
    # ä¼˜åŒ–å»ºè®®
    recommendations = header_performance.get('performance_recommendations', [])
    if recommendations:
        ws.cell(row=row, column=1, value="æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        row += 1
        for recommendation in recommendations[:5]:
            ws.cell(row=row, column=1, value=f"  - {recommendation}")
            row += 1
    
    return row + 1


def _add_http_lifecycle_analysis(ws, outputs, start_row):
    """æ·»åŠ HTTPç”Ÿå‘½å‘¨æœŸæ€§èƒ½åˆ†æ"""
    row = start_row
    ws.cell(row=row, column=1, value="HTTPè¯·æ±‚ç”Ÿå‘½å‘¨æœŸåˆ†æ:").font = Font(bold=True, size=12)
    row += 1

    # æ£€æŸ¥æ˜¯å¦æœ‰ç”Ÿå‘½å‘¨æœŸæ•°æ®
    slowest_requests = outputs.get('slowest_requests')
    if not isinstance(slowest_requests, pd.DataFrame) or slowest_requests.empty:
        ws.cell(row=row, column=1, value="- æ— ç”Ÿå‘½å‘¨æœŸæ•°æ®")
        return row + 2

    # ç”Ÿå‘½å‘¨æœŸå„é˜¶æ®µå¹³å‡æ—¶é•¿åˆ†æ
    lifecycle_metrics = [
        ('backend_connect_phase', 'åç«¯è¿æ¥é˜¶æ®µ'),
        ('backend_process_phase', 'åç«¯å¤„ç†é˜¶æ®µ'),
        ('backend_transfer_phase', 'åç«¯ä¼ è¾“é˜¶æ®µ'),
        ('nginx_transfer_phase', 'Nginxä¼ è¾“é˜¶æ®µ')
    ]

    ws.cell(row=row, column=1, value="å„é˜¶æ®µå¹³å‡æ—¶é•¿åˆ†æ:")
    row += 1

    total_avg = slowest_requests.get('total_request_duration', pd.Series([0.001])).mean()
    if total_avg <= 0:
        total_avg = 0.001

    for metric_key, metric_name in lifecycle_metrics:
        if metric_key in slowest_requests.columns:
            avg_time = slowest_requests[metric_key].mean()
            percentage = (avg_time / total_avg) * 100

            cell_text = f"- {metric_name}: {avg_time:.3f}ç§’ ({percentage:.1f}%)"
            cell = ws.cell(row=row, column=1, value=cell_text)

            # æ ¹æ®é˜ˆå€¼è®¾ç½®é¢œè‰²é¢„è­¦
            if _is_phase_problematic(metric_key, percentage):
                cell.font = Font(color="FF0000")

            row += 1

    # æ•ˆç‡æŒ‡æ ‡åˆ†æ
    row = _add_efficiency_metrics_analysis(ws, slowest_requests, row)

    return row + 1


def _add_efficiency_metrics_analysis(ws, df, start_row):
    """æ·»åŠ æ•ˆç‡æŒ‡æ ‡åˆ†æ"""
    row = start_row
    ws.cell(row=row, column=1, value="æ•ˆç‡æŒ‡æ ‡åˆ†æ:")
    row += 1

    efficiency_metrics = [
        ('backend_efficiency', 'åç«¯å¤„ç†æ•ˆç‡', '%'),
        ('network_overhead', 'ç½‘ç»œå¼€é”€å æ¯”', '%'),
        ('transfer_ratio', 'ä¼ è¾“æ—¶é—´å æ¯”', '%'),
        ('connection_cost_ratio', 'è¿æ¥æˆæœ¬æ¯”ä¾‹', '%'),
        ('processing_efficiency_index', 'å¤„ç†æ•ˆç‡æŒ‡æ•°', '')
    ]

    for metric_key, metric_name, unit in efficiency_metrics:
        if metric_key in df.columns:
            avg_value = df[metric_key].mean()

            if unit == '%':
                cell_text = f"- {metric_name}: {avg_value:.1f}%"
                cell = ws.cell(row=row, column=1, value=cell_text)

                # æ•ˆç‡æŒ‡æ ‡é˜ˆå€¼é¢„è­¦
                if _is_efficiency_problematic(metric_key, avg_value):
                    cell.font = Font(color="FF0000")
            else:
                cell_text = f"- {metric_name}: {avg_value:.2f}"
                ws.cell(row=row, column=1, value=cell_text)

            row += 1

    return row


def _add_optimized_resource_analysis(ws, outputs, start_row):
    """æ·»åŠ ä¼˜åŒ–åçš„èµ„æºå ç”¨åˆ†æï¼ˆKBå•ä½ç»Ÿä¸€ï¼‰"""
    row = start_row
    ws.cell(row=row, column=1, value="èµ„æºå ç”¨åˆ†æ:").font = Font(bold=True, size=12)
    row += 1

    # å¸¦å®½æ¶ˆè€—åˆ†æï¼ˆKBç»Ÿä¸€å•ä½ï¼‰
    bandwidth_df = outputs.get('resource_usage_bandwidth')
    if isinstance(bandwidth_df, pd.DataFrame) and not bandwidth_df.empty:
        row = _analyze_bandwidth_usage_kb(ws, bandwidth_df, row)

    # è¯·æ±‚é¢‘ç‡åˆ†æ
    freq_df = outputs.get('service_request_frequency')
    if isinstance(freq_df, pd.DataFrame) and not freq_df.empty:
        row = _analyze_request_frequency(ws, freq_df, row)

    return row


def _analyze_bandwidth_usage_kb(ws, df, start_row):
    """åˆ†æå¸¦å®½ä½¿ç”¨æƒ…å†µï¼ˆKBå•ä½ï¼‰"""
    row = start_row
    ws.cell(row=row, column=1, value="å¸¦å®½æ¶ˆè€—åˆ†æ(KBå•ä½):")
    row += 1

    try:
        # æŸ¥æ‰¾KBå•ä½çš„å¸¦å®½åˆ—
        kb_columns = [col for col in df.columns if 'kb' in col.lower() or 'KB' in col]
        bandwidth_col = None

        for col in ['æ€»å“åº”å¤§å°KB', 'total_bytes_sent_kb', 'æ€»å¸¦å®½æ¶ˆè€—KB']:
            if col in df.columns:
                bandwidth_col = col
                break

        if bandwidth_col:
            total_bandwidth_kb = df[bandwidth_col].sum()
            ws.cell(row=row, column=1, value=f"æ€»å¸¦å®½æ¶ˆè€—: {total_bandwidth_kb:,.1f}KB ({total_bandwidth_kb / 1024:.1f}MB)")
            row += 1

            # æ˜¾ç¤ºtop5æ¶ˆè€—
            top_bandwidth = df.nlargest(5, bandwidth_col)
            for _, data in top_bandwidth.iterrows():
                service = data.get('æœåŠ¡åç§°', data.get('service_name', 'æœªçŸ¥æœåŠ¡'))
                path = _get_request_path(data)
                bandwidth = data[bandwidth_col]
                count = _get_request_count(data)

                info = f"- {service}{path}: {bandwidth:,.1f}KB"
                if count:
                    info += f" ({count:,}æ¬¡è¯·æ±‚)"

                ws.cell(row=row, column=1, value=info)
                row += 1

            _cleanup_dataframe(top_bandwidth)

    except Exception as e:
        ws.cell(row=row, column=1, value=f"å¸¦å®½åˆ†æé”™è¯¯: {str(e)}")
        row += 1

    return row + 1


def _add_connection_efficiency_analysis(ws, outputs, start_row):
    """æ·»åŠ è¿æ¥æ•ˆç‡åˆ†æ"""
    row = start_row
    ws.cell(row=row, column=1, value="è¿æ¥æ•ˆç‡åˆ†æ:").font = Font(bold=True, size=12)
    row += 1

    # è¿æ¥æŒ‡æ ‡åˆ†æ
    conn_metrics_df = outputs.get('connection_metrics')
    if isinstance(conn_metrics_df, pd.DataFrame) and not conn_metrics_df.empty:
        # è¿æ¥æ•ˆç‡è¯„åˆ†
        conn_summary = outputs.get('connection_summary', {})
        avg_ratio = conn_summary.get('å¹³å‡è¿æ¥/è¯·æ±‚æ¯”ä¾‹', 0)
        efficiency_score = max(0, 100 - min(avg_ratio * 200, 90))

        score_text = f"è¿æ¥æ•ˆç‡è¯„åˆ†: {efficiency_score:.1f}/100"
        score_cell = ws.cell(row=row, column=1, value=score_text)
        score_cell.font = Font(bold=True, color=_get_score_color(efficiency_score))
        row += 1

        # è¿æ¥å¤ç”¨ç‡åˆ†æ
        if 'total_requests' in outputs:
            total_requests = _safe_int_convert(outputs['total_requests'])
            total_connections = conn_metrics_df['æ–°è¿æ¥æ•°'].sum()
            if total_connections > 0:
                reuse_ratio = total_requests / total_connections
                ws.cell(row=row, column=1,
                        value=f"å¹³å‡è¿æ¥å¤ç”¨ç‡: {reuse_ratio:.2f} è¯·æ±‚/è¿æ¥")
                row += 1

        # è¿æ¥ç±»å‹åˆ†å¸ƒ
        row = _analyze_connection_types(ws, outputs, row)

    return row + 1


def _add_performance_bottleneck_diagnosis(ws, outputs, start_row):
    """æ·»åŠ æ€§èƒ½ç“¶é¢ˆè¯Šæ–­"""
    row = start_row
    ws.cell(row=row, column=1, value="æ€§èƒ½ç“¶é¢ˆè¯Šæ–­:").font = Font(bold=True, size=12)
    row += 1

    slowest_requests = outputs.get('slowest_requests')
    if not isinstance(slowest_requests, pd.DataFrame) or slowest_requests.empty:
        ws.cell(row=row, column=1, value="æ— æ€§èƒ½æ•°æ®å¯ä¾›è¯Šæ–­")
        return row + 2

    # è¯†åˆ«ä¸»è¦ç“¶é¢ˆ
    bottlenecks = _identify_performance_bottlenecks(slowest_requests)

    if bottlenecks:
        ws.cell(row=row, column=1, value="å‘ç°çš„æ€§èƒ½ç“¶é¢ˆ:").font = Font(bold=True)
        row += 1

        for bottleneck in bottlenecks:
            ws.cell(row=row, column=1, value=f"- {bottleneck}").font = Font(color="FF0000")
            row += 1

        row += 1
        ws.cell(row=row, column=1, value="é’ˆå¯¹æ€§ä¼˜åŒ–å»ºè®®:")
        row += 1

        suggestions = _get_bottleneck_suggestions(bottlenecks)
        for suggestion in suggestions:
            ws.cell(row=row, column=1, value=f"  {suggestion}")
            row += 1
    else:
        ws.cell(row=row, column=1, value="æœªå‘ç°æ˜æ˜¾æ€§èƒ½ç“¶é¢ˆ")
        row += 1

    return row + 1


def _add_optimization_suggestions_enhanced(ws, start_row):
    """æ·»åŠ å¢å¼ºç‰ˆä¼˜åŒ–å»ºè®®"""
    row = start_row
    ws.cell(row=row, column=1, value="ç³»ç»Ÿä¼˜åŒ–å»ºè®®:").font = Font(bold=True, size=12)
    row += 1

    suggestions = [
        "ğŸ”§ HTTPç”Ÿå‘½å‘¨æœŸä¼˜åŒ–:",
        "  â€¢ åç«¯è¿æ¥é˜¶æ®µä¼˜åŒ–: å¯ç”¨è¿æ¥æ± ï¼Œå¢åŠ keepaliveæ—¶é—´",
        "  â€¢ åç«¯å¤„ç†é˜¶æ®µä¼˜åŒ–: ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œå¯ç”¨ç¼“å­˜æœºåˆ¶",
        "  â€¢ ä¼ è¾“é˜¶æ®µä¼˜åŒ–: å¯ç”¨gzipå‹ç¼©ï¼Œä¼˜åŒ–å“åº”ä½“å¤§å°",
        "",
        "âš¡ è¿æ¥æ•ˆç‡æå‡:",
        "  â€¢ é…ç½®åˆé€‚çš„keepalive_timeoutå‚æ•°(å»ºè®®30-60ç§’)",
        "  â€¢ å¯ç”¨HTTP/2å‡å°‘è¿æ¥å¼€é”€",
        "  â€¢ ä¼˜åŒ–upstream keepaliveè¿æ¥æ•°",
        "",
        "ğŸ“Š æ€§èƒ½ç›‘æ§å»ºè®¾:",
        "  â€¢ å»ºç«‹HTTPç”Ÿå‘½å‘¨æœŸå„é˜¶æ®µç›‘æ§",
        "  â€¢ è®¾ç½®è¿æ¥æ•ˆç‡æŒ‡æ ‡å‘Šè­¦",
        "  â€¢ ç›‘æ§ä¼ è¾“é€Ÿåº¦å’Œå“åº”ä½“å¤§å°è¶‹åŠ¿",
        "",
        "ğŸ› ï¸ æ¶æ„ä¼˜åŒ–æ–¹å‘:",
        "  â€¢ å®æ–½å¾®æœåŠ¡æ‹†åˆ†ï¼Œå‡å°‘å•æœåŠ¡å¤„ç†æ—¶é—´",
        "  â€¢ éƒ¨ç½²CDNåŠ é€Ÿé™æ€èµ„æºä¼ è¾“",
        "  â€¢ ä¼˜åŒ–è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œæå‡æ•´ä½“åå"
    ]

    for suggestion in suggestions:
        ws.cell(row=row, column=1, value=suggestion)
        row += 1

    return row


# è¾…åŠ©å‡½æ•°
def _calculate_error_rate(status_stats):
    """è®¡ç®—é”™è¯¯ç‡"""
    if status_stats.empty:
        return 0

    error_rate = 0
    if isinstance(status_stats, pd.DataFrame):
        for _, row_data in status_stats.iterrows():
            status_code = str(row_data.get('çŠ¶æ€ç ', ''))
            if status_code.startswith('5'):
                error_rate += float(row_data.get('ç™¾åˆ†æ¯”(%)', 0))

    return error_rate


def _evaluate_lifecycle_performance(outputs):
    """è¯„ä¼°HTTPç”Ÿå‘½å‘¨æœŸæ€§èƒ½"""
    slowest_requests = outputs.get('slowest_requests')
    if not isinstance(slowest_requests, pd.DataFrame) or slowest_requests.empty:
        return 100

    score = 100

    # æ£€æŸ¥å„é˜¶æ®µå¹³å‡æ—¶é•¿
    phase_thresholds = {
        'backend_connect_phase': 0.1,  # è¿æ¥é˜¶æ®µä¸åº”è¶…è¿‡100ms
        'backend_process_phase': 0.5,  # å¤„ç†é˜¶æ®µä¸åº”è¶…è¿‡500ms
        'nginx_transfer_phase': 0.1  # Nginxä¼ è¾“ä¸åº”è¶…è¿‡100ms
    }

    for phase, threshold in phase_thresholds.items():
        if phase in slowest_requests.columns:
            avg_time = slowest_requests[phase].mean()
            if avg_time > threshold:
                score -= min(20, (avg_time - threshold) * 40)

    return max(0, score)


def _evaluate_connection_efficiency(outputs):
    """è¯„ä¼°è¿æ¥æ•ˆç‡"""
    conn_summary = outputs.get('connection_summary', {})
    avg_ratio = conn_summary.get('å¹³å‡è¿æ¥/è¯·æ±‚æ¯”ä¾‹', 0.5)

    # è¿æ¥/è¯·æ±‚æ¯”ä¾‹è¶Šä½ï¼Œæ•ˆç‡è¶Šé«˜
    if avg_ratio < 0.2:
        return 90
    elif avg_ratio < 0.3:
        return 80
    elif avg_ratio < 0.5:
        return 70
    else:
        return max(0, 70 - (avg_ratio - 0.5) * 100)


def _get_health_status(score):
    """è·å–å¥åº·çŠ¶æ€æè¿°"""
    if score >= 90:
        return "ä¼˜ç§€"
    elif score >= 80:
        return "è‰¯å¥½"
    elif score >= 70:
        return "ä¸€èˆ¬"
    else:
        return "éœ€è¦å…³æ³¨"


def _get_score_color(score):
    """è·å–åˆ†æ•°å¯¹åº”çš„é¢œè‰²"""
    if score >= 80:
        return "008000"  # ç»¿è‰²
    elif score >= 70:
        return "FFA500"  # æ©™è‰²
    else:
        return "FF0000"  # çº¢è‰²


def _is_phase_problematic(phase_key, percentage):
    """åˆ¤æ–­é˜¶æ®µæ˜¯å¦æœ‰é—®é¢˜"""
    thresholds = {
        'backend_connect_phase': 30,
        'backend_process_phase': 40,
        'backend_transfer_phase': 50,
        'nginx_transfer_phase': 20
    }
    return percentage > thresholds.get(phase_key, 50)


def _is_efficiency_problematic(metric_key, value):
    """åˆ¤æ–­æ•ˆç‡æŒ‡æ ‡æ˜¯å¦æœ‰é—®é¢˜"""
    problematic_thresholds = {
        'backend_efficiency': (None, 60),  # ä½äº60%æœ‰é—®é¢˜
        'network_overhead': (30, None),  # é«˜äº30%æœ‰é—®é¢˜
        'transfer_ratio': (40, None),  # é«˜äº40%æœ‰é—®é¢˜
        'connection_cost_ratio': (20, None)  # é«˜äº20%æœ‰é—®é¢˜
    }

    if metric_key not in problematic_thresholds:
        return False

    min_threshold, max_threshold = problematic_thresholds[metric_key]

    if min_threshold and value < min_threshold:
        return True
    if max_threshold and value > max_threshold:
        return True

    return False


def _identify_performance_bottlenecks(df):
    """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
    bottlenecks = []

    total_avg = df['è¯·æ±‚æ€»æ—¶é•¿(ç§’)'].mean()
    if total_avg <= 0:
        return bottlenecks

    # æ£€æŸ¥å„é˜¶æ®µå æ¯”
    phases = {
        'backend_connect_phase': ('åç«¯è¿æ¥é˜¶æ®µ(ç§’)', 30),
        'backend_process_phase': ('åç«¯å¤„ç†é˜¶æ®µ(ç§’)', 40),
        'backend_transfer_phase': ('åç«¯ä¼ è¾“é˜¶æ®µ(ç§’)', 50),
        'nginx_transfer_phase': ('Nginxä¼ è¾“é˜¶æ®µ(ç§’)', 20)
    }

    for phase_key, (phase_name, threshold) in phases.items():
        if phase_key in df.columns:
            avg_time = df[phase_key].mean()
            percentage = (avg_time / total_avg) * 100
            if percentage > threshold:
                bottlenecks.append(f"{phase_name}è€—æ—¶è¿‡é•¿({percentage:.1f}%)")

    return bottlenecks


def _get_bottleneck_suggestions(bottlenecks):
    """è·å–ç“¶é¢ˆå¯¹åº”çš„ä¼˜åŒ–å»ºè®®"""
    suggestions = []
    suggestion_map = {
        "åç«¯è¿æ¥é˜¶æ®µ": "ä¼˜åŒ–è¿æ¥æ± é…ç½®ï¼Œå¯ç”¨keepaliveé•¿è¿æ¥",
        "åç«¯å¤„ç†é˜¶æ®µ": "ä¼˜åŒ–ä¸šåŠ¡é€»è¾‘ï¼Œæ·»åŠ ç¼“å­˜å±‚ï¼Œæ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢",
        "åç«¯ä¼ è¾“é˜¶æ®µ": "å¯ç”¨å“åº”å‹ç¼©ï¼Œä¼˜åŒ–å“åº”ä½“å¤§å°ï¼Œæ£€æŸ¥ç½‘ç»œå¸¦å®½",
        "Nginxä¼ è¾“é˜¶æ®µ": "æ£€æŸ¥Nginxé…ç½®ï¼Œä¼˜åŒ–å®¢æˆ·ç«¯ç½‘ç»œè¿æ¥"
    }

    for bottleneck in bottlenecks:
        for key, suggestion in suggestion_map.items():
            if key in bottleneck:
                suggestions.append(f"â€¢ {suggestion}")
                break

    return suggestions


def _add_stats_table(ws, outputs, key, title, columns, start_row, max_items=5):
    """æ·»åŠ ç»Ÿè®¡è¡¨æ ¼"""
    row = start_row
    df = outputs.get(key)

    if df is None or (isinstance(df, pd.DataFrame) and df.empty):
        ws.cell(row=row, column=1, value=f"{title} æ— æ•°æ®")
        return row + 2

    ws.cell(row=row, column=1, value=title).font = Font(bold=True)
    row += 1

    try:
        if isinstance(df, dict):
            df = pd.DataFrame([df])

        for i in range(min(max_items, len(df))):
            values = []
            for col in columns:
                values.append(df.iloc[i].get(col, "N/A") if col in df.columns else "N/A")

            if len(values) >= 3 and all(isinstance(v, (int, float)) for v in values[1:3]):
                cell_text = f"- {values[0]}: {values[1]:,} ({values[2]:.2f}%)"
            else:
                cell_text = f"- {' '.join(str(v) for v in values)}"

            ws.cell(row=row, column=1, value=cell_text)
            row += 1

    except Exception as e:
        ws.cell(row=row, column=1, value=f"æ•°æ®å¤„ç†é”™è¯¯: {str(e)}")
        row += 1

    return row + 1


def _safe_int_convert(value):
    """å®‰å…¨çš„æ•´æ•°è½¬æ¢"""
    try:
        return int(value)
    except (ValueError, TypeError):
        return 0


def _get_request_path(data):
    """è·å–è¯·æ±‚è·¯å¾„"""
    path_cols = ['è¯·æ±‚è·¯å¾„', 'request_path', 'è¯·æ±‚æ–¹æ³•']
    return next((data.get(col, '') for col in path_cols if col in data), '')


def _get_request_count(data):
    """è·å–è¯·æ±‚æ•°é‡"""
    count_cols = ['è¯·æ±‚æ¬¡æ•°', 'æ€»è¯·æ±‚æ•°', 'request_count']
    return next((data.get(col) for col in count_cols if col in data), None)


def _analyze_request_frequency(ws, df, start_row):
    """åˆ†æè¯·æ±‚é¢‘ç‡"""
    row = start_row
    ws.cell(row=row, column=1, value="è¯·æ±‚é¢‘ç‡åˆ†æ:")
    row += 1

    try:
        top_freq = df.head(5)
        for _, data in top_freq.iterrows():
            service = data.get('æœåŠ¡åç§°', 'æœªçŸ¥æœåŠ¡')
            avg_rpm = data.get('å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚æ•°', 0)
            max_rpm = data.get('æœ€å¤§æ¯åˆ†é’Ÿè¯·æ±‚æ•°', 0)
            total = data.get('æ€»è¯·æ±‚æ•°', 0)

            ws.cell(row=row, column=1,
                    value=f"- {service}: å¹³å‡{avg_rpm:.1f}æ¬¡/åˆ†é’Ÿ, å³°å€¼{max_rpm:.1f}æ¬¡/åˆ†é’Ÿ, æ€»è®¡{total:,}æ¬¡")
            row += 1

        _cleanup_dataframe(top_freq)
    except Exception:
        ws.cell(row=row, column=1, value="é¢‘ç‡åˆ†æå¤„ç†é”™è¯¯")
        row += 1

    return row + 1


def _analyze_connection_types(ws, outputs, start_row):
    """åˆ†æè¿æ¥ç±»å‹åˆ†å¸ƒ"""
    row = start_row
    conn_types = outputs.get('connection_types', {})

    if conn_types:
        ws.cell(row=row, column=1, value="è¿æ¥ç±»å‹åˆ†å¸ƒ:")
        row += 1

        keep_alive = conn_types.get('keep_alive', 0)
        short_lived = conn_types.get('short_lived', 0)
        total_conn = keep_alive + short_lived

        if total_conn > 0:
            keep_alive_pct = keep_alive / total_conn * 100
            short_pct = short_lived / total_conn * 100

            ws.cell(row=row, column=1,
                    value=f"- é•¿è¿æ¥(Keep-Alive): {keep_alive:,} ({keep_alive_pct:.1f}%)")
            row += 1

            ws.cell(row=row, column=1,
                    value=f"- çŸ­è¿æ¥: {short_lived:,} ({short_pct:.1f}%)")
            row += 1

            # è¿æ¥å¤ç”¨ç‡è¯„ä¼°
            if keep_alive_pct < 50:
                ws.cell(row=row, column=1,
                        value="âš ï¸ é•¿è¿æ¥å æ¯”è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–keepaliveé…ç½®").font = Font(color="FFA500")
                row += 1

    return row


def _cleanup_dataframe(df):
    """æ¸…ç†DataFrameå†…å­˜"""
    if df is not None:
        del df
        gc.collect()


def _cleanup_resources(*resources):
    """æ¸…ç†èµ„æº"""
    for resource in resources:
        if resource is not None:
            del resource
    gc.collect()


def _add_trend_analysis_section(ws, outputs, start_row):
    """ä¼˜åŒ–åçš„æ—¶é—´è¶‹åŠ¿åˆ†æ"""
    row = start_row
    ws.cell(row=row, column=1, value="æ—¶é—´è¶‹åŠ¿åˆ†æ:").font = Font(bold=True, size=12)
    row += 1

    # å“åº”æ—¶é—´è¶‹åŠ¿åˆ†æ
    row = _analyze_response_time_trend(ws, outputs, row)

    # å¹¶å‘è¿æ¥è¶‹åŠ¿åˆ†æ
    row = _analyze_concurrency_trend(ws, outputs, row)

    # çŠ¶æ€ç è¶‹åŠ¿åˆ†æ
    row = _analyze_status_trend(ws, outputs, row)

    return row


def _analyze_response_time_trend(ws, outputs, start_row):
    """åˆ†æå“åº”æ—¶é—´è¶‹åŠ¿"""
    row = start_row
    resp_trend_df = outputs.get('hourly_response_trend')

    if not isinstance(resp_trend_df, pd.DataFrame) or resp_trend_df.empty:
        return row

    ws.cell(row=row, column=1, value="å“åº”æ—¶é—´è¶‹åŠ¿åˆ†æ:")
    row += 1

    try:
        # æŸ¥æ‰¾æ—¶é—´å’Œå“åº”æ—¶é—´åˆ—
        time_col = _find_column(resp_trend_df, ['å°æ—¶', 'hour', 'date_hour'])
        resp_time_col = _find_column(resp_trend_df, ['å¹³å‡å“åº”æ—¶é—´(ç§’)', 'avg_response_time', 'total_request_duration'])

        if time_col and resp_time_col:
            # æ‰¾å‡ºå“åº”æ—¶é—´æœ€é«˜çš„3ä¸ªæ—¶æ®µ
            peak_hours = resp_trend_df.nlargest(3, resp_time_col)

            for _, data in peak_hours.iterrows():
                hour = data[time_col]
                avg_time = data[resp_time_col]

                # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
                time_str = _format_datetime(hour)

                # è·å–å…¶ä»–æŒ‡æ ‡
                median_col = _find_column(data, ['ä¸­ä½æ•°å“åº”æ—¶é—´(ç§’)', 'median_response_time'])
                count_col = _find_column(data, ['è¯·æ±‚æ•°', 'request_count'])

                info_parts = [f"é«˜å³°æ—¶æ®µ {time_str}: å¹³å‡{avg_time:.3f}ç§’"]

                if median_col and median_col in data.index:
                    median_val = data[median_col]
                    info_parts.append(f"ä¸­ä½æ•°{median_val:.3f}ç§’")

                if count_col and count_col in data.index:
                    count = data[count_col]
                    info_parts.append(f"è¯·æ±‚æ•°{count:,}")

                ws.cell(row=row, column=1, value=f"- {', '.join(info_parts)}")
                row += 1

            _cleanup_dataframe(peak_hours)

    except Exception as e:
        ws.cell(row=row, column=1, value=f"å“åº”æ—¶é—´è¶‹åŠ¿åˆ†æé”™è¯¯: {str(e)}")
        row += 1

    finally:
        _cleanup_dataframe(resp_trend_df)

    return row + 1


def _analyze_concurrency_trend(ws, outputs, start_row):
    """åˆ†æå¹¶å‘è¿æ¥è¶‹åŠ¿"""
    row = start_row
    concurrency_df = outputs.get('concurrency_estimation')

    if not isinstance(concurrency_df, pd.DataFrame) or concurrency_df.empty:
        return row

    ws.cell(row=row, column=1, value="å¹¶å‘è¿æ¥è¶‹åŠ¿åˆ†æ:")
    row += 1

    try:
        # è¶‹åŠ¿åˆ†æï¼šæ¯”è¾ƒå‰ååŠæ®µæ•°æ®
        if len(concurrency_df) > 1 and 'å¹³å‡å¹¶å‘æ•°' in concurrency_df.columns:
            half_point = len(concurrency_df) // 2
            first_half_avg = concurrency_df.iloc[:half_point]['å¹³å‡å¹¶å‘æ•°'].mean()
            second_half_avg = concurrency_df.iloc[half_point:]['å¹³å‡å¹¶å‘æ•°'].mean()

            if second_half_avg > first_half_avg * 1.2:
                ws.cell(row=row, column=1, value="- å¹¶å‘æ•°å‘ˆæ˜æ˜¾ä¸Šå‡è¶‹åŠ¿ï¼Œéœ€å…³æ³¨ç³»ç»Ÿå®¹é‡").font = Font(color="FFA500")
                row += 1
            elif second_half_avg < first_half_avg * 0.8:
                ws.cell(row=row, column=1, value="- å¹¶å‘æ•°å‘ˆä¸‹é™è¶‹åŠ¿")
                row += 1

        # æ˜¾ç¤ºé«˜å³°å¹¶å‘æ—¶æ®µ
        if 'æœ€å¤§å¹¶å‘æ•°' in concurrency_df.columns:
            peak_concurrency = concurrency_df.nlargest(3, 'æœ€å¤§å¹¶å‘æ•°')
            time_col = _find_column(concurrency_df, ['æ—¶é—´æ®µ', 'minute', 'date_hour_minute'])

            if time_col:
                for _, data in peak_concurrency.iterrows():
                    time_val = data[time_col]
                    time_str = _format_datetime(time_val)
                    avg_concurrent = data.get('å¹³å‡å¹¶å‘æ•°', 0)
                    max_concurrent = data.get('æœ€å¤§å¹¶å‘æ•°', 0)

                    ws.cell(row=row, column=1,
                            value=f"- é«˜å³°æ—¶æ®µ {time_str}: å¹³å‡å¹¶å‘{avg_concurrent:.1f}, æœ€å¤§å¹¶å‘{max_concurrent:.1f}")
                    row += 1

            # æ•´ä½“ç»Ÿè®¡
            overall_avg = concurrency_df['å¹³å‡å¹¶å‘æ•°'].mean()
            overall_max = concurrency_df['æœ€å¤§å¹¶å‘æ•°'].max()
            ws.cell(row=row, column=1, value=f"- æ•´ä½“ç»Ÿè®¡: å¹³å‡å¹¶å‘{overall_avg:.1f}, æœ€å¤§å¹¶å‘{overall_max:.1f}")
            row += 1

            _cleanup_dataframe(peak_concurrency)

    except Exception as e:
        ws.cell(row=row, column=1, value=f"å¹¶å‘è¶‹åŠ¿åˆ†æé”™è¯¯: {str(e)}")
        row += 1

    finally:
        _cleanup_dataframe(concurrency_df)

    return row + 1


def _analyze_status_trend(ws, outputs, start_row):
    """åˆ†æçŠ¶æ€ç è¶‹åŠ¿"""
    row = start_row
    status_trend_df = outputs.get('hourly_status_trend')

    if not isinstance(status_trend_df, pd.DataFrame) or status_trend_df.empty:
        return row

    ws.cell(row=row, column=1, value="çŠ¶æ€ç åˆ†å¸ƒè¶‹åŠ¿:")
    row += 1

    try:
        if '5xxæœåŠ¡å™¨é”™è¯¯' in status_trend_df.columns:
            # è®¡ç®—æ€»è¯·æ±‚æ•°å’Œé”™è¯¯ç‡
            status_trend_df['æ€»è¯·æ±‚æ•°'] = status_trend_df.select_dtypes(include=['number']).sum(axis=1)

            if status_trend_df['æ€»è¯·æ±‚æ•°'].sum() > 0:
                status_trend_df['5xxé”™è¯¯ç‡'] = (status_trend_df['5xxæœåŠ¡å™¨é”™è¯¯'] /
                                             status_trend_df['æ€»è¯·æ±‚æ•°'] * 100)

                # æ‰¾å‡ºé”™è¯¯ç‡æœ€é«˜çš„æ—¶æ®µ
                error_threshold = 1.0  # 1%é”™è¯¯ç‡é˜ˆå€¼
                high_error_periods = status_trend_df[status_trend_df['5xxé”™è¯¯ç‡'] > error_threshold]

                if not high_error_periods.empty:
                    ws.cell(row=row, column=1, value=f"å‘ç°{len(high_error_periods)}ä¸ªé«˜é”™è¯¯ç‡æ—¶æ®µ(>1%):")
                    row += 1

                    time_col = _find_column(status_trend_df, ['hour_bucket', 'å°æ—¶', 'æ—¶é—´', 'date_hour'])
                    peak_errors = high_error_periods.nlargest(3, '5xxé”™è¯¯ç‡')

                    if time_col:
                        for _, data in peak_errors.iterrows():
                            time_val = data[time_col]
                            time_str = _format_datetime(time_val)
                            error_rate = data['5xxé”™è¯¯ç‡']
                            error_count = data['5xxæœåŠ¡å™¨é”™è¯¯']

                            ws.cell(row=row, column=1,
                                    value=f"- {time_str}: 5xxé”™è¯¯ç‡{error_rate:.2f}% ({error_count:.0f}æ¬¡)").font = Font(
                                color="FF0000")
                            row += 1

                    _cleanup_dataframe(peak_errors)
                else:
                    ws.cell(row=row, column=1, value="- ç³»ç»Ÿé”™è¯¯ç‡ä¿æŒåœ¨å¥åº·æ°´å¹³(<1%)")
                    row += 1

                _cleanup_dataframe(high_error_periods)

    except Exception as e:
        ws.cell(row=row, column=1, value=f"çŠ¶æ€ç è¶‹åŠ¿åˆ†æé”™è¯¯: {str(e)}")
        row += 1

    finally:
        _cleanup_dataframe(status_trend_df)

    return row + 1


def _find_column(data, possible_names):
    """åœ¨æ•°æ®ä¸­æŸ¥æ‰¾å¯èƒ½çš„åˆ—å"""
    if isinstance(data, pd.DataFrame):
        columns = data.columns
    elif isinstance(data, pd.Series):
        columns = data.index
    else:
        return None

    for name in possible_names:
        if name in columns:
            return name
    return None


def _format_datetime(dt_value):
    """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´æ˜¾ç¤º"""
    if hasattr(dt_value, 'strftime'):
        return dt_value.strftime('%Y-%m-%d %H:%M')
    else:
        return str(dt_value)


# ä¸»è¦çš„è¾…åŠ©åˆ†æå‡½æ•°
def calculate_http_lifecycle_metrics(df):
    """è®¡ç®—HTTPç”Ÿå‘½å‘¨æœŸæŒ‡æ ‡"""
    if df.empty:
        return {}

    metrics = {}

    # åŸºç¡€æ—¶é—´æŒ‡æ ‡ç»Ÿè®¡
    basic_metrics = ['total_request_duration', 'upstream_response_time',
                     'upstream_header_time', 'upstream_connect_time']

    for metric in basic_metrics:
        if metric in df.columns:
            metrics[f'{metric}_avg'] = df[metric].mean()
            metrics[f'{metric}_p95'] = df[metric].quantile(0.95)
            metrics[f'{metric}_max'] = df[metric].max()

    # é˜¶æ®µæŒ‡æ ‡ç»Ÿè®¡
    phase_metrics = ['backend_connect_phase', 'backend_process_phase',
                     'backend_transfer_phase', 'nginx_transfer_phase']

    for metric in phase_metrics:
        if metric in df.columns:
            metrics[f'{metric}_avg'] = df[metric].mean()
            metrics[f'{metric}_p95'] = df[metric].quantile(0.95)

    # æ•ˆç‡æŒ‡æ ‡ç»Ÿè®¡
    efficiency_metrics = ['backend_efficiency', 'network_overhead',
                          'transfer_ratio', 'connection_cost_ratio']

    for metric in efficiency_metrics:
        if metric in df.columns:
            metrics[f'{metric}_avg'] = df[metric].mean()

    # ä¼ è¾“é€Ÿåº¦æŒ‡æ ‡
    speed_metrics = ['response_transfer_speed', 'total_transfer_speed',
                     'nginx_transfer_speed']

    for metric in speed_metrics:
        if metric in df.columns:
            metrics[f'{metric}_avg'] = df[metric].mean()
            metrics[f'{metric}_max'] = df[metric].max()

    return metrics


def generate_lifecycle_performance_summary(metrics):
    """ç”Ÿæˆç”Ÿå‘½å‘¨æœŸæ€§èƒ½æ‘˜è¦"""
    summary = []

    # æ£€æŸ¥å…³é”®æ€§èƒ½æŒ‡æ ‡
    if 'backend_efficiency_avg' in metrics:
        efficiency = metrics['backend_efficiency_avg']
        if efficiency < 60:
            summary.append(f"åç«¯å¤„ç†æ•ˆç‡åä½({efficiency:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–ä¸šåŠ¡é€»è¾‘")
        elif efficiency > 80:
            summary.append(f"åç«¯å¤„ç†æ•ˆç‡è‰¯å¥½({efficiency:.1f}%)")

    if 'network_overhead_avg' in metrics:
        overhead = metrics['network_overhead_avg']
        if overhead > 30:
            summary.append(f"ç½‘ç»œå¼€é”€è¾ƒé«˜({overhead:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–ç½‘ç»œé…ç½®")
        elif overhead < 20:
            summary.append(f"ç½‘ç»œå¼€é”€æ§åˆ¶è‰¯å¥½({overhead:.1f}%)")

    if 'connection_cost_ratio_avg' in metrics:
        conn_cost = metrics['connection_cost_ratio_avg']
        if conn_cost > 20:
            summary.append(f"è¿æ¥æˆæœ¬è¾ƒé«˜({conn_cost:.1f}%)ï¼Œå»ºè®®å¯ç”¨é•¿è¿æ¥")
        elif conn_cost < 10:
            summary.append(f"è¿æ¥æˆæœ¬æ§åˆ¶è‰¯å¥½({conn_cost:.1f}%)")

    # ä¼ è¾“é€Ÿåº¦è¯„ä¼°
    if 'total_transfer_speed_avg' in metrics:
        speed = metrics['total_transfer_speed_avg']
        if speed < 100:  # KB/s
            summary.append(f"ä¼ è¾“é€Ÿåº¦è¾ƒæ…¢({speed:.1f}KB/s)ï¼Œå»ºè®®æ£€æŸ¥å¸¦å®½å’Œå‹ç¼©è®¾ç½®")
        elif speed > 1000:
            summary.append(f"ä¼ è¾“é€Ÿåº¦è‰¯å¥½({speed:.1f}KB/s)")

    return summary


def analyze_performance_trends(df, time_column='arrival_date_hour'):
    """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
    if df.empty or time_column not in df.columns:
        return {}

    # æŒ‰å°æ—¶åˆ†ç»„åˆ†æ
    hourly_stats = df.groupby(time_column).agg({
        'total_request_duration': ['mean', 'count'],
        'backend_connect_phase': 'mean',
        'backend_process_phase': 'mean',
        'backend_transfer_phase': 'mean',
        'nginx_transfer_phase': 'mean',
        'backend_efficiency': 'mean',
        'network_overhead': 'mean'
    }).round(4)

    # æ‰å¹³åŒ–åˆ—å
    hourly_stats.columns = ['_'.join(col) for col in hourly_stats.columns]

    # è¯†åˆ«æ€§èƒ½å¼‚å¸¸æ—¶æ®µ
    response_time_mean = hourly_stats['total_request_duration_mean'].mean()
    response_time_std = hourly_stats['total_request_duration_mean'].std()
    threshold = response_time_mean + 2 * response_time_std

    anomaly_hours = hourly_stats[
        hourly_stats['total_request_duration_mean'] > threshold
        ].index.tolist()

    return {
        'hourly_performance': hourly_stats,
        'anomaly_hours': anomaly_hours,
        'performance_baseline': {
            'avg_response_time': response_time_mean,
            'response_time_std': response_time_std,
            'anomaly_threshold': threshold
        }
    }


# æ•°æ®éªŒè¯å’Œæ¸…æ´—å‡½æ•°
def validate_lifecycle_data(df):
    """éªŒè¯ç”Ÿå‘½å‘¨æœŸæ•°æ®å®Œæ•´æ€§"""
    required_columns = [
        'total_request_duration',
        'upstream_connect_time',
        'upstream_header_time',
        'upstream_response_time'
    ]

    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        return False, f"ç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_columns)}"

    # æ£€æŸ¥æ•°æ®é€»è¾‘æ€§
    invalid_rows = 0

    # æ£€æŸ¥æ—¶é—´é€»è¾‘å…³ç³»
    if all(col in df.columns for col in required_columns):
        # upstream_connect_time <= upstream_header_time <= upstream_response_time <= total_request_duration
        logical_errors = (
                (df['upstream_connect_time'] > df['upstream_header_time']) |
                (df['upstream_header_time'] > df['upstream_response_time']) |
                (df['upstream_response_time'] > df['total_request_duration'])
        )
        invalid_rows = logical_errors.sum()

    if invalid_rows > len(df) * 0.1:  # å¦‚æœè¶…è¿‡10%çš„æ•°æ®æœ‰é€»è¾‘é”™è¯¯
        return False, f"å‘ç°{invalid_rows}è¡Œæ•°æ®å­˜åœ¨æ—¶é—´é€»è¾‘é”™è¯¯"

    return True, "æ•°æ®éªŒè¯é€šè¿‡"


def clean_lifecycle_data(df):
    """æ¸…æ´—ç”Ÿå‘½å‘¨æœŸæ•°æ®"""
    original_rows = len(df)

    # ç§»é™¤æ˜æ˜¾å¼‚å¸¸çš„æ•°æ®
    # 1. ç§»é™¤è´Ÿæ•°æ—¶é—´
    time_columns = [col for col in df.columns if 'time' in col.lower() or 'phase' in col.lower()]
    for col in time_columns:
        if col in df.columns:
            df = df[df[col] >= 0]

    # 2. ç§»é™¤è¶…é•¿å“åº”æ—¶é—´ï¼ˆè¶…è¿‡10åˆ†é’Ÿçš„è¯·æ±‚é€šå¸¸æ˜¯å¼‚å¸¸ï¼‰
    if 'total_request_duration' in df.columns:
        df = df[df['total_request_duration'] <= 600]

    # 3. ä¿®å¤é€»è¾‘é”™è¯¯ï¼ˆä¿å®ˆå¤„ç†ï¼‰
    if all(col in df.columns for col in ['upstream_connect_time', 'upstream_header_time',
                                         'upstream_response_time', 'total_request_duration']):
        # ç¡®ä¿æ—¶é—´é€’å¢å…³ç³»
        df['upstream_header_time'] = df[['upstream_header_time', 'upstream_connect_time']].max(axis=1)
        df['upstream_response_time'] = df[['upstream_response_time', 'upstream_header_time']].max(axis=1)
        df['total_request_duration'] = df[['total_request_duration', 'upstream_response_time']].max(axis=1)

    cleaned_rows = len(df)
    removed_rows = original_rows - cleaned_rows

    log_info(f"æ•°æ®æ¸…æ´—å®Œæˆ: åŸå§‹{original_rows}è¡Œ, æ¸…æ´—å{cleaned_rows}è¡Œ, ç§»é™¤{removed_rows}è¡Œå¼‚å¸¸æ•°æ®")

    return df