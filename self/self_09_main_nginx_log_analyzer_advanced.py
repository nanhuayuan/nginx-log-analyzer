import os
import gc
import pandas as pd
from datetime import datetime
import traceback

# å¯¼å…¥ä¼˜åŒ–ç‰ˆæœ¬çš„åˆ†ææ¨¡å—
from self_01_api_analyzer_optimized import analyze_api_performance
from self_02_service_analyzer_advanced import analyze_service_performance_advanced
from self_03_slow_requests_analyzer_advanced import analyze_slow_requests_advanced
from self_04_status_analyzer_advanced import analyze_status_codes
from self_05_time_dimension_analyzer_advanced import analyze_time_dimension
from self_06_performance_stability_analyzer_advanced import analyze_service_stability
from self_07_generate_summary_report_analyzer_advanced import generate_advanced_summary_report
from self_08_ip_analyzer_advanced import analyze_ip_sources
from self_10_request_header_analyzer import analyze_request_headers
from self_11_header_performance_analyzer import analyze_header_performance_correlation

# å¯¼å…¥å¸¸é‡å’Œå·¥å…·å‡½æ•°
from self_00_01_constants import (
    DEFAULT_LOG_DIR, DEFAULT_SUCCESS_CODES, DEFAULT_SLOW_THRESHOLD, 
    DEFAULT_COLUMN_API, DEFAULT_START_DATE, DEFAULT_END_DATE
)
from self_00_03_log_parser import collect_log_files, process_log_files
from self_00_02_utils import log_info


class AdvancedNginxLogAnalyzer:
    """é«˜çº§Nginxæ—¥å¿—åˆ†æå™¨ - ç»Ÿä¸€åè°ƒæ‰€æœ‰åˆ†ææ¨¡å—"""
    
    def __init__(self):
        self.script_start_time = datetime.now()
        self.outputs = {}
        self.temp_files = []
        
    def main(self):
        """ä¸»åˆ†ææµç¨‹ - ä¼˜åŒ–ç‰ˆ"""
        try:
            log_info(f"ğŸš€ å¼€å§‹æ‰§è¡Œé«˜çº§Nginxæ—¥å¿—åˆ†æä»»åŠ¡ (ç‰ˆæœ¬: 2.0.0-Advanced)", show_memory=True)
            
            # åˆå§‹åŒ–åˆ†æç¯å¢ƒ
            log_dir, output_dir, temp_dir, temp_csv = self._setup_analysis_environment()
            
            # æ”¶é›†å’Œå¤„ç†æ—¥å¿—æ–‡ä»¶
            total_records = self._collect_and_process_logs(log_dir, temp_csv)
            if total_records == 0:
                log_info("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ—¥å¿—æ•°æ®ï¼Œåˆ†æç»ˆæ­¢", level="ERROR")
                return
            
            # åˆå§‹åŒ–è¾“å‡ºæ•°æ®ç»“æ„
            self._initialize_outputs(log_dir, total_records)
            
            # å®šä¹‰åˆ†æä»»åŠ¡é…ç½®
            analysis_tasks = self._define_analysis_tasks(temp_csv, output_dir)
            
            # æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
            self._execute_analysis_tasks(analysis_tasks, temp_csv, output_dir)
            
            # ç”Ÿæˆé«˜çº§ç»¼åˆæŠ¥å‘Š
            summary_output = os.path.join(output_dir, "12_é«˜çº§ç»¼åˆæŠ¥å‘Š.xlsx")
            self._generate_advanced_summary_report(summary_output)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files()
            
            # æ˜¾ç¤ºåˆ†æå®Œæˆæ‘˜è¦
            self._show_completion_summary(output_dir)
            
        except Exception as e:
            log_info(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", level="ERROR")
            log_info(traceback.format_exc(), level="ERROR")
        finally:
            self._final_cleanup()
            total_elapsed = (datetime.now() - self.script_start_time).total_seconds()
            log_info(f"ğŸ‰ é«˜çº§åˆ†æä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.2f} ç§’", level="INFO")
    
    def _setup_analysis_environment(self):
        """è®¾ç½®åˆ†æç¯å¢ƒ"""
        log_info("ğŸ”§ è®¾ç½®åˆ†æç¯å¢ƒ...")
        
        log_dir = DEFAULT_LOG_DIR
        output_dir = f"{log_dir}_é«˜çº§åˆ†æç»“æœ_{self.script_start_time.strftime('%Y%m%d_%H%M%S')}"
        temp_dir = f"{output_dir}_temp"
        temp_csv = os.path.join(temp_dir, "processed_logs.csv")
        
        # åˆ›å»ºç›®å½•
        for directory in [output_dir, temp_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                log_info(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")
        
        self.temp_files.append(temp_csv)
        return log_dir, output_dir, temp_dir, temp_csv
    
    def _collect_and_process_logs(self, log_dir, temp_csv):
        """æ”¶é›†å’Œå¤„ç†æ—¥å¿—æ–‡ä»¶"""
        log_info("ğŸ“‚ æ”¶é›†æ—¥å¿—æ–‡ä»¶...")
        
        log_files = collect_log_files(log_dir)
        if not log_files:
            log_info("âš ï¸ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶!", level="ERROR")
            return 0
        
        log_info(f"âœ… æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        # å¤„ç†æ—¥å¿—æ–‡ä»¶
        log_info("ğŸ”„ å¤„ç†æ—¥å¿—æ–‡ä»¶...")
        total_records = process_log_files(
            log_files, temp_csv, 
            start_date=DEFAULT_START_DATE, 
            end_date=DEFAULT_END_DATE
        )
        
        log_info(f"âœ… æ—¥å¿—å¤„ç†å®Œæˆï¼Œå…± {total_records:,} æ¡è®°å½•")
        return total_records
    
    def _initialize_outputs(self, log_dir, total_records):
        """åˆå§‹åŒ–è¾“å‡ºæ•°æ®ç»“æ„"""
        self.outputs = {
            'log_dir': log_dir,
            'total_requests': total_records,
            'analysis_time': self.script_start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'analyzer_version': '2.0.0-Advanced',
            'optimization_features': [
                'T-Digeståˆ†ä½æ•°è®¡ç®—',
                'HyperLogLogå”¯ä¸€å€¼è®¡æ•°',
                'è“„æ°´æ± é‡‡æ ·ç®—æ³•',
                'æµå¼å†…å­˜ç®¡ç†',
                'æ™ºèƒ½å¼‚å¸¸æ£€æµ‹',
                'å¤šç»´åº¦æ€§èƒ½åˆ†æ'
            ]
        }
    
    def _define_analysis_tasks(self, temp_csv, output_dir):
        """å®šä¹‰åˆ†æä»»åŠ¡é…ç½®"""
        return [
            {
                "name": "APIæ€§èƒ½åˆ†æ", 
                "priority": 1,
                "func": analyze_api_performance, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "01.æ¥å£æ€§èƒ½åˆ†æ.xlsx"),
                    "success_codes": DEFAULT_SUCCESS_CODES, 
                    "slow_threshold": DEFAULT_SLOW_THRESHOLD
                },
                "description": "åˆ†æAPIæ€§èƒ½æŒ‡æ ‡ï¼Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ",
                "output_key": "slow_apis"
            },
            {
                "name": "é«˜çº§æœåŠ¡å±‚çº§åˆ†æ", 
                "priority": 2,
                "func": analyze_service_performance_advanced, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "02.æœåŠ¡å±‚çº§åˆ†æ.xlsx"),
                    "success_codes": DEFAULT_SUCCESS_CODES
                },
                "description": "æ·±åº¦åˆ†ææœåŠ¡å±‚çº§æ€§èƒ½ï¼Œä½¿ç”¨æµå¼ç®—æ³•",
                "output_key": "service_stats"
            },
            {
                "name": "é«˜çº§æ…¢è¯·æ±‚åˆ†æ", 
                "priority": 3,
                "func": analyze_slow_requests_advanced, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "03_æ…¢è¯·æ±‚åˆ†æ.xlsx"),
                    "slow_threshold": DEFAULT_SLOW_THRESHOLD
                },
                "description": "æ™ºèƒ½è¯†åˆ«å’Œåˆ†ææ…¢è¯·æ±‚æ¨¡å¼",
                "output_key": "slowest_requests"
            },
            {
                "name": "é«˜çº§çŠ¶æ€ç åˆ†æ", 
                "priority": 4,
                "func": analyze_status_codes, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "04.çŠ¶æ€ç ç»Ÿè®¡.xlsx"),
                    "slow_request_threshold": DEFAULT_SLOW_THRESHOLD
                },
                "description": "å…¨é¢åˆ†æHTTPçŠ¶æ€ç åˆ†å¸ƒ",
                "output_key": "status_stats"
            },
            {
                "name": "é«˜çº§æ—¶é—´ç»´åº¦åˆ†æ-å…¨éƒ¨æ¥å£", 
                "priority": 5,
                "func": analyze_time_dimension, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "05.æ—¶é—´ç»´åº¦åˆ†æ-å…¨éƒ¨æ¥å£.xlsx")
                },
                "description": "åŸºäºT-Digestçš„æ—¶é—´ç»´åº¦æ·±åº¦åˆ†æ",
                "output_key": "time_analysis_all"
            },
            {
                "name": "æ—¶é—´ç»´åº¦åˆ†æ-ç‰¹å®šæ¥å£", 
                "priority": 6,
                "func": analyze_time_dimension, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "05_01.æ—¶é—´ç»´åº¦åˆ†æ-æŒ‡å®šæ¥å£.xlsx"),
                    "specific_uri_list": DEFAULT_COLUMN_API
                },
                "description": "é’ˆå¯¹å…³é”®æ¥å£çš„æ—¶é—´ç»´åº¦åˆ†æ",
                "output_key": "time_analysis_specific"
            },
            {
                "name": "é«˜çº§æœåŠ¡ç¨³å®šæ€§åˆ†æ", 
                "priority": 7,
                "func": analyze_service_stability, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "06_æœåŠ¡ç¨³å®šæ€§.xlsx")
                },
                "description": "å¤šç»´åº¦æœåŠ¡ç¨³å®šæ€§è¯„ä¼°ï¼Œå«å¼‚å¸¸æ£€æµ‹",
                "output_key": "service_stability"
            },
            {
                "name": "é«˜çº§IPæ¥æºåˆ†æ", 
                "priority": 8,
                "func": analyze_ip_sources, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "08_IPæ¥æºåˆ†æ.xlsx")
                },
                "description": "æ™ºèƒ½IPè¡Œä¸ºåˆ†æï¼Œå«é£é™©è¯„ä¼°",
                "output_key": "ip_analysis"
            },
            {
                "name": "è¯·æ±‚å¤´åˆ†æ", 
                "priority": 9,
                "func": analyze_request_headers, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "10_è¯·æ±‚å¤´åˆ†æ.xlsx")
                },
                "description": "User-Agentå’ŒRefereræ·±åº¦åˆ†æ",
                "output_key": "header_analysis"
            },
            {
                "name": "è¯·æ±‚å¤´æ€§èƒ½å…³è”åˆ†æ", 
                "priority": 10,
                "func": analyze_header_performance_correlation, 
                "args": {
                    "csv_path": temp_csv, 
                    "output_path": os.path.join(output_dir, "11_è¯·æ±‚å¤´æ€§èƒ½å…³è”åˆ†æ.xlsx"),
                    "slow_threshold": DEFAULT_SLOW_THRESHOLD
                },
                "description": "è¯·æ±‚å¤´ä¸æ€§èƒ½æŒ‡æ ‡çš„å…³è”æ€§åˆ†æ",
                "output_key": "header_performance_analysis"
            }
        ]
    
    def _execute_analysis_tasks(self, analysis_tasks, temp_csv, output_dir):
        """æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡"""
        # ä¼˜å…ˆæ‰§è¡ŒAPIæ€§èƒ½åˆ†æä»¥è·å–æœ€æ…¢çš„æ¥å£
        api_task = next((task for task in analysis_tasks if task["name"] == "APIæ€§èƒ½åˆ†æ"), None)
        top_5_slowest = None
        
        if api_task:
            log_info("ğŸ” é¢„å…ˆæ‰§è¡ŒAPIæ€§èƒ½åˆ†æä»¥è¯†åˆ«æœ€æ…¢çš„æ¥å£...")
            top_5_slowest = self._execute_single_task(api_task)
            
            # ä¸ºæœ€æ…¢æ¥å£æ·»åŠ ä¸“é—¨çš„æ—¶é—´ç»´åº¦åˆ†æ
            if top_5_slowest is not None and not top_5_slowest.empty:
                self._add_slow_api_analysis_tasks(analysis_tasks, top_5_slowest, temp_csv, output_dir)
        
        # æ‰§è¡Œæ‰€æœ‰åˆ†æä»»åŠ¡
        total_tasks = len(analysis_tasks)
        log_info(f"ğŸ“Š å¼€å§‹æ‰§è¡Œ {total_tasks} ä¸ªåˆ†æä»»åŠ¡...")
        
        for i, task in enumerate(analysis_tasks, 1):
            task_name = task["name"]
            
            # è·³è¿‡å·²æ‰§è¡Œçš„APIæ€§èƒ½åˆ†æ
            if task_name == "APIæ€§èƒ½åˆ†æ" and top_5_slowest is not None:
                log_info(f"[{i}/{total_tasks}] âœ… {task_name} (å·²é¢„å…ˆæ‰§è¡Œ)")
                self.outputs[task.get('output_key', task_name.lower())] = top_5_slowest
                continue
            
            log_info(f"[{i}/{total_tasks}] ğŸ”„ å¼€å§‹æ‰§è¡Œ: {task_name}")
            log_info(f"    ğŸ“ {task.get('description', 'æ‰§è¡Œåˆ†æä»»åŠ¡')}")
            
            result = self._execute_single_task(task)
            
            # å¤„ç†ä»»åŠ¡ç»“æœ
            self._process_task_result(task, result)
            
            # å®šæœŸæ‰§è¡Œåƒåœ¾å›æ”¶
            if i % 3 == 0:
                gc.collect()
                log_info(f"ğŸ§¹ æ‰§è¡Œåƒåœ¾å›æ”¶ ({i}/{total_tasks} ä»»åŠ¡å·²å®Œæˆ)")
    
    def _execute_single_task(self, task):
        """æ‰§è¡Œå•ä¸ªåˆ†æä»»åŠ¡"""
        start_time = datetime.now()
        result = None
        
        try:
            result = task["func"](**task["args"])
            elapsed = (datetime.now() - start_time).total_seconds()
            log_info(f"    âœ… å®Œæˆåˆ†æ: {task['name']} (è€—æ—¶: {elapsed:.2f} ç§’)", show_memory=True)
            
        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            log_info(f"    âŒ ä»»åŠ¡å¤±è´¥: {task['name']} (è€—æ—¶: {elapsed:.2f} ç§’)", level="ERROR")
            log_info(f"    é”™è¯¯è¯¦æƒ…: {str(e)}", level="ERROR")
            result = None
            
        return result
    
    def _add_slow_api_analysis_tasks(self, analysis_tasks, top_5_slowest, temp_csv, output_dir):
        """ä¸ºæœ€æ…¢æ¥å£æ·»åŠ ä¸“é—¨çš„æ—¶é—´ç»´åº¦åˆ†æä»»åŠ¡"""
        log_info("ğŸŒ ä¸ºæœ€æ…¢æ¥å£æ·»åŠ ä¸“é—¨åˆ†æä»»åŠ¡...")
        
        for i, row in top_5_slowest.iterrows():
            slow_api = row['è¯·æ±‚URI']
            # æˆªå–URIï¼Œé¿å…æ–‡ä»¶åè¿‡é•¿
            safe_api_name = slow_api.replace("/", "_").replace("?", "_").replace("&", "_")[:30]
            specific_api_output = os.path.join(output_dir, f"05_02.æ—¶é—´ç»´åº¦åˆ†æ-æ…¢æ¥å£-{safe_api_name}.xlsx")
            
            log_info(f"    ğŸ“Œ æ·»åŠ æ…¢æ¥å£åˆ†æ: {slow_api}")
            analysis_tasks.append({
                "name": f"æ…¢æ¥å£æ—¶é—´ç»´åº¦åˆ†æ ({safe_api_name})",
                "priority": 5.5 + i * 0.1,  # æ’å…¥åˆ°æ—¶é—´åˆ†æä»»åŠ¡ä¹‹å
                "func": analyze_time_dimension,
                "args": {
                    "csv_path": temp_csv,
                    "output_path": specific_api_output,
                    "specific_uri_list": slow_api
                },
                "description": f"é’ˆå¯¹æ…¢æ¥å£ {slow_api[:50]} çš„æ·±åº¦æ—¶é—´åˆ†æ",
                "output_key": f"slow_api_analysis_{i}"
            })
        
        # æŒ‰ä¼˜å…ˆçº§é‡æ–°æ’åºä»»åŠ¡
        analysis_tasks.sort(key=lambda x: x.get('priority', 999))
    
    def _process_task_result(self, task, result):
        """å¤„ç†ä»»åŠ¡ç»“æœ"""
        task_name = task["name"]
        output_key = task.get('output_key', task_name.lower())
        
        # ä¿å­˜ç»“æœåˆ°outputs
        self.outputs[output_key] = result
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºç‰¹å®šçš„ç»“æœæ‘˜è¦
        if "APIæ€§èƒ½åˆ†æ" in task_name and result is not None and not result.empty:
            self._show_api_analysis_summary(result)
            
        elif "æœåŠ¡å±‚çº§åˆ†æ" in task_name and result is not None:
            log_info(f"    ğŸ“‹ æœåŠ¡å±‚çº§åˆ†æå®Œæˆ")
            
        elif "æ…¢è¯·æ±‚åˆ†æ" in task_name and result is not None and not result.empty:
            log_info(f"    ğŸŒ å‘ç° {len(result)} æ¡æ…¢è¯·æ±‚è®°å½•")
            
        elif "IPæ¥æºåˆ†æ" in task_name and result is not None and not result.empty:
            self._show_ip_analysis_summary(result)
            
        elif "è¯·æ±‚å¤´åˆ†æ" in task_name and result is not None and isinstance(result, dict):
            self._show_header_analysis_summary(result)
            
        elif "è¯·æ±‚å¤´æ€§èƒ½å…³è”åˆ†æ" in task_name and result is not None and isinstance(result, dict):
            self._show_header_performance_summary(result)
    
    def _show_api_analysis_summary(self, result):
        """æ˜¾ç¤ºAPIåˆ†ææ‘˜è¦"""
        log_info(f"    ğŸ“Š æœ€æ…¢çš„æ¥å£å‰5å:")
        for idx, row in result.iterrows():
            log_info(f"      {idx + 1}. {row['è¯·æ±‚URI']} (å¹³å‡å“åº”æ—¶é—´: {row['å¹³å‡è¯·æ±‚æ—¶é•¿(ç§’)']:.3f}ç§’)")
    
    def _show_ip_analysis_summary(self, result):
        """æ˜¾ç¤ºIPåˆ†ææ‘˜è¦"""
        log_info(f"    ğŸ“Š åˆ†æäº† {len(result)} ä¸ªIPåœ°å€")
        log_info(f"    ğŸ“Š IPè¯·æ±‚é‡å‰5åï¼š")
        for idx, row in result.iterrows():
            if idx >= 5:
                break
            risk_level = "ğŸ”´é«˜é£é™©" if row.get('é£é™©è¯„åˆ†', 0) > 70 else ("ğŸŸ¡ä¸­é£é™©" if row.get('é£é™©è¯„åˆ†', 0) > 50 else "ğŸŸ¢ä½é£é™©")
            log_info(f"      {idx + 1}. {row['IPåœ°å€']} (è¯·æ±‚æ•°: {row['æ€»è¯·æ±‚æ•°']}, {risk_level}: {row.get('é£é™©è¯„åˆ†', 0)})")
    
    def _show_header_analysis_summary(self, result):
        """æ˜¾ç¤ºè¯·æ±‚å¤´åˆ†ææ‘˜è¦"""
        log_info(f"    ğŸ“Š è¯·æ±‚å¤´åˆ†æå®Œæˆï¼š")
        log_info(f"      å”¯ä¸€User-Agent: {result.get('unique_user_agents', 0)} ä¸ª")
        log_info(f"      å”¯ä¸€Referer: {result.get('unique_referers', 0)} ä¸ª")
        
        if result.get('top_browsers'):
            log_info(f"      TOPæµè§ˆå™¨ï¼š")
            for browser, count in list(result['top_browsers'].items())[:3]:
                log_info(f"        - {browser}: {count}")
    
    def _show_header_performance_summary(self, result):
        """æ˜¾ç¤ºè¯·æ±‚å¤´æ€§èƒ½åˆ†ææ‘˜è¦"""
        log_info(f"    ğŸ“Š è¯·æ±‚å¤´æ€§èƒ½å…³è”åˆ†æå®Œæˆï¼š")
        log_info(f"      æ•´ä½“æ…¢è¯·æ±‚ç‡: {result.get('slow_rate_overall', 0)}%")
        
        if result.get('worst_browsers'):
            log_info(f"      æ€§èƒ½æœ€å·®æµè§ˆå™¨ï¼š")
            for browser_info in result['worst_browsers'][:3]:
                log_info(f"        - {browser_info['browser']}: æ…¢è¯·æ±‚ç‡ {browser_info['slow_rate']}%")
    
    def _generate_advanced_summary_report(self, summary_output):
        """ç”Ÿæˆé«˜çº§ç»¼åˆæŠ¥å‘Š"""
        log_info("ğŸ“‹ ç”Ÿæˆé«˜çº§ç»¼åˆæŠ¥å‘Š...")
        try:
            generate_advanced_summary_report(self.outputs, summary_output)
            log_info(f"âœ… é«˜çº§ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆï¼š{summary_output}")
        except Exception as e:
            log_info(f"âŒ ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {str(e)}", level="ERROR")
            log_info(traceback.format_exc(), level="ERROR")
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        log_info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for temp_file in self.temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    log_info(f"    ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file}")
            except Exception as e:
                log_info(f"    âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file}: {e}", level="WARNING")
    
    def _show_completion_summary(self, output_dir):
        """æ˜¾ç¤ºåˆ†æå®Œæˆæ‘˜è¦"""
        log_info("ğŸ¯ === é«˜çº§åˆ†æå®Œæˆæ‘˜è¦ ===")
        log_info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        log_info(f"ğŸ“Š æ€»å¤„ç†è®°å½•: {self.outputs.get('total_requests', 0):,}")
        log_info(f"â±ï¸ åˆ†æå¼€å§‹æ—¶é—´: {self.outputs.get('analysis_time', 'Unknown')}")
        log_info(f"ğŸš€ åˆ†æå™¨ç‰ˆæœ¬: {self.outputs.get('analyzer_version', 'Unknown')}")
        
        log_info("ğŸ”§ ä¼˜åŒ–ç‰¹æ€§:")
        for feature in self.outputs.get('optimization_features', []):
            log_info(f"  âœ… {feature}")
        
        # ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶
        if os.path.exists(output_dir):
            output_files = [f for f in os.listdir(output_dir) if f.endswith('.xlsx')]
            log_info(f"ğŸ“„ ç”Ÿæˆäº† {len(output_files)} ä¸ªåˆ†ææŠ¥å‘Š")
    
    def _final_cleanup(self):
        """æœ€ç»ˆæ¸…ç†"""
        gc.collect()
        log_info("ğŸ§¹ æ‰§è¡Œæœ€ç»ˆåƒåœ¾å›æ”¶", level="INFO")


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
def main():
    """ä¸»å‡½æ•° - å…¼å®¹æ¥å£"""
    analyzer = AdvancedNginxLogAnalyzer()
    analyzer.main()


# å¯åŠ¨é«˜çº§åˆ†æ
if __name__ == "__main__":
    main()