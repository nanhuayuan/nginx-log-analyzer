import os
import gc
import numpy as np
import openpyxl
import pandas as pd
from datetime import datetime, timedelta

from self_00_02_utils import log_info
from self_00_04_excel_processor import (
    add_dataframe_to_excel_with_grouped_headers,
    format_excel_sheet
)


def analyze_service_stability(csv_path, output_path, threshold=None):
    """åˆ†ææœåŠ¡ç¨³å®šæ€§æŒ‡æ ‡"""
    log_info("å¼€å§‹åˆ†ææœåŠ¡ç¨³å®šæ€§æŒ‡æ ‡...", show_memory=True)

    # é»˜è®¤é˜ˆå€¼é…ç½®
    if threshold is None:
        threshold = {
            'success_rate': 99.0,  # æˆåŠŸç‡é˜ˆå€¼
            'response_time': 0.5,  # å“åº”æ—¶é—´é˜ˆå€¼ï¼ˆç§’ï¼‰
            'error_rate': 1.0,  # é”™è¯¯ç‡é˜ˆå€¼
            'backend_efficiency': 60.0,  # åç«¯å¤„ç†æ•ˆç‡é˜ˆå€¼
            'network_overhead': 30.0,  # ç½‘ç»œå¼€é”€é˜ˆå€¼
            'transfer_speed': 1000.0  # ä¼ è¾“é€Ÿåº¦é˜ˆå€¼ï¼ˆKB/sï¼‰
        }

    # åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨
    data_collectors = {
        'success_rate': {},
        'response_time': {},
        'resource_usage': {},
        'request_frequency': {},
        'concurrency': [],
        'connection': {},
        'backend_performance': {},  # æ–°å¢ï¼šåç«¯æ€§èƒ½åˆ†æ
        'transfer_performance': {},  # æ–°å¢ï¼šä¼ è¾“æ€§èƒ½åˆ†æ
        'nginx_lifecycle': {}  # æ–°å¢ï¼šNginxç”Ÿå‘½å‘¨æœŸåˆ†æ
    }

    # åˆ†å—å¤„ç†æ•°æ®
    chunk_size = 100000
    total_records = process_data_chunks(csv_path, chunk_size, data_collectors)

    # ç”Ÿæˆæœ€ç»ˆåˆ†æç»“æœ
    outputs = generate_analysis_results(data_collectors, threshold)

    # ä¿å­˜åˆ°Excel
    save_performance_analysis_to_excel(outputs, output_path)

    log_info(f"æœåŠ¡ç¨³å®šæ€§åˆ†æå®Œæˆï¼Œå…±å¤„ç† {total_records} æ¡è®°å½•", show_memory=True)
    return outputs


def process_data_chunks(csv_path, chunk_size, data_collectors):
    """åˆ†å—å¤„ç†æ•°æ®æ–‡ä»¶"""
    log_info("æ­£åœ¨åˆ†å—è¯»å–æ•°æ®æ–‡ä»¶...", show_memory=True)

    chunks_processed = 0
    total_records = 0
    start_time = datetime.now()

    for chunk in pd.read_csv(csv_path, chunksize=chunk_size):
        chunks_processed += 1
        chunk_records = len(chunk)
        total_records += chunk_records

        # é¢„å¤„ç†æ—¶é—´æˆ³
        preprocess_timestamps(chunk)

        # å¤„ç†å„ç±»æŒ‡æ ‡
        process_success_rate_chunk(chunk, data_collectors['success_rate'])
        process_response_time_chunk(chunk, data_collectors['response_time'])
        process_resource_usage_chunk(chunk, data_collectors['resource_usage'])
        process_request_frequency_chunk(chunk, data_collectors['request_frequency'])
        process_concurrency_chunk(chunk, data_collectors['concurrency'])
        process_connection_chunk(chunk, data_collectors['connection'])

        # æ–°å¢çš„æ€§èƒ½åˆ†æ
        process_backend_performance_chunk(chunk, data_collectors['backend_performance'])
        process_transfer_performance_chunk(chunk, data_collectors['transfer_performance'])
        process_nginx_lifecycle_chunk(chunk, data_collectors['nginx_lifecycle'])

        # æ¸…ç†å†…å­˜
        del chunk
        gc.collect()

        # è¿›åº¦æ—¥å¿—
        elapsed = (datetime.now() - start_time).total_seconds()
        log_info(f"å·²å¤„ç† {chunks_processed} ä¸ªæ•°æ®å—, {total_records} æ¡è®°å½•, è€—æ—¶: {elapsed:.2f}ç§’", show_memory=True)

    return total_records


def preprocess_timestamps(chunk):
    """é¢„å¤„ç†æ—¶é—´æˆ³å­—æ®µ"""
    # å¤„ç†åŸå§‹æ—¶é—´å­—æ®µ
    if 'raw_time' in chunk.columns:
        chunk['time'] = pd.to_datetime(chunk['raw_time'])
        chunk['hour_bucket'] = chunk['time'].dt.floor('H')
        chunk['minute_bucket'] = chunk['time'].dt.floor('min')

    # å¤„ç†åˆ°è¾¾æ—¶é—´å­—æ®µ
    if 'arrival_time' in chunk.columns:
        chunk['arrival_time'] = pd.to_datetime(chunk['arrival_time'], errors='coerce')
    elif 'arrival_timestamp' in chunk.columns:
        chunk['arrival_timestamp'] = pd.to_numeric(chunk['arrival_timestamp'], errors='coerce')
        chunk['arrival_time'] = pd.to_datetime(chunk['arrival_timestamp'], unit='s', errors='coerce')


def process_success_rate_chunk(chunk, success_rate_data):
    """å¤„ç†æˆåŠŸç‡æ•°æ®å—"""
    for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
        # ç»Ÿè®¡2xxçŠ¶æ€ç ä¸ºæˆåŠŸ
        success_count = group['response_status_code'].astype(str).str.startswith('2').sum()
        total_count = len(group)

        if (hour, service) not in success_rate_data:
            success_rate_data[(hour, service)] = {'success': 0, 'total': 0}

        success_rate_data[(hour, service)]['success'] += success_count
        success_rate_data[(hour, service)]['total'] += total_count


def process_response_time_chunk(chunk, response_time_data):
    """å¤„ç†å“åº”æ—¶é—´æ•°æ®å—"""
    if 'total_request_duration' not in chunk.columns:
        return

    for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
        if (hour, service) not in response_time_data:
            response_time_data[(hour, service)] = {
                'sum': 0, 'count': 0, 'min': float('inf'), 'max': float('-inf'),
                'values': []  # ç”¨äºè®¡ç®—ç™¾åˆ†ä½æ•°
            }

        data = response_time_data[(hour, service)]
        request_times = group['total_request_duration'].dropna()

        if len(request_times) > 0:
            data['sum'] += request_times.sum()
            data['count'] += len(request_times)
            data['min'] = min(data['min'], request_times.min())
            data['max'] = max(data['max'], request_times.max())
            # ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œåªä¿ç•™éƒ¨åˆ†å€¼ç”¨äºç™¾åˆ†ä½æ•°è®¡ç®—
            if len(data['values']) < 10000:
                data['values'].extend(request_times.tolist())


def process_resource_usage_chunk(chunk, resource_usage_data):
    """å¤„ç†èµ„æºä½¿ç”¨æ•°æ®å— - ä½¿ç”¨KBå•ä½"""
    required_cols = ['response_body_size_kb', 'total_bytes_sent_kb', 'http_method']
    if not all(col in chunk.columns for col in required_cols):
        return

    for (service, method), group in chunk.groupby(['service_name', 'http_method']):
        if (service, method) not in resource_usage_data:
            resource_usage_data[(service, method)] = {
                'response_kb_sum': 0, 'total_kb_sum': 0, 'count': 0
            }

        data = resource_usage_data[(service, method)]
        data['response_kb_sum'] += group['response_body_size_kb'].sum()
        data['total_kb_sum'] += group['total_bytes_sent_kb'].sum()
        data['count'] += len(group)


def process_request_frequency_chunk(chunk, request_frequency_data):
    """å¤„ç†è¯·æ±‚é¢‘ç‡æ•°æ®å—"""
    for (minute, service), count in chunk.groupby(['minute_bucket', 'service_name']).size().items():
        if service not in request_frequency_data:
            request_frequency_data[service] = {'counts': [], 'total': 0}

        request_frequency_data[service]['counts'].append(count)
        request_frequency_data[service]['total'] += count


def process_concurrency_chunk(chunk, concurrency_data):
    """å¤„ç†å¹¶å‘æ•°æ®å—"""
    valid_requests = chunk.dropna(subset=['arrival_time', 'total_request_duration'])

    for _, row in valid_requests.iterrows():
        arrival_ts = row['arrival_time']
        duration = row['total_request_duration']
        end_ts = arrival_ts + pd.Timedelta(seconds=duration)

        if pd.notna(arrival_ts) and pd.notna(end_ts):
            concurrency_data.append((arrival_ts, 1))  # è¯·æ±‚å¼€å§‹
            concurrency_data.append((end_ts, -1))  # è¯·æ±‚ç»“æŸ


def process_connection_chunk(chunk, connection_data):
    """å¤„ç†è¿æ¥æ•°æ®å—"""
    if 'connection_cost_ratio' not in chunk.columns:
        return

    for minute, group in chunk.groupby('minute_bucket'):
        if minute not in connection_data:
            connection_data[minute] = {
                'request_count': 0,
                'connection_cost_sum': 0.0,
                'avg_connection_cost': 0.0
            }

        data = connection_data[minute]
        data['request_count'] += len(group)
        data['connection_cost_sum'] += group['connection_cost_ratio'].sum()


def process_backend_performance_chunk(chunk, backend_performance_data):
    """å¤„ç†åç«¯æ€§èƒ½æ•°æ®å— - åŸºäºnginxç”Ÿå‘½å‘¨æœŸåˆ†æ"""
    required_cols = ['backend_efficiency', 'processing_efficiency_index',
                     'backend_connect_phase', 'backend_process_phase', 'backend_transfer_phase']

    if not all(col in chunk.columns for col in required_cols):
        return

    for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
        if (hour, service) not in backend_performance_data:
            backend_performance_data[(hour, service)] = {
                'efficiency_sum': 0, 'processing_index_sum': 0,
                'connect_time_sum': 0, 'process_time_sum': 0, 'transfer_time_sum': 0,
                'count': 0
            }

        data = backend_performance_data[(hour, service)]
        valid_group = group.dropna(subset=required_cols)

        if len(valid_group) > 0:
            data['efficiency_sum'] += valid_group['backend_efficiency'].sum()
            data['processing_index_sum'] += valid_group['processing_efficiency_index'].sum()
            data['connect_time_sum'] += valid_group['backend_connect_phase'].sum()
            data['process_time_sum'] += valid_group['backend_process_phase'].sum()
            data['transfer_time_sum'] += valid_group['backend_transfer_phase'].sum()
            data['count'] += len(valid_group)


def process_transfer_performance_chunk(chunk, transfer_performance_data):
    """å¤„ç†ä¼ è¾“æ€§èƒ½æ•°æ®å—"""
    required_cols = ['response_transfer_speed', 'total_transfer_speed', 'nginx_transfer_speed']

    if not all(col in chunk.columns for col in required_cols):
        return

    for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
        if (hour, service) not in transfer_performance_data:
            transfer_performance_data[(hour, service)] = {
                'response_speed_sum': 0, 'total_speed_sum': 0, 'nginx_speed_sum': 0,
                'count': 0
            }

        data = transfer_performance_data[(hour, service)]
        valid_group = group.dropna(subset=required_cols)

        if len(valid_group) > 0:
            data['response_speed_sum'] += valid_group['response_transfer_speed'].sum()
            data['total_speed_sum'] += valid_group['total_transfer_speed'].sum()
            data['nginx_speed_sum'] += valid_group['nginx_transfer_speed'].sum()
            data['count'] += len(valid_group)


def process_nginx_lifecycle_chunk(chunk, nginx_lifecycle_data):
    """å¤„ç†Nginxç”Ÿå‘½å‘¨æœŸæ•°æ®å—"""
    required_cols = ['network_overhead', 'transfer_ratio', 'nginx_transfer_phase']

    if not all(col in chunk.columns for col in required_cols):
        return

    for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
        if (hour, service) not in nginx_lifecycle_data:
            nginx_lifecycle_data[(hour, service)] = {
                'network_overhead_sum': 0, 'transfer_ratio_sum': 0,
                'nginx_phase_sum': 0, 'count': 0
            }

        data = nginx_lifecycle_data[(hour, service)]
        valid_group = group.dropna(subset=required_cols)

        if len(valid_group) > 0:
            data['network_overhead_sum'] += valid_group['network_overhead'].sum()
            data['transfer_ratio_sum'] += valid_group['transfer_ratio'].sum()
            data['nginx_phase_sum'] += valid_group['nginx_transfer_phase'].sum()
            data['count'] += len(valid_group)


def generate_analysis_results(data_collectors, threshold):
    """ç”Ÿæˆæœ€ç»ˆåˆ†æç»“æœ"""
    log_info("æ­£åœ¨è®¡ç®—æœ€ç»ˆåˆ†æç»“æœ...", show_memory=True)

    outputs = {}

    # åŸºç¡€ç¨³å®šæ€§æŒ‡æ ‡
    outputs['æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§'] = finalize_success_rate_analysis(
        data_collectors['success_rate'], threshold)
    outputs['æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§'] = finalize_response_time_analysis(
        data_collectors['response_time'], threshold)
    outputs['èµ„æºä½¿ç”¨å’Œå¸¦å®½'] = finalize_resource_usage_analysis(
        data_collectors['resource_usage'])
    outputs['æœåŠ¡è¯·æ±‚é¢‘ç‡'] = finalize_request_frequency_analysis(
        data_collectors['request_frequency'])

    # é«˜çº§åˆ†ææŒ‡æ ‡
    if data_collectors['concurrency']:
        outputs['å¹¶å‘è¿æ¥ä¼°ç®—'] = finalize_concurrency_analysis(
            data_collectors['concurrency'])

    if data_collectors['connection']:
        connection_metrics, connection_summary = finalize_connections_analysis(
            data_collectors['connection'])
        outputs['è¿æ¥æ€§èƒ½æŒ‡æ ‡'] = connection_metrics
        outputs['è¿æ¥æ€§èƒ½æ‘˜è¦'] = connection_summary

    # æ–°å¢çš„æ€§èƒ½åˆ†æ
    outputs['åç«¯å¤„ç†æ€§èƒ½'] = finalize_backend_performance_analysis(
        data_collectors['backend_performance'], threshold)
    outputs['æ•°æ®ä¼ è¾“æ€§èƒ½'] = finalize_transfer_performance_analysis(
        data_collectors['transfer_performance'], threshold)
    outputs['Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ'] = finalize_nginx_lifecycle_analysis(
        data_collectors['nginx_lifecycle'], threshold)

    return outputs


def finalize_success_rate_analysis(success_rate_data, threshold):
    """å®ŒæˆæˆåŠŸç‡åˆ†æ"""
    hourly_rates = []
    for (hour, service), data in success_rate_data.items():
        success_rate = (data['success'] / data['total'] * 100) if data['total'] > 0 else 0
        hourly_rates.append({
            'æ—¶é—´': hour,
            'æœåŠ¡åç§°': service,
            'æˆåŠŸç‡(%)': success_rate,
            'æˆåŠŸè¯·æ±‚æ•°': data['success'],
            'æ€»è¯·æ±‚æ•°': data['total']
        })

    if not hourly_rates:
        return pd.DataFrame()

    hourly_df = pd.DataFrame(hourly_rates)

    # æŒ‰æœåŠ¡èšåˆç»Ÿè®¡
    stability_stats = []
    for service, service_data in hourly_df.groupby('æœåŠ¡åç§°'):
        mean_rate = service_data['æˆåŠŸç‡(%)'].mean()
        std_rate = service_data['æˆåŠŸç‡(%)'].std() if len(service_data) > 1 else 0
        min_rate = service_data['æˆåŠŸç‡(%)'].min()
        max_rate = service_data['æˆåŠŸç‡(%)'].max()

        # å¼‚å¸¸åˆ¤æ–­
        status = 'æ­£å¸¸'
        if mean_rate < threshold['success_rate']:
            status = 'æˆåŠŸç‡ä½'
        elif std_rate > 5.0:
            status = 'æ³¢åŠ¨è¾ƒå¤§'

        stability_stats.append({
            'æœåŠ¡åç§°': service,
            'å¹³å‡æˆåŠŸç‡(%)': round(mean_rate, 2),
            'æˆåŠŸç‡æ³¢åŠ¨(æ ‡å‡†å·®)': round(std_rate, 2),
            'æœ€ä½æˆåŠŸç‡(%)': round(min_rate, 2),
            'æœ€é«˜æˆåŠŸç‡(%)': round(max_rate, 2),
            'æ€»è¯·æ±‚æ•°': service_data['æ€»è¯·æ±‚æ•°'].sum(),
            'å¼‚å¸¸çŠ¶æ€': status
        })

    del hourly_df
    gc.collect()

    return pd.DataFrame(stability_stats).sort_values('æˆåŠŸç‡æ³¢åŠ¨(æ ‡å‡†å·®)', ascending=False)


def finalize_response_time_analysis(response_time_data, threshold):
    """å®Œæˆå“åº”æ—¶é—´åˆ†æ"""
    response_stats = []

    for (hour, service), data in response_time_data.items():
        if data['count'] > 0:
            mean_time = data['sum'] / data['count']

            # è®¡ç®—ç™¾åˆ†ä½æ•°ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼‰
            p95_time = p99_time = mean_time
            if data['values']:
                p95_time = np.percentile(data['values'], 95)
                p99_time = np.percentile(data['values'], 99)

            response_stats.append({
                'æ—¶é—´': hour,
                'æœåŠ¡åç§°': service,
                'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(mean_time, 3),
                'æœ€ä½å“åº”æ—¶é—´(ç§’)': round(data['min'], 3),
                'æœ€é«˜å“åº”æ—¶é—´(ç§’)': round(data['max'], 3),
                'P95å“åº”æ—¶é—´(ç§’)': round(p95_time, 3),
                'P99å“åº”æ—¶é—´(ç§’)': round(p99_time, 3),
                'è¯·æ±‚æ•°é‡': data['count']
            })

    if not response_stats:
        return pd.DataFrame()

    hourly_df = pd.DataFrame(response_stats)

    # æŒ‰æœåŠ¡èšåˆ
    service_stats = []
    for service, service_data in hourly_df.groupby('æœåŠ¡åç§°'):
        mean_time = service_data['å¹³å‡å“åº”æ—¶é—´(ç§’)'].mean()
        std_time = service_data['å¹³å‡å“åº”æ—¶é—´(ç§’)'].std() if len(service_data) > 1 else 0
        min_time = service_data['æœ€ä½å“åº”æ—¶é—´(ç§’)'].min()
        max_time = service_data['æœ€é«˜å“åº”æ—¶é—´(ç§’)'].max()

        # å¼‚å¸¸åˆ¤æ–­
        status = 'æ­£å¸¸'
        if mean_time > threshold['response_time']:
            status = 'å“åº”æ—¶é—´é•¿'
        elif std_time > mean_time * 0.5:  # æ³¢åŠ¨è¶…è¿‡å‡å€¼çš„50%
            status = 'å“åº”ä¸ç¨³å®š'

        service_stats.append({
            'æœåŠ¡åç§°': service,
            'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(mean_time, 3),
            'å“åº”æ—¶é—´æ³¢åŠ¨(æ ‡å‡†å·®)': round(std_time, 3),
            'æœ€ä½å“åº”æ—¶é—´(ç§’)': round(min_time, 3),
            'æœ€é«˜å“åº”æ—¶é—´(ç§’)': round(max_time, 3),
            'P95å“åº”æ—¶é—´(ç§’)': round(service_data['P95å“åº”æ—¶é—´(ç§’)'].mean(), 3),
            'P99å“åº”æ—¶é—´(ç§’)': round(service_data['P99å“åº”æ—¶é—´(ç§’)'].mean(), 3),
            'æ€»è¯·æ±‚æ•°': service_data['è¯·æ±‚æ•°é‡'].sum(),
            'å¼‚å¸¸çŠ¶æ€': status
        })

    del hourly_df
    gc.collect()

    return pd.DataFrame(service_stats).sort_values('å¹³å‡å“åº”æ—¶é—´(ç§’)', ascending=False)


def finalize_resource_usage_analysis(resource_usage_data):
    """å®Œæˆèµ„æºä½¿ç”¨åˆ†æ - åŸºäºKBå•ä½"""
    resource_results = []

    for (service, method), data in resource_usage_data.items():
        avg_response_kb = data['response_kb_sum'] / data['count'] if data['count'] > 0 else 0
        avg_total_kb = data['total_kb_sum'] / data['count'] if data['count'] > 0 else 0
        total_response_mb = data['response_kb_sum'] / 1024  # è½¬æ¢ä¸ºMB
        total_transfer_mb = data['total_kb_sum'] / 1024  # è½¬æ¢ä¸ºMB

        resource_results.append({
            'æœåŠ¡åç§°': service,
            'è¯·æ±‚æ–¹æ³•': method,
            'å¹³å‡å“åº”å¤§å°(KB)': round(avg_response_kb, 2),
            'å¹³å‡ä¼ è¾“å¤§å°(KB)': round(avg_total_kb, 2),
            'æ€»å“åº”æµé‡(MB)': round(total_response_mb, 2),
            'æ€»ä¼ è¾“æµé‡(MB)': round(total_transfer_mb, 2),
            'è¯·æ±‚æ¬¡æ•°': data['count']
        })

    return pd.DataFrame(resource_results).sort_values('æ€»ä¼ è¾“æµé‡(MB)', ascending=False)


def finalize_request_frequency_analysis(request_frequency_data):
    """å®Œæˆè¯·æ±‚é¢‘ç‡åˆ†æ"""
    frequency_results = []

    for service, data in request_frequency_data.items():
        counts = data['counts']
        if counts:
            frequency_results.append({
                'æœåŠ¡åç§°': service,
                'å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚æ•°(QPS)': round(np.mean(counts), 2),
                'æœ€å¤§æ¯åˆ†é’Ÿè¯·æ±‚æ•°(å³°å€¼QPS)': np.max(counts),
                'æœ€å°æ¯åˆ†é’Ÿè¯·æ±‚æ•°': np.min(counts),
                'è¯·æ±‚é¢‘ç‡æ³¢åŠ¨(æ ‡å‡†å·®)': round(np.std(counts), 2),
                'æ€»è¯·æ±‚æ•°': data['total']
            })

    return pd.DataFrame(frequency_results).sort_values('å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚æ•°(QPS)', ascending=False)


def finalize_concurrency_analysis(concurrency_data):
    """å®Œæˆå¹¶å‘åˆ†æ"""
    # æŒ‰æ—¶é—´æ’åºå¹¶è®¡ç®—å¹¶å‘æ•°
    concurrency_data.sort(key=lambda x: x[0])

    concurrent_requests = []
    current_count = 0

    for ts, event in concurrency_data:
        current_count += event
        concurrent_requests.append((ts, current_count))

    concurrent_df = pd.DataFrame(concurrent_requests, columns=['æ—¶é—´æˆ³', 'å¹¶å‘æ•°'])
    concurrent_df['åˆ†é’Ÿæ—¶é—´æ®µ'] = concurrent_df['æ—¶é—´æˆ³'].dt.floor('min')

    # æŒ‰åˆ†é’Ÿèšåˆç»Ÿè®¡
    concurrency_stats = concurrent_df.groupby('åˆ†é’Ÿæ—¶é—´æ®µ').agg(
        å¹³å‡å¹¶å‘æ•°=('å¹¶å‘æ•°', 'mean'),
        æœ€å¤§å¹¶å‘æ•°=('å¹¶å‘æ•°', 'max'),
        æœ€å°å¹¶å‘æ•°=('å¹¶å‘æ•°', 'min')
    ).reset_index()

    concurrency_stats.rename(columns={'åˆ†é’Ÿæ—¶é—´æ®µ': 'æ—¶é—´æ®µ'}, inplace=True)

    # å››èˆäº”å…¥
    concurrency_stats['å¹³å‡å¹¶å‘æ•°'] = concurrency_stats['å¹³å‡å¹¶å‘æ•°'].round(2)

    del concurrent_df, concurrent_requests
    gc.collect()

    return concurrency_stats


def finalize_connections_analysis(connection_data):
    """å®Œæˆè¿æ¥åˆ†æ"""
    connection_metrics = []

    for minute, data in connection_data.items():
        avg_connection_cost = (data['connection_cost_sum'] / data['request_count']
                               if data['request_count'] > 0 else 0)

        connection_metrics.append({
            'æ—¶é—´': minute,
            'è¯·æ±‚æ•°é‡': data['request_count'],
            'å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡': round(avg_connection_cost, 4),
            'æ€»è¿æ¥æˆæœ¬': round(data['connection_cost_sum'], 2)
        })

    connection_df = pd.DataFrame(connection_metrics)

    # ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
    connection_summary = {
        'å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚æ•°': round(connection_df['è¯·æ±‚æ•°é‡'].mean(), 2),
        'æœ€å¤§æ¯åˆ†é’Ÿè¯·æ±‚æ•°': connection_df['è¯·æ±‚æ•°é‡'].max(),
        'å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡': round(connection_df['å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡'].mean(), 4),
        'æœ€é«˜è¿æ¥æˆæœ¬æ¯”ç‡': round(connection_df['å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡'].max(), 4),
        'è¿æ¥æˆæœ¬æ³¢åŠ¨(æ ‡å‡†å·®)': round(connection_df['å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡'].std(), 4)
    }

    return connection_df, connection_summary


def finalize_backend_performance_analysis(backend_performance_data, threshold):
    """å®Œæˆåç«¯æ€§èƒ½åˆ†æ"""
    backend_stats = []

    for (hour, service), data in backend_performance_data.items():
        if data['count'] > 0:
            avg_efficiency = data['efficiency_sum'] / data['count']
            avg_processing_index = data['processing_index_sum'] / data['count']
            avg_connect_time = data['connect_time_sum'] / data['count']
            avg_process_time = data['process_time_sum'] / data['count']
            avg_transfer_time = data['transfer_time_sum'] / data['count']

            # æ€§èƒ½çŠ¶æ€åˆ¤æ–­
            status = 'æ­£å¸¸'
            if avg_efficiency < threshold['backend_efficiency']:
                status = 'å¤„ç†æ•ˆç‡ä½'
            elif avg_connect_time > 0.1:  # è¿æ¥æ—¶é—´è¶…è¿‡100ms
                status = 'è¿æ¥å»¶è¿Ÿé«˜'

            backend_stats.append({
                'æ—¶é—´': hour,
                'æœåŠ¡åç§°': service,
                'åç«¯å¤„ç†æ•ˆç‡(%)': round(avg_efficiency, 2),
                'å¤„ç†æ•ˆç‡æŒ‡æ•°': round(avg_processing_index, 3),
                'å¹³å‡è¿æ¥æ—¶é—´(ç§’)': round(avg_connect_time, 3),
                'å¹³å‡å¤„ç†æ—¶é—´(ç§’)': round(avg_process_time, 3),
                'å¹³å‡ä¼ è¾“æ—¶é—´(ç§’)': round(avg_transfer_time, 3),
                'è¯·æ±‚æ•°é‡': data['count'],
                'æ€§èƒ½çŠ¶æ€': status
            })

    return pd.DataFrame(backend_stats).sort_values('åç«¯å¤„ç†æ•ˆç‡(%)', ascending=True)


def finalize_transfer_performance_analysis(transfer_performance_data, threshold):
    """å®Œæˆä¼ è¾“æ€§èƒ½åˆ†æ"""
    transfer_stats = []

    for (hour, service), data in transfer_performance_data.items():
        if data['count'] > 0:
            avg_response_speed = data['response_speed_sum'] / data['count']
            avg_total_speed = data['total_speed_sum'] / data['count']
            avg_nginx_speed = data['nginx_speed_sum'] / data['count']

            # ä¼ è¾“æ€§èƒ½çŠ¶æ€åˆ¤æ–­
            status = 'æ­£å¸¸'
            if avg_total_speed < threshold['transfer_speed']:
                status = 'ä¼ è¾“é€Ÿåº¦æ…¢'
            elif avg_nginx_speed < avg_response_speed * 0.8:
                status = 'Nginxä¼ è¾“ç“¶é¢ˆ'

            transfer_stats.append({
                'æ—¶é—´': hour,
                'æœåŠ¡åç§°': service,
                'å“åº”ä¼ è¾“é€Ÿåº¦(KB/s)': round(avg_response_speed, 2),
                'æ€»ä¼ è¾“é€Ÿåº¦(KB/s)': round(avg_total_speed, 2),
                'Nginxä¼ è¾“é€Ÿåº¦(KB/s)': round(avg_nginx_speed, 2),
                'è¯·æ±‚æ•°é‡': data['count'],
                'ä¼ è¾“çŠ¶æ€': status
            })

    return pd.DataFrame(transfer_stats).sort_values('æ€»ä¼ è¾“é€Ÿåº¦(KB/s)', ascending=True)


def finalize_nginx_lifecycle_analysis(nginx_lifecycle_data, threshold):
    """å®ŒæˆNginxç”Ÿå‘½å‘¨æœŸåˆ†æ"""
    lifecycle_stats = []

    for (hour, service), data in nginx_lifecycle_data.items():
        if data['count'] > 0:
            avg_network_overhead = data['network_overhead_sum'] / data['count']
            avg_transfer_ratio = data['transfer_ratio_sum'] / data['count']
            avg_nginx_phase = data['nginx_phase_sum'] / data['count']

            # ç”Ÿå‘½å‘¨æœŸçŠ¶æ€åˆ¤æ–­
            status = 'æ­£å¸¸'
            if avg_network_overhead > threshold['network_overhead']:
                status = 'ç½‘ç»œå¼€é”€é«˜'
            elif avg_transfer_ratio > 60.0:  # ä¼ è¾“æ—¶é—´å æ¯”è¶…è¿‡60%
                status = 'ä¼ è¾“æ—¶é—´å æ¯”é«˜'

            lifecycle_stats.append({
                'æ—¶é—´': hour,
                'æœåŠ¡åç§°': service,
                'ç½‘ç»œå¼€é”€å æ¯”(%)': round(avg_network_overhead, 2),
                'ä¼ è¾“æ—¶é—´å æ¯”(%)': round(avg_transfer_ratio, 2),
                'å¹³å‡Nginxä¼ è¾“é˜¶æ®µ(ç§’)': round(avg_nginx_phase, 3),
                'è¯·æ±‚æ•°é‡': data['count'],
                'ç”Ÿå‘½å‘¨æœŸçŠ¶æ€': status
            })

    return pd.DataFrame(lifecycle_stats).sort_values('ç½‘ç»œå¼€é”€å æ¯”(%)', ascending=False)


def save_performance_analysis_to_excel(outputs, output_path):
    log_info(f"æ­£åœ¨ä¿å­˜æœåŠ¡ç¨³å®šæ€§åˆ†æåˆ°Excel: {output_path}", show_memory=True)

    wb = openpyxl.Workbook()
    if 'Sheet' in wb.sheetnames:
        wb.remove(wb['Sheet'])

    # å®šä¹‰å·¥ä½œè¡¨ä¿¡æ¯å’Œé«˜äº®è§„åˆ™
    sheet_info = {
        'æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§': {
            'data': outputs.get('æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§'),
            'highlight_column': 'å¼‚å¸¸çŠ¶æ€',
            'highlight_values': {'æˆåŠŸç‡ä½': 'FF0000', 'æ³¢åŠ¨è¾ƒå¤§': 'FFA500'}
        },
        'æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§': {
            'data': outputs.get('æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§'),
            'highlight_column': 'å¼‚å¸¸çŠ¶æ€',
            'highlight_values': {'å“åº”æ—¶é—´é•¿': 'FF0000', 'å“åº”ä¸ç¨³å®š': 'FFA500'}
        },
        'èµ„æºä½¿ç”¨å’Œå¸¦å®½': {
            'data': outputs.get('èµ„æºä½¿ç”¨å’Œå¸¦å®½')
        },
        'æœåŠ¡è¯·æ±‚é¢‘ç‡': {
            'data': outputs.get('æœåŠ¡è¯·æ±‚é¢‘ç‡')
        },
        'åç«¯å¤„ç†æ€§èƒ½': {
            'data': outputs.get('åç«¯å¤„ç†æ€§èƒ½'),
            'highlight_column': 'æ€§èƒ½çŠ¶æ€',
            'highlight_values': {'å¤„ç†æ•ˆç‡ä½': 'FF0000', 'è¿æ¥å»¶è¿Ÿé«˜': 'FFA500'}
        },
        'æ•°æ®ä¼ è¾“æ€§èƒ½': {
            'data': outputs.get('æ•°æ®ä¼ è¾“æ€§èƒ½'),
            'highlight_column': 'ä¼ è¾“çŠ¶æ€',
            'highlight_values': {'ä¼ è¾“é€Ÿåº¦æ…¢': 'FF0000', 'Nginxä¼ è¾“ç“¶é¢ˆ': 'FFA500'}
        },
        'Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ': {
            'data': outputs.get('Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ'),
            'highlight_column': 'ç”Ÿå‘½å‘¨æœŸçŠ¶æ€',
            'highlight_values': {'ç½‘ç»œå¼€é”€é«˜': 'FF0000', 'ä¼ è¾“æ—¶é—´å æ¯”é«˜': 'FFA500'}
        }
    }

    # æ·»åŠ å¹¶å‘è¿æ¥ä¼°ç®—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if outputs.get('å¹¶å‘è¿æ¥ä¼°ç®—') is not None:
        sheet_info['å¹¶å‘è¿æ¥ä¼°ç®—'] = {'data': outputs['å¹¶å‘è¿æ¥ä¼°ç®—']}

    # æ·»åŠ è¿æ¥æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if outputs.get('è¿æ¥æ€§èƒ½æŒ‡æ ‡') is not None:
        sheet_info['è¿æ¥æ€§èƒ½æŒ‡æ ‡'] = {'data': outputs['è¿æ¥æ€§èƒ½æŒ‡æ ‡']}

    # åˆ›å»ºå„ä¸ªå·¥ä½œè¡¨
    for sheet_name, info in sheet_info.items():
        if info['data'] is not None and not info['data'].empty:
            ws = add_dataframe_to_excel_with_grouped_headers(wb, info['data'], sheet_name)

            # åº”ç”¨æ¡ä»¶æ ¼å¼é«˜äº®
            if 'highlight_column' in info and 'highlight_values' in info:
                apply_highlighting(ws, info['data'], info['highlight_column'], info['highlight_values'])

            # æ ¼å¼åŒ–å·¥ä½œè¡¨
            format_excel_sheet(ws)
            gc.collect()

    # æ·»åŠ è¿æ¥æ€§èƒ½æ‘˜è¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if outputs.get('è¿æ¥æ€§èƒ½æ‘˜è¦') is not None:
        add_summary_sheet(wb, outputs['è¿æ¥æ€§èƒ½æ‘˜è¦'], 'è¿æ¥æ€§èƒ½æ‘˜è¦')

    # æ·»åŠ æ•´ä½“æ€§èƒ½æ‘˜è¦
    add_overall_performance_summary(wb, outputs)

    wb.save(output_path)
    log_info(f"æœåŠ¡ç¨³å®šæ€§åˆ†æå·²ä¿å­˜åˆ°: {output_path}", show_memory=True)


def apply_highlighting(ws, df, highlight_column, highlight_values):
    """åº”ç”¨æ¡ä»¶æ ¼å¼é«˜äº®æ˜¾ç¤º"""
    from openpyxl.styles import Font, PatternFill

    if highlight_column not in df.columns:
        return

    col_idx = list(df.columns).index(highlight_column) + 1

    for r, row in enumerate(df.itertuples(index=False), start=2):
        try:
            # å¤„ç†åˆ—åä¸­çš„ç‰¹æ®Šå­—ç¬¦
            clean_column = highlight_column.replace(' ', '_').replace('(', '').replace(')', '').replace('%', '')
            cell_value = getattr(row, clean_column, None)

            if cell_value in highlight_values:
                cell = ws.cell(row=r, column=col_idx)
                # ä½¿ç”¨èƒŒæ™¯è‰²è€Œä¸æ˜¯å­—ä½“è‰²ï¼Œæ›´æ˜æ˜¾
                cell.fill = PatternFill(start_color=highlight_values[cell_value],
                                        end_color=highlight_values[cell_value],
                                        fill_type='solid')
                cell.font = Font(bold=True)
        except AttributeError:
            continue


def add_summary_sheet(wb, summary_data, sheet_name):
    """æ·»åŠ æ‘˜è¦å·¥ä½œè¡¨"""
    from openpyxl.styles import Font, Alignment

    ws = wb.create_sheet(title=sheet_name)

    # è®¾ç½®æ ‡é¢˜è¡Œ
    ws.cell(row=1, column=1, value='æ€§èƒ½æŒ‡æ ‡').font = Font(bold=True, size=12)
    ws.cell(row=1, column=2, value='æ•°å€¼').font = Font(bold=True, size=12)

    # å¡«å……æ•°æ®
    for r, (metric, value) in enumerate(summary_data.items(), start=2):
        ws.cell(row=r, column=1, value=metric)
        ws.cell(row=r, column=2, value=value)

        # è®¾ç½®å¯¹é½æ–¹å¼
        ws.cell(row=r, column=1).alignment = Alignment(horizontal='left')
        ws.cell(row=r, column=2).alignment = Alignment(horizontal='right')

    # è°ƒæ•´åˆ—å®½
    ws.column_dimensions['A'].width = 25
    ws.column_dimensions['B'].width = 15

    format_excel_sheet(ws)


def add_overall_performance_summary(wb, outputs):
    """æ·»åŠ æ•´ä½“æ€§èƒ½æ‘˜è¦å·¥ä½œè¡¨"""
    from openpyxl.styles import Font, Alignment, PatternFill

    ws = wb.create_sheet(title='æ•´ä½“æ€§èƒ½æ‘˜è¦')

    # æ ‡é¢˜
    ws.cell(row=1, column=1, value='NginxæœåŠ¡æ€§èƒ½åˆ†ææ‘˜è¦').font = Font(bold=True, size=14)
    ws.merge_cells('A1:D1')

    current_row = 3

    # 1. æœåŠ¡æˆåŠŸç‡æ‘˜è¦
    if outputs.get('æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§') is not None and not outputs['æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§'].empty:
        success_df = outputs['æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§']
        ws.cell(row=current_row, column=1, value='ğŸ“Š æœåŠ¡æˆåŠŸç‡æ¦‚è§ˆ').font = Font(bold=True, size=12)
        current_row += 1

        avg_success_rate = success_df['å¹³å‡æˆåŠŸç‡(%)'].mean()
        min_success_rate = success_df['å¹³å‡æˆåŠŸç‡(%)'].min()
        max_volatility = success_df['æˆåŠŸç‡æ³¢åŠ¨(æ ‡å‡†å·®)'].max()
        total_requests = success_df['æ€»è¯·æ±‚æ•°'].sum()

        summary_data = [
            ('å¹³å‡æˆåŠŸç‡', f"{avg_success_rate:.2f}%"),
            ('æœ€ä½æˆåŠŸç‡', f"{min_success_rate:.2f}%"),
            ('æœ€å¤§æ³¢åŠ¨æ€§', f"{max_volatility:.2f}%"),
            ('æ€»è¯·æ±‚æ•°', f"{total_requests:,}")
        ]

        for metric, value in summary_data:
            ws.cell(row=current_row, column=2, value=metric)
            ws.cell(row=current_row, column=3, value=value)
            current_row += 1
        current_row += 1

    # 2. å“åº”æ—¶é—´æ‘˜è¦
    if outputs.get('æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§') is not None and not outputs['æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§'].empty:
        response_df = outputs['æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§']
        ws.cell(row=current_row, column=1, value='â±ï¸ å“åº”æ—¶é—´æ¦‚è§ˆ').font = Font(bold=True, size=12)
        current_row += 1

        avg_response_time = response_df['å¹³å‡å“åº”æ—¶é—´(ç§’)'].mean()
        max_response_time = response_df['æœ€é«˜å“åº”æ—¶é—´(ç§’)'].max()
        avg_p95 = response_df['P95å“åº”æ—¶é—´(ç§’)'].mean()
        avg_p99 = response_df['P99å“åº”æ—¶é—´(ç§’)'].mean()

        summary_data = [
            ('å¹³å‡å“åº”æ—¶é—´', f"{avg_response_time:.3f}ç§’"),
            ('æœ€é«˜å“åº”æ—¶é—´', f"{max_response_time:.3f}ç§’"),
            ('å¹³å‡P95å“åº”æ—¶é—´', f"{avg_p95:.3f}ç§’"),
            ('å¹³å‡P99å“åº”æ—¶é—´', f"{avg_p99:.3f}ç§’")
        ]

        for metric, value in summary_data:
            ws.cell(row=current_row, column=2, value=metric)
            ws.cell(row=current_row, column=3, value=value)
            current_row += 1
        current_row += 1

    # 3. åç«¯æ€§èƒ½æ‘˜è¦
    if outputs.get('åç«¯å¤„ç†æ€§èƒ½') is not None and not outputs['åç«¯å¤„ç†æ€§èƒ½'].empty:
        backend_df = outputs['åç«¯å¤„ç†æ€§èƒ½']
        ws.cell(row=current_row, column=1, value='ğŸ”§ åç«¯å¤„ç†æ€§èƒ½æ¦‚è§ˆ').font = Font(bold=True, size=12)
        current_row += 1

        avg_efficiency = backend_df['åç«¯å¤„ç†æ•ˆç‡(%)'].mean()
        avg_connect_time = backend_df['å¹³å‡è¿æ¥æ—¶é—´(ç§’)'].mean()
        avg_process_time = backend_df['å¹³å‡å¤„ç†æ—¶é—´(ç§’)'].mean()
        avg_transfer_time = backend_df['å¹³å‡ä¼ è¾“æ—¶é—´(ç§’)'].mean()

        summary_data = [
            ('å¹³å‡åç«¯æ•ˆç‡', f"{avg_efficiency:.2f}%"),
            ('å¹³å‡è¿æ¥æ—¶é—´', f"{avg_connect_time:.3f}ç§’"),
            ('å¹³å‡å¤„ç†æ—¶é—´', f"{avg_process_time:.3f}ç§’"),
            ('å¹³å‡ä¼ è¾“æ—¶é—´', f"{avg_transfer_time:.3f}ç§’")
        ]

        for metric, value in summary_data:
            ws.cell(row=current_row, column=2, value=metric)
            ws.cell(row=current_row, column=3, value=value)
            current_row += 1
        current_row += 1

    # 4. æ•°æ®ä¼ è¾“æ€§èƒ½æ‘˜è¦
    if outputs.get('æ•°æ®ä¼ è¾“æ€§èƒ½') is not None and not outputs['æ•°æ®ä¼ è¾“æ€§èƒ½'].empty:
        transfer_df = outputs['æ•°æ®ä¼ è¾“æ€§èƒ½']
        ws.cell(row=current_row, column=1, value='ğŸ“¡ æ•°æ®ä¼ è¾“æ€§èƒ½æ¦‚è§ˆ').font = Font(bold=True, size=12)
        current_row += 1

        avg_response_speed = transfer_df['å“åº”ä¼ è¾“é€Ÿåº¦(KB/s)'].mean()
        avg_total_speed = transfer_df['æ€»ä¼ è¾“é€Ÿåº¦(KB/s)'].mean()
        avg_nginx_speed = transfer_df['Nginxä¼ è¾“é€Ÿåº¦(KB/s)'].mean()

        summary_data = [
            ('å¹³å‡å“åº”ä¼ è¾“é€Ÿåº¦', f"{avg_response_speed:.2f} KB/s"),
            ('å¹³å‡æ€»ä¼ è¾“é€Ÿåº¦', f"{avg_total_speed:.2f} KB/s"),
            ('å¹³å‡Nginxä¼ è¾“é€Ÿåº¦', f"{avg_nginx_speed:.2f} KB/s")
        ]

        for metric, value in summary_data:
            ws.cell(row=current_row, column=2, value=metric)
            ws.cell(row=current_row, column=3, value=value)
            current_row += 1
        current_row += 1

    # 5. Nginxç”Ÿå‘½å‘¨æœŸæ‘˜è¦
    if outputs.get('Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ') is not None and not outputs['Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ'].empty:
        lifecycle_df = outputs['Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ']
        ws.cell(row=current_row, column=1, value='ğŸ”„ Nginxç”Ÿå‘½å‘¨æœŸæ¦‚è§ˆ').font = Font(bold=True, size=12)
        current_row += 1

        avg_network_overhead = lifecycle_df['ç½‘ç»œå¼€é”€å æ¯”(%)'].mean()
        avg_transfer_ratio = lifecycle_df['ä¼ è¾“æ—¶é—´å æ¯”(%)'].mean()
        avg_nginx_phase = lifecycle_df['å¹³å‡Nginxä¼ è¾“é˜¶æ®µ(ç§’)'].mean()

        summary_data = [
            ('å¹³å‡ç½‘ç»œå¼€é”€å æ¯”', f"{avg_network_overhead:.2f}%"),
            ('å¹³å‡ä¼ è¾“æ—¶é—´å æ¯”', f"{avg_transfer_ratio:.2f}%"),
            ('å¹³å‡Nginxä¼ è¾“é˜¶æ®µ', f"{avg_nginx_phase:.3f}ç§’")
        ]

        for metric, value in summary_data:
            ws.cell(row=current_row, column=2, value=metric)
            ws.cell(row=current_row, column=3, value=value)
            current_row += 1

    # è®¾ç½®åˆ—å®½å’Œæ ·å¼
    ws.column_dimensions['A'].width = 25
    ws.column_dimensions['B'].width = 20
    ws.column_dimensions['C'].width = 20
    ws.column_dimensions['D'].width = 15

    # è®¾ç½®å¯¹é½æ–¹å¼
    for row in ws.iter_rows():
        for cell in row:
            if cell.value:
                cell.alignment = Alignment(horizontal='left', vertical='center')

    format_excel_sheet(ws)