import gc
import numpy as np
import pandas as pd
# ipaddressæ¨¡å—åœ¨Python 3.3+ä¸­å¯ç”¨
try:
    import ipaddress
    IPADDRESS_AVAILABLE = True
except ImportError:
    IPADDRESS_AVAILABLE = False
from collections import defaultdict
from openpyxl import Workbook
from openpyxl.styles import Font
import math

from self_00_01_constants import DEFAULT_CHUNK_SIZE, DEFAULT_SLOW_THRESHOLD
from self_00_02_utils import log_info
from self_00_04_excel_processor import (
    format_excel_sheet,
    add_dataframe_to_excel_with_grouped_headers,
    create_pie_chart,
    create_line_chart
)
from self_00_05_sampling_algorithms import (
    TDigest, HyperLogLog, ReservoirSampler, StratifiedSampler
)


class AdvancedIPAnalyzer:
    """é«˜çº§IPåˆ†æå™¨ - ä½¿ç”¨æµå¼ç®—æ³•ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    
    def __init__(self):
        # IPç»Ÿè®¡æ•°æ®ç»“æ„ - ä½¿ç”¨æµå¼ç®—æ³•
        self.ip_stats = {}
        self.total_processed = 0
        
        # å…¨å±€æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
        self.global_hourly_distribution = defaultdict(int)
        
        # é…ç½®å‚æ•°
        self.max_sample_size = 1000  # é™åˆ¶æ ·æœ¬å¤§å°
        self.compression = 100       # T-Digestå‹ç¼©å‚æ•°
        self.hll_precision = 12      # HyperLogLogç²¾åº¦
        
    def _init_ip_stats(self, ip):
        """åˆå§‹åŒ–å•ä¸ªIPçš„ç»Ÿè®¡ç»“æ„"""
        return {
            # åŸºç¡€è®¡æ•°
            'total_requests': 0,
            'success_requests': 0,
            'error_requests': 0,
            'slow_requests': 0,
            
            # æµå¼ç»Ÿè®¡ç®—æ³•
            'response_time_digest': TDigest(compression=self.compression),
            'data_size_digest': TDigest(compression=self.compression),
            'unique_apis_hll': HyperLogLog(precision=self.hll_precision),
            'user_agents_sampler': ReservoirSampler(self.max_sample_size),
            
            # ç´¯è®¡ç»Ÿè®¡ï¼ˆå†…å­˜å¯æ§ï¼‰
            'total_response_time': 0.0,
            'total_data_size': 0.0,
            'status_codes': defaultdict(int),
            'hourly_distribution': defaultdict(int),
            
            # é™åˆ¶å¤§å°çš„é›†åˆ
            'sample_request_times': [],
            'sample_apis': set()
        }
    
    def analyze_ip_sources(self, csv_path, output_path, top_n=100):
        """åˆ†ææ¥æºIPï¼ŒåŒ…æ‹¬è¯·æ±‚åˆ†å¸ƒã€åœ°ç†ä½ç½®ã€å¼‚å¸¸æ£€æµ‹ç­‰ - ä¼˜åŒ–ç‰ˆ"""
        log_info("ğŸš€ å¼€å§‹é«˜çº§IPåˆ†æï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰...", show_memory=True)
        
        chunk_size = max(DEFAULT_CHUNK_SIZE // 2, 10000)
        
        # ç¬¬ä¸€éï¼šæ”¶é›†IPç»Ÿè®¡æ•°æ®
        log_info("ğŸ“Š ç¬¬ä¸€éæ‰«æï¼šæ”¶é›†IPç»Ÿè®¡æ•°æ®")
        for chunk in pd.read_csv(csv_path, chunksize=chunk_size):
            self._process_chunk(chunk)
            
            if self.total_processed % 100000 == 0:
                gc.collect()
                log_info(f"å·²å¤„ç† {self.total_processed:,} æ¡è®°å½•ï¼Œå‘ç° {len(self.ip_stats)} ä¸ªå”¯ä¸€IP")
        
        total_unique_ips = len(self.ip_stats)
        log_info(f"âœ… IPç»Ÿè®¡å®Œæˆï¼šæ€»è®°å½• {self.total_processed:,}ï¼Œå”¯ä¸€IP {total_unique_ips:,}")
        
        if total_unique_ips == 0:
            log_info("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„IPæ•°æ®", level="WARNING")
            return pd.DataFrame()
        
        # ç”Ÿæˆé«˜çº§IPåˆ†ææŠ¥å‘Š
        ip_analysis_results = self._generate_advanced_ip_analysis_report(top_n)
        
        # åˆ›å»ºé«˜çº§ExcelæŠ¥å‘Š
        self._create_advanced_ip_analysis_excel(ip_analysis_results, output_path)
        
        log_info(f"ğŸ‰ é«˜çº§IPåˆ†æå®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆï¼š{output_path}", show_memory=True)
        return ip_analysis_results.head(10)
    
    def _process_chunk(self, chunk):
        """å¤„ç†æ•°æ®å—"""
        chunk_size_actual = len(chunk)
        self.total_processed += chunk_size_actual
        
        # å¤„ç†å¿…è¦çš„åˆ—
        if 'client_ip_address' not in chunk.columns:
            log_info("âš ï¸ æœªæ‰¾åˆ°client_ip_addressåˆ—ï¼Œè·³è¿‡IPåˆ†æ", level="WARNING")
            return
            
        # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…æ´—
        chunk = self._clean_chunk_data(chunk)
        
        # æŒ‰IPåˆ†ç»„å¤„ç†
        for ip, group in chunk.groupby('client_ip_address'):
            if pd.isna(ip) or ip == '' or ip == 'unknown':
                continue
            
            # åˆå§‹åŒ–IPç»Ÿè®¡ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if ip not in self.ip_stats:
                self.ip_stats[ip] = self._init_ip_stats(ip)
            
            self._process_ip_group(ip, group)
    
    def _clean_chunk_data(self, chunk):
        """æ¸…æ´—æ•°æ®å—"""
        # æ•°æ®ç±»å‹è½¬æ¢
        if 'total_request_duration' in chunk.columns:
            chunk['total_request_duration'] = pd.to_numeric(chunk['total_request_duration'], errors='coerce')
        
        # çŠ¶æ€ç å¤„ç† - æ›´ä¸¥æ ¼çš„æ¸…ç†
        if 'response_status_code' in chunk.columns:
            # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åæ¸…ç†
            chunk['response_status_code'] = chunk['response_status_code'].astype(str).str.strip()
            # è¿‡æ»¤æ‰æ— æ•ˆçš„çŠ¶æ€ç 
            chunk['response_status_code'] = chunk['response_status_code'].replace({'nan': None, '': None, '-': None})
        
        if 'response_body_size_kb' in chunk.columns:
            chunk['response_body_size_kb'] = pd.to_numeric(chunk['response_body_size_kb'], errors='coerce')
        
        return chunk
    
    def _process_ip_group(self, ip, group):
        """å¤„ç†å•ä¸ªIPçš„åˆ†ç»„æ•°æ®"""
        stats = self.ip_stats[ip]
        group_size = len(group)
        
        # åŸºç¡€ç»Ÿè®¡
        stats['total_requests'] += group_size
        
        # æˆåŠŸå’Œé”™è¯¯è¯·æ±‚ç»Ÿè®¡
        if 'response_status_code' in group.columns:
            self._process_status_codes(stats, group)
        
        # å“åº”æ—¶é—´ç»Ÿè®¡ - ä½¿ç”¨T-Digestæµå¼ç®—æ³•
        if 'total_request_duration' in group.columns:
            self._process_response_times(stats, group)
        
        # æ•°æ®å¤§å°ç»Ÿè®¡ - ä½¿ç”¨T-Digest
        if 'response_body_size_kb' in group.columns:
            self._process_data_sizes(stats, group)
        
        # APIç»Ÿè®¡ - ä½¿ç”¨HyperLogLog
        if 'request_full_uri' in group.columns:
            self._process_apis(stats, group)
        
        # æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
        if 'hour' in group.columns:
            self._process_time_distribution(stats, group)
        
        # User Agentç»Ÿè®¡ - ä½¿ç”¨è“„æ°´æ± é‡‡æ ·
        if 'user_agent_string' in group.columns:
            self._process_user_agents(stats, group)
    
    def _process_status_codes(self, stats, group):
        """å¤„ç†çŠ¶æ€ç ç»Ÿè®¡"""
        # è¿‡æ»¤æ‰ç©ºå€¼å’Œæ— æ•ˆçŠ¶æ€ç 
        valid_status_codes = group['response_status_code'].dropna()
        if valid_status_codes.empty:
            return
            
        status_counts = valid_status_codes.value_counts()
        for status, count in status_counts.items():
            # ç¡®ä¿çŠ¶æ€ç æ˜¯å­—ç¬¦ä¸²æ ¼å¼å¹¶æ¸…ç†
            status_str = str(status).strip()
            
            # è·³è¿‡æ— æ•ˆçŠ¶æ€ç 
            if status_str in ['None', 'nan', '', '-'] or len(status_str) < 3:
                continue
                
            stats['status_codes'][status_str] += count
            
            # æ›´ä¸¥æ ¼çš„çŠ¶æ€ç åˆ¤æ–­
            if status_str.startswith('2') or status_str.startswith('3'):
                stats['success_requests'] += count
            elif status_str.startswith('4') or status_str.startswith('5'):
                stats['error_requests'] += count
            # å¦‚æœçŠ¶æ€ç ä¸æ˜¯æ ‡å‡†æ ¼å¼ï¼ˆå¦‚1xxï¼‰ï¼Œè®°å½•ä½†ä¸è®¡å…¥æˆåŠŸ/é”™è¯¯
    
    def _process_response_times(self, stats, group):
        """å¤„ç†å“åº”æ—¶é—´ç»Ÿè®¡ - æµå¼ç®—æ³•"""
        durations = group['total_request_duration'].dropna()
        if durations.empty:
            return
            
        # ç´¯è®¡ç»Ÿè®¡
        stats['total_response_time'] += durations.sum()
        slow_count = (durations > DEFAULT_SLOW_THRESHOLD).sum()
        stats['slow_requests'] += slow_count
        
        # T-Digestæµå¼åˆ†ä½æ•°è®¡ç®—
        for duration in durations:
            if not math.isinf(duration) and not math.isnan(duration):
                stats['response_time_digest'].add(float(duration))
        
        # ä¿æŒå°‘é‡æ ·æœ¬ç”¨äºè¯¦ç»†åˆ†æï¼ˆå†…å­˜å¯æ§ï¼‰
        if len(stats['sample_request_times']) < self.max_sample_size:
            sample_size = min(self.max_sample_size - len(stats['sample_request_times']), len(durations))
            if sample_size > 0:
                sample = durations.sample(sample_size, random_state=42) if len(durations) > sample_size else durations
                stats['sample_request_times'].extend(sample.tolist())
    
    def _process_data_sizes(self, stats, group):
        """å¤„ç†æ•°æ®å¤§å°ç»Ÿè®¡ - æµå¼ç®—æ³•"""
        sizes = group['response_body_size_kb'].dropna()
        if sizes.empty:
            return
            
        # ç´¯è®¡ç»Ÿè®¡
        stats['total_data_size'] += sizes.sum()
        
        # T-Digestæµå¼ç»Ÿè®¡
        for size in sizes:
            if not math.isinf(size) and not math.isnan(size) and size >= 0:
                stats['data_size_digest'].add(float(size))
    
    def _process_apis(self, stats, group):
        """å¤„ç†APIç»Ÿè®¡ - HyperLogLog"""
        apis = group['request_full_uri'].dropna().unique()
        
        # HyperLogLogæµå¼å”¯ä¸€è®¡æ•°
        for api in apis:
            stats['unique_apis_hll'].add(str(api))
        
        # ä¿æŒå°‘é‡æ ·æœ¬ç”¨äºå±•ç¤º
        if len(stats['sample_apis']) < 50:
            remaining_slots = 50 - len(stats['sample_apis'])
            sample_apis = apis[:remaining_slots] if len(apis) > remaining_slots else apis
            stats['sample_apis'].update(sample_apis)
    
    def _process_time_distribution(self, stats, group):
        """å¤„ç†æ—¶é—´åˆ†å¸ƒç»Ÿè®¡"""
        hour_counts = group['hour'].value_counts()
        for hour, count in hour_counts.items():
            if pd.notna(hour):
                hour_int = int(hour)
                stats['hourly_distribution'][hour_int] += count
                self.global_hourly_distribution[hour_int] += count
    
    def _process_user_agents(self, stats, group):
        """å¤„ç†User Agentç»Ÿè®¡ - è“„æ°´æ± é‡‡æ ·"""
        agents = group['user_agent_string'].dropna().unique()
        for agent in agents:
            stats['user_agents_sampler'].add({'user_agent': str(agent)})
    
    def _generate_advanced_ip_analysis_report(self, top_n):
        """ç”Ÿæˆé«˜çº§IPåˆ†ææŠ¥å‘Š"""
        log_info("ğŸ“‹ ç”Ÿæˆé«˜çº§IPåˆ†ææŠ¥å‘Š...")
        
        results = []
        for ip, stats in self.ip_stats.items():
            result = self._calculate_ip_metrics(ip, stats)
            results.append(result)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
        df = pd.DataFrame(results)
        df = df.sort_values(by='æ€»è¯·æ±‚æ•°', ascending=False).head(top_n)
        
        log_info(f"âœ… ç”Ÿæˆäº† {len(df)} ä¸ªIPçš„é«˜çº§åˆ†ææŠ¥å‘Š")
        return df
    
    def _calculate_ip_metrics(self, ip, stats):
        """è®¡ç®—å•ä¸ªIPçš„æŒ‡æ ‡"""
        total_requests = stats['total_requests']
        success_requests = stats['success_requests']
        error_requests = stats['error_requests']
        slow_requests = stats['slow_requests']
        
        # è®¡ç®—æ¯”ç‡
        success_rate = (success_requests / total_requests * 100) if total_requests > 0 else 0
        error_rate = (error_requests / total_requests * 100) if total_requests > 0 else 0
        slow_rate = (slow_requests / total_requests * 100) if total_requests > 0 else 0
        
        # å¹³å‡å“åº”æ—¶é—´
        avg_response_time = (stats['total_response_time'] / success_requests) if success_requests > 0 else 0
        
        # é«˜çº§åˆ†ä½æ•°è®¡ç®— - ä½¿ç”¨T-Digest
        response_time_digest = stats['response_time_digest']
        median_time = response_time_digest.percentile(50) if response_time_digest.count > 0 else 0
        p95_time = response_time_digest.percentile(95) if response_time_digest.count > 0 else 0
        p99_time = response_time_digest.percentile(99) if response_time_digest.count > 0 else 0
        
        # æ•°æ®ä¼ è¾“ç»Ÿè®¡
        data_size_digest = stats['data_size_digest']
        avg_data_size = (stats['total_data_size'] / total_requests) if total_requests > 0 else 0
        median_data_size = data_size_digest.percentile(50) if data_size_digest.count > 0 else 0
        p95_data_size = data_size_digest.percentile(95) if data_size_digest.count > 0 else 0
        
        # å”¯ä¸€APIæ•°é‡ - HyperLogLogä¼°è®¡
        unique_api_count = stats['unique_apis_hll'].cardinality()
        
        # IPç±»å‹åˆ†æ
        ip_type = self._classify_ip_type(ip)
        
        # é«˜çº§é£é™©è¯„åˆ†
        risk_score, risk_factors = self._calculate_advanced_risk_score(stats, total_requests, error_rate, slow_rate, unique_api_count)
        
        # å¼‚å¸¸æ£€æµ‹è¯„åˆ†
        anomaly_score, anomaly_level = self._calculate_anomaly_score(stats, total_requests, error_rate, slow_rate)
        
        # æœ€å¸¸è§çš„çŠ¶æ€ç å’Œæ—¶æ®µ
        most_common_status = max(stats['status_codes'].items(), key=lambda x: x[1])[0] if stats['status_codes'] else 'N/A'
        peak_hour = max(stats['hourly_distribution'].items(), key=lambda x: x[1])[0] if stats['hourly_distribution'] else 'N/A'
        
        # User Agenté‡‡æ ·æ•°é‡
        user_agent_count = len(stats['user_agents_sampler'].get_samples())
        
        # è¡Œä¸ºæ¨¡å¼åˆ†æ
        behavior_pattern = self._analyze_behavior_pattern(stats, total_requests, unique_api_count, error_rate)
        
        return {
            'IPåœ°å€': ip,
            'IPç±»å‹': ip_type,
            'æ€»è¯·æ±‚æ•°': total_requests,
            'æˆåŠŸè¯·æ±‚æ•°': success_requests,
            'é”™è¯¯è¯·æ±‚æ•°': error_requests,
            'æ…¢è¯·æ±‚æ•°': slow_requests,
            'æˆåŠŸç‡(%)': round(success_rate, 2),
            'é”™è¯¯ç‡(%)': round(error_rate, 2),
            'æ…¢è¯·æ±‚ç‡(%)': round(slow_rate, 2),
            'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(avg_response_time, 3),
            'å“åº”æ—¶é—´ä¸­ä½æ•°(ç§’)': round(median_time, 3),
            'P95å“åº”æ—¶é—´(ç§’)': round(p95_time, 3),
            'P99å“åº”æ—¶é—´(ç§’)': round(p99_time, 3),
            'å¹³å‡æ•°æ®ä¼ è¾“(KB)': round(avg_data_size, 2),
            'æ•°æ®ä¼ è¾“ä¸­ä½æ•°(KB)': round(median_data_size, 2),
            'P95æ•°æ®ä¼ è¾“(KB)': round(p95_data_size, 2),
            'æ€»æ•°æ®ä¼ è¾“(MB)': round(stats['total_data_size'] / 1024, 2),
            'å”¯ä¸€APIæ•°(ä¼°è®¡)': int(unique_api_count),
            'æœ€å¸¸è§çŠ¶æ€ç ': most_common_status,
            'æ´»è·ƒæ—¶æ®µ': f"{peak_hour}:00" if peak_hour != 'N/A' else 'N/A',
            'é£é™©è¯„åˆ†': risk_score,
            'é£é™©å› å­': '; '.join(risk_factors) if risk_factors else 'æ— ',
            'å¼‚å¸¸è¯„åˆ†': anomaly_score,
            'å¼‚å¸¸ç­‰çº§': anomaly_level,
            'è¡Œä¸ºæ¨¡å¼': behavior_pattern,
            'User Agenté‡‡æ ·æ•°': user_agent_count
        }
    
    def _classify_ip_type(self, ip_str):
        """åˆ†ç±»IPç±»å‹"""
        if not IPADDRESS_AVAILABLE:
            # å½“ipaddressæ¨¡å—ä¸å¯ç”¨æ—¶çš„ç®€å•åˆ†ç±»
            if ip_str.startswith(('192.168.', '10.', '172.')):
                return "å†…ç½‘IP"
            elif ip_str.startswith('127.'):
                return "å›ç¯IP"
            elif ip_str.startswith(('224.', '225.', '226.', '227.', '228.', '229.', '230.', '231.', '232.', '233.', '234.', '235.', '236.', '237.', '238.', '239.')):
                return "ç»„æ’­IP"
            else:
                return "å…¬ç½‘IP"
        
        try:
            ip = ipaddress.ip_address(ip_str)
            if ip.is_private:
                return "å†…ç½‘IP"
            elif ip.is_loopback:
                return "å›ç¯IP"
            elif ip.is_multicast:
                return "ç»„æ’­IP"
            elif ip.is_reserved:
                return "ä¿ç•™IP"
            else:
                return "å…¬ç½‘IP"
        except ValueError:
            return "æ— æ•ˆIP"
    
    def _calculate_advanced_risk_score(self, stats, total_requests, error_rate, slow_rate, unique_api_count):
        """è®¡ç®—é«˜çº§é£é™©è¯„åˆ†ï¼ˆ0-100ï¼Œåˆ†æ•°è¶Šé«˜é£é™©è¶Šå¤§ï¼‰"""
        risk_score = 0
        risk_factors = []
        
        # åŸºäºè¯·æ±‚é‡çš„é£é™©ï¼ˆå¤§é‡è¯·æ±‚å¯èƒ½æ˜¯æ”»å‡»ï¼‰
        if total_requests > 50000:
            risk_score += 40
            risk_factors.append('è¶…é«˜è¯·æ±‚é‡')
        elif total_requests > 10000:
            risk_score += 30
            risk_factors.append('é«˜è¯·æ±‚é‡')
        elif total_requests > 1000:
            risk_score += 15
            risk_factors.append('ä¸­ç­‰è¯·æ±‚é‡')
        elif total_requests > 100:
            risk_score += 5
        
        # åŸºäºé”™è¯¯ç‡çš„é£é™©
        if error_rate > 50:
            risk_score += 25
            risk_factors.append('æé«˜é”™è¯¯ç‡')
        elif error_rate > 20:
            risk_score += 15
            risk_factors.append('é«˜é”™è¯¯ç‡')
        elif error_rate > 10:
            risk_score += 10
            risk_factors.append('ä¸­ç­‰é”™è¯¯ç‡')
        
        # åŸºäºæ…¢è¯·æ±‚ç‡çš„é£é™©
        if slow_rate > 30:
            risk_score += 20
            risk_factors.append('æé«˜æ…¢è¯·æ±‚ç‡')
        elif slow_rate > 10:
            risk_score += 10
            risk_factors.append('é«˜æ…¢è¯·æ±‚ç‡')
        
        # åŸºäºAPIå¤šæ ·æ€§çš„é£é™©ï¼ˆè®¿é—®è¿‡å¤šä¸åŒAPIå¯èƒ½æ˜¯æ‰«æï¼‰
        if unique_api_count > 100:
            risk_score += 20
            risk_factors.append('APIæ‰«æè¡Œä¸º')
        elif unique_api_count > 50:
            risk_score += 15
            risk_factors.append('é«˜APIå¤šæ ·æ€§')
        elif unique_api_count > 20:
            risk_score += 10
            risk_factors.append('ä¸­ç­‰APIå¤šæ ·æ€§')
        
        # åŸºäº4xxçŠ¶æ€ç æ¯”ä¾‹
        status_4xx_count = sum(count for status, count in stats['status_codes'].items() if status.startswith('4'))
        if total_requests > 0:
            status_4xx_rate = status_4xx_count / total_requests * 100
            if status_4xx_rate > 30:
                risk_score += 15
                risk_factors.append('é«˜4xxé”™è¯¯ç‡')
            elif status_4xx_rate > 10:
                risk_score += 10
                risk_factors.append('ä¸­ç­‰4xxé”™è¯¯ç‡')
        
        # åŸºäºæ—¶é—´åˆ†å¸ƒçš„é£é™©ï¼ˆéæ­£å¸¸æ—¶é—´å¤§é‡è®¿é—®ï¼‰
        if stats['hourly_distribution']:
            night_requests = sum(stats['hourly_distribution'].get(hour, 0) for hour in [0, 1, 2, 3, 4, 5])
            night_ratio = night_requests / total_requests * 100 if total_requests > 0 else 0
            if night_ratio > 50:
                risk_score += 15
                risk_factors.append('æ·±å¤œå¼‚å¸¸æ´»è·ƒ')
        
        return min(risk_score, 100), risk_factors
    
    def _calculate_anomaly_score(self, stats, total_requests, error_rate, slow_rate):
        """è®¡ç®—å¼‚å¸¸æ£€æµ‹è¯„åˆ†"""
        anomaly_score = 0
        
        # è¯·æ±‚é‡å¼‚å¸¸
        if total_requests > 20000:
            anomaly_score += 30
        
        # æˆåŠŸç‡å¼‚å¸¸
        success_rate = ((stats['success_requests'] / total_requests * 100) if total_requests > 0 else 0)
        if success_rate < 50:
            anomaly_score += 40
        
        # å“åº”æ—¶é—´å¼‚å¸¸
        if stats['response_time_digest'].count > 0:
            p99_time = stats['response_time_digest'].percentile(99)
            if p99_time > 10:  # P99è¶…è¿‡10ç§’
                anomaly_score += 30
        
        # é”™è¯¯ç‡å¼‚å¸¸
        if error_rate > 20:
            anomaly_score += 25
        
        # å¼‚å¸¸ç­‰çº§åˆ†ç±»
        if anomaly_score >= 80:
            anomaly_level = "ä¸¥é‡å¼‚å¸¸"
        elif anomaly_score >= 60:
            anomaly_level = "ä¸­åº¦å¼‚å¸¸"
        elif anomaly_score >= 40:
            anomaly_level = "è½»å¾®å¼‚å¸¸"
        else:
            anomaly_level = "æ­£å¸¸"
            
        return anomaly_score, anomaly_level
    
    def _analyze_behavior_pattern(self, stats, total_requests, unique_api_count, error_rate):
        """åˆ†æè¡Œä¸ºæ¨¡å¼"""
        if unique_api_count > 50 and total_requests > 1000:
            if error_rate > 30:
                return "ç–‘ä¼¼æ¶æ„æ‰«æ"
            else:
                return "æ·±åº¦è®¿é—®ç”¨æˆ·"
        elif total_requests > 5000 and unique_api_count < 5:
            return "é«˜é¢‘å•ä¸€è®¿é—®"
        elif error_rate > 50:
            return "å¼‚å¸¸è®¿é—®"
        elif total_requests < 10:
            return "è½»åº¦è®¿é—®"
        else:
            return "æ­£å¸¸è®¿é—®"
    
    def _create_advanced_ip_analysis_excel(self, ip_df, output_path):
        """åˆ›å»ºé«˜çº§IPåˆ†æExcelæŠ¥å‘Š"""
        log_info(f"ğŸ“Š åˆ›å»ºé«˜çº§IPåˆ†æExcelæŠ¥å‘Š: {output_path}")
        
        wb = Workbook()
        if 'Sheet' in wb.sheetnames:
            del wb['Sheet']
        
        # ä¸»è¦IPç»Ÿè®¡è¡¨ - å¢å¼ºç‰ˆ
        header_groups = {
            "åŸºç¡€ä¿¡æ¯": ["IPåœ°å€", "IPç±»å‹", "è¡Œä¸ºæ¨¡å¼"],
            "è¯·æ±‚ç»Ÿè®¡": ["æ€»è¯·æ±‚æ•°", "æˆåŠŸè¯·æ±‚æ•°", "é”™è¯¯è¯·æ±‚æ•°", "æ…¢è¯·æ±‚æ•°"],
            "æ€§èƒ½æ¯”ç‡": ["æˆåŠŸç‡(%)", "é”™è¯¯ç‡(%)", "æ…¢è¯·æ±‚ç‡(%)"],
            "å“åº”æ—¶é—´åˆ†æ": ["å¹³å‡å“åº”æ—¶é—´(ç§’)", "å“åº”æ—¶é—´ä¸­ä½æ•°(ç§’)", "P95å“åº”æ—¶é—´(ç§’)", "P99å“åº”æ—¶é—´(ç§’)"],
            "æ•°æ®ä¼ è¾“åˆ†æ": ["å¹³å‡æ•°æ®ä¼ è¾“(KB)", "æ•°æ®ä¼ è¾“ä¸­ä½æ•°(KB)", "P95æ•°æ®ä¼ è¾“(KB)", "æ€»æ•°æ®ä¼ è¾“(MB)"],
            "é£é™©è¯„ä¼°": ["é£é™©è¯„åˆ†", "é£é™©å› å­", "å¼‚å¸¸è¯„åˆ†", "å¼‚å¸¸ç­‰çº§"],
            "å…¶ä»–æŒ‡æ ‡": ["å”¯ä¸€APIæ•°(ä¼°è®¡)", "æœ€å¸¸è§çŠ¶æ€ç ", "æ´»è·ƒæ—¶æ®µ", "User Agenté‡‡æ ·æ•°"]
        }
        
        ws_main = add_dataframe_to_excel_with_grouped_headers(
            wb, ip_df, 'é«˜çº§IPåˆ†æç»Ÿè®¡', header_groups=header_groups
        )
        
        # é«˜é£é™©IPå·¥ä½œè¡¨ - å¢å¼ºç‰ˆ
        self._create_advanced_high_risk_ip_sheet(wb, ip_df)
        
        # IPç±»å‹åˆ†å¸ƒå·¥ä½œè¡¨ - å¢å¼ºç‰ˆ
        self._create_advanced_ip_type_distribution_sheet(wb, ip_df)
        
        # è¡Œä¸ºæ¨¡å¼åˆ†æå·¥ä½œè¡¨
        self._create_behavior_pattern_analysis_sheet(wb, ip_df)
        
        # æ—¶é—´åˆ†å¸ƒåˆ†æå·¥ä½œè¡¨ - å¢å¼ºç‰ˆ
        self._create_advanced_time_distribution_sheet(wb)
        
        # å¼‚å¸¸æ£€æµ‹å·¥ä½œè¡¨
        self._create_anomaly_detection_sheet(wb, ip_df)
        
        # æ¦‚è§ˆå·¥ä½œè¡¨ - å¢å¼ºç‰ˆ
        self._create_advanced_ip_overview_sheet(wb, ip_df)
        
        # ä¿å­˜æ–‡ä»¶
        wb.save(output_path)
        log_info(f"âœ… é«˜çº§IPåˆ†æExcelæŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    def _create_advanced_high_risk_ip_sheet(self, wb, ip_df):
        """åˆ›å»ºé«˜çº§é«˜é£é™©IPå·¥ä½œè¡¨"""
        ws = wb.create_sheet(title='é«˜é£é™©IPåˆ†æ')
        
        # ç­›é€‰é«˜é£é™©IPï¼ˆé£é™©è¯„åˆ† > 50æˆ–å¼‚å¸¸è¯„åˆ† > 60ï¼‰
        high_risk_ips = ip_df[
            (ip_df['é£é™©è¯„åˆ†'] > 50) | (ip_df['å¼‚å¸¸è¯„åˆ†'] > 60)
        ].sort_values(by=['é£é™©è¯„åˆ†', 'å¼‚å¸¸è¯„åˆ†'], ascending=False)
        
        if high_risk_ips.empty:
            ws.cell(row=1, column=1, value="ğŸ‰ æœªå‘ç°é«˜é£é™©IP").font = Font(bold=True)
            return
        
        # é«˜é£é™©IPè¡¨å¤´
        high_risk_headers = {
            "åŸºç¡€ä¿¡æ¯": ["IPåœ°å€", "IPç±»å‹", "è¡Œä¸ºæ¨¡å¼"],
            "é£é™©æŒ‡æ ‡": ["é£é™©è¯„åˆ†", "å¼‚å¸¸è¯„åˆ†", "å¼‚å¸¸ç­‰çº§"],
            "å…³é”®ç»Ÿè®¡": ["æ€»è¯·æ±‚æ•°", "é”™è¯¯ç‡(%)", "æ…¢è¯·æ±‚ç‡(%)", "å”¯ä¸€APIæ•°(ä¼°è®¡)"],
            "è¯¦ç»†ä¿¡æ¯": ["é£é™©å› å­", "æœ€å¸¸è§çŠ¶æ€ç ", "æ´»è·ƒæ—¶æ®µ", "æ€»æ•°æ®ä¼ è¾“(MB)"]
        }
        
        risk_columns = [
            "IPåœ°å€", "IPç±»å‹", "è¡Œä¸ºæ¨¡å¼", "é£é™©è¯„åˆ†", "å¼‚å¸¸è¯„åˆ†", "å¼‚å¸¸ç­‰çº§", 
            "æ€»è¯·æ±‚æ•°", "é”™è¯¯ç‡(%)", "æ…¢è¯·æ±‚ç‡(%)", "å”¯ä¸€APIæ•°(ä¼°è®¡)", 
            "é£é™©å› å­", "æœ€å¸¸è§çŠ¶æ€ç ", "æ´»è·ƒæ—¶æ®µ", "æ€»æ•°æ®ä¼ è¾“(MB)"
        ]
        risk_df = high_risk_ips[risk_columns].copy()
        
        # ç”±äºå·¥ä½œè¡¨å·²åˆ›å»ºï¼Œéœ€è¦å…ˆåˆ é™¤å†é‡æ–°åˆ›å»º
        wb.remove(ws)
        ws = add_dataframe_to_excel_with_grouped_headers(
            wb, risk_df, 'é«˜é£é™©IPåˆ†æ', header_groups=high_risk_headers
        )
        
        # æ·»åŠ é£é™©åˆ†æè¯´æ˜
        note_row = len(risk_df) + 5
        ws.cell(row=note_row, column=1, value="ğŸ” é£é™©è¯„åˆ†è¯´æ˜ï¼š").font = Font(bold=True)
        ws.cell(row=note_row + 1, column=1, value="â€¢ é£é™©è¯„åˆ† 70-100: é«˜é£é™©ï¼Œéœ€è¦ç«‹å³å…³æ³¨")
        ws.cell(row=note_row + 2, column=1, value="â€¢ é£é™©è¯„åˆ† 50-70: ä¸­ç­‰é£é™©ï¼Œå»ºè®®ç›‘æ§")
        ws.cell(row=note_row + 3, column=1, value="â€¢ å¼‚å¸¸è¯„åˆ† 80+: ä¸¥é‡å¼‚å¸¸")
        ws.cell(row=note_row + 4, column=1, value="â€¢ å¼‚å¸¸è¯„åˆ† 60-79: ä¸­åº¦å¼‚å¸¸")
        
        format_excel_sheet(ws)
    
    def _create_advanced_ip_type_distribution_sheet(self, wb, ip_df):
        """åˆ›å»ºé«˜çº§IPç±»å‹åˆ†å¸ƒå·¥ä½œè¡¨"""
        ws = wb.create_sheet(title='IPç±»å‹åˆ†å¸ƒåˆ†æ')
        
        # IPç±»å‹ç»Ÿè®¡ - å¢å¼ºç‰ˆ
        ip_type_stats = ip_df.groupby('IPç±»å‹').agg({
            'IPåœ°å€': 'count',
            'æ€»è¯·æ±‚æ•°': ['sum', 'mean'],
            'æˆåŠŸç‡(%)': 'mean',
            'é”™è¯¯ç‡(%)': 'mean',
            'é£é™©è¯„åˆ†': 'mean',
            'å¼‚å¸¸è¯„åˆ†': 'mean',
            'å”¯ä¸€APIæ•°(ä¼°è®¡)': 'mean'
        }).round(2)
        
        # å±•å¹³åˆ—å
        ip_type_stats.columns = [
            'IPæ•°é‡', 'æ€»è¯·æ±‚æ•°', 'å¹³å‡æ¯IPè¯·æ±‚æ•°', 'å¹³å‡æˆåŠŸç‡(%)', 
            'å¹³å‡é”™è¯¯ç‡(%)', 'å¹³å‡é£é™©è¯„åˆ†', 'å¹³å‡å¼‚å¸¸è¯„åˆ†', 'å¹³å‡APIæ•°'
        ]
        ip_type_stats = ip_type_stats.reset_index()
        
        # æ·»åŠ åˆ°å·¥ä½œè¡¨
        type_headers = {
            "åˆ†ç±»": ["IPç±»å‹"],
            "æ•°é‡ç»Ÿè®¡": ["IPæ•°é‡", "æ€»è¯·æ±‚æ•°", "å¹³å‡æ¯IPè¯·æ±‚æ•°"],
            "æ€§èƒ½æŒ‡æ ‡": ["å¹³å‡æˆåŠŸç‡(%)", "å¹³å‡é”™è¯¯ç‡(%)"],
            "é£é™©æŒ‡æ ‡": ["å¹³å‡é£é™©è¯„åˆ†", "å¹³å‡å¼‚å¸¸è¯„åˆ†", "å¹³å‡APIæ•°"]
        }
        
        ws = add_dataframe_to_excel_with_grouped_headers(
            wb, ip_type_stats, 'IPç±»å‹åˆ†å¸ƒåˆ†æ', header_groups=type_headers
        )
        
        format_excel_sheet(ws)
    
    def _create_behavior_pattern_analysis_sheet(self, wb, ip_df):
        """åˆ›å»ºè¡Œä¸ºæ¨¡å¼åˆ†æå·¥ä½œè¡¨"""
        ws = wb.create_sheet(title='è¡Œä¸ºæ¨¡å¼åˆ†æ')
        
        # è¡Œä¸ºæ¨¡å¼ç»Ÿè®¡
        behavior_stats = ip_df.groupby('è¡Œä¸ºæ¨¡å¼').agg({
            'IPåœ°å€': 'count',
            'æ€»è¯·æ±‚æ•°': ['sum', 'mean'],
            'é£é™©è¯„åˆ†': 'mean',
            'å¼‚å¸¸è¯„åˆ†': 'mean',
            'é”™è¯¯ç‡(%)': 'mean'
        }).round(2)
        
        behavior_stats.columns = ['IPæ•°é‡', 'æ€»è¯·æ±‚æ•°', 'å¹³å‡æ¯IPè¯·æ±‚æ•°', 'å¹³å‡é£é™©è¯„åˆ†', 'å¹³å‡å¼‚å¸¸è¯„åˆ†', 'å¹³å‡é”™è¯¯ç‡(%)']
        behavior_stats = behavior_stats.reset_index()
        behavior_stats = behavior_stats.sort_values(by='å¹³å‡é£é™©è¯„åˆ†', ascending=False)
        
        # æ·»åŠ åˆ°å·¥ä½œè¡¨
        behavior_headers = {
            "æ¨¡å¼": ["è¡Œä¸ºæ¨¡å¼"],
            "æ•°é‡ç»Ÿè®¡": ["IPæ•°é‡", "æ€»è¯·æ±‚æ•°", "å¹³å‡æ¯IPè¯·æ±‚æ•°"],
            "é£é™©è¯„ä¼°": ["å¹³å‡é£é™©è¯„åˆ†", "å¹³å‡å¼‚å¸¸è¯„åˆ†", "å¹³å‡é”™è¯¯ç‡(%)"]
        }
        
        ws = add_dataframe_to_excel_with_grouped_headers(
            wb, behavior_stats, 'è¡Œä¸ºæ¨¡å¼åˆ†æ', header_groups=behavior_headers
        )
        
        format_excel_sheet(ws)
    
    def _create_advanced_time_distribution_sheet(self, wb):
        """åˆ›å»ºé«˜çº§æ—¶é—´åˆ†å¸ƒåˆ†æå·¥ä½œè¡¨"""
        ws = wb.create_sheet(title='æ—¶é—´åˆ†å¸ƒåˆ†æ')
        
        if not self.global_hourly_distribution:
            ws.cell(row=1, column=1, value="âš ï¸ æ— æ—¶é—´åˆ†å¸ƒæ•°æ®").font = Font(bold=True)
            return
        
        # åˆ›å»ºå°æ—¶åˆ†å¸ƒæ•°æ® - å¢å¼ºç‰ˆ
        hours = list(range(24))
        total_requests = sum(self.global_hourly_distribution.values())
        
        time_data = []
        for hour in hours:
            requests = self.global_hourly_distribution.get(hour, 0)
            percentage = round(requests / total_requests * 100, 2) if total_requests > 0 else 0
            
            # åˆ†ææ—¶æ®µç‰¹å¾
            if 6 <= hour <= 12:
                period = "ä¸Šåˆ"
            elif 13 <= hour <= 18:
                period = "ä¸‹åˆ"
            elif 19 <= hour <= 23:
                period = "æ™šä¸Š"
            else:
                period = "æ·±å¤œ"
            
            time_data.append({
                'å°æ—¶': f"{hour:02d}:00",
                'æ—¶æ®µ': period,
                'è¯·æ±‚æ•°': requests,
                'å æ¯”(%)': percentage,
                'æ´»è·ƒåº¦': 'é«˜' if percentage > 6 else ('ä¸­' if percentage > 3 else 'ä½')
            })
        
        time_df = pd.DataFrame(time_data)
        
        # æ·»åŠ åˆ°å·¥ä½œè¡¨
        time_headers = {
            "æ—¶é—´": ["å°æ—¶", "æ—¶æ®µ"],
            "ç»Ÿè®¡": ["è¯·æ±‚æ•°", "å æ¯”(%)", "æ´»è·ƒåº¦"]
        }
        
        ws = add_dataframe_to_excel_with_grouped_headers(
            wb, time_df, 'æ—¶é—´åˆ†å¸ƒåˆ†æ', header_groups=time_headers
        )
        
        format_excel_sheet(ws)
    
    def _create_anomaly_detection_sheet(self, wb, ip_df):
        """åˆ›å»ºå¼‚å¸¸æ£€æµ‹å·¥ä½œè¡¨"""
        ws = wb.create_sheet(title='å¼‚å¸¸æ£€æµ‹åˆ†æ')
        
        # ç­›é€‰å¼‚å¸¸IP
        anomaly_ips = ip_df[ip_df['å¼‚å¸¸ç­‰çº§'] != 'æ­£å¸¸'].sort_values(by='å¼‚å¸¸è¯„åˆ†', ascending=False)
        
        if anomaly_ips.empty:
            ws.cell(row=1, column=1, value="ğŸ‰ æœªæ£€æµ‹åˆ°å¼‚å¸¸IP").font = Font(bold=True)
            return
        
        # å¼‚å¸¸IPè¡¨å¤´
        anomaly_headers = {
            "åŸºç¡€ä¿¡æ¯": ["IPåœ°å€", "IPç±»å‹", "è¡Œä¸ºæ¨¡å¼"],
            "å¼‚å¸¸æŒ‡æ ‡": ["å¼‚å¸¸è¯„åˆ†", "å¼‚å¸¸ç­‰çº§", "é£é™©è¯„åˆ†"],
            "æ€§èƒ½æŒ‡æ ‡": ["æ€»è¯·æ±‚æ•°", "æˆåŠŸç‡(%)", "P99å“åº”æ—¶é—´(ç§’)", "å”¯ä¸€APIæ•°(ä¼°è®¡)"]
        }
        
        anomaly_columns = [
            "IPåœ°å€", "IPç±»å‹", "è¡Œä¸ºæ¨¡å¼", "å¼‚å¸¸è¯„åˆ†", "å¼‚å¸¸ç­‰çº§", "é£é™©è¯„åˆ†",
            "æ€»è¯·æ±‚æ•°", "æˆåŠŸç‡(%)", "P99å“åº”æ—¶é—´(ç§’)", "å”¯ä¸€APIæ•°(ä¼°è®¡)"
        ]
        anomaly_df = anomaly_ips[anomaly_columns].copy()
        
        ws = add_dataframe_to_excel_with_grouped_headers(
            wb, anomaly_df, 'å¼‚å¸¸æ£€æµ‹åˆ†æ', header_groups=anomaly_headers
        )
        
        format_excel_sheet(ws)
    
    def _create_advanced_ip_overview_sheet(self, wb, ip_df):
        """åˆ›å»ºé«˜çº§IPåˆ†ææ¦‚è§ˆå·¥ä½œè¡¨"""
        ws = wb.create_sheet(title='åˆ†ææ¦‚è§ˆ')
        
        # ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªä½ç½®
        wb.move_sheet(ws, -(len(wb.worksheets) - 1))
        
        # æ€»ä½“ç»Ÿè®¡
        total_unique_ips = len(ip_df)
        total_requests = ip_df['æ€»è¯·æ±‚æ•°'].sum()
        avg_requests_per_ip = total_requests / total_unique_ips if total_unique_ips > 0 else 0
        
        # é£é™©ç»Ÿè®¡
        high_risk_count = len(ip_df[ip_df['é£é™©è¯„åˆ†'] > 70])
        medium_risk_count = len(ip_df[(ip_df['é£é™©è¯„åˆ†'] > 50) & (ip_df['é£é™©è¯„åˆ†'] <= 70)])
        low_risk_count = len(ip_df[ip_df['é£é™©è¯„åˆ†'] <= 50])
        
        # å¼‚å¸¸ç»Ÿè®¡
        severe_anomaly = len(ip_df[ip_df['å¼‚å¸¸ç­‰çº§'] == 'ä¸¥é‡å¼‚å¸¸'])
        moderate_anomaly = len(ip_df[ip_df['å¼‚å¸¸ç­‰çº§'] == 'ä¸­åº¦å¼‚å¸¸'])
        mild_anomaly = len(ip_df[ip_df['å¼‚å¸¸ç­‰çº§'] == 'è½»å¾®å¼‚å¸¸'])
        normal_count = len(ip_df[ip_df['å¼‚å¸¸ç­‰çº§'] == 'æ­£å¸¸'])
        
        # IPç±»å‹ç»Ÿè®¡
        ip_type_counts = ip_df['IPç±»å‹'].value_counts()
        
        # è¡Œä¸ºæ¨¡å¼ç»Ÿè®¡
        behavior_counts = ip_df['è¡Œä¸ºæ¨¡å¼'].value_counts()
        
        # æ€§èƒ½ç»Ÿè®¡
        avg_success_rate = ip_df['æˆåŠŸç‡(%)'].mean()
        avg_error_rate = ip_df['é”™è¯¯ç‡(%)'].mean()
        avg_response_time = ip_df['å¹³å‡å“åº”æ—¶é—´(ç§’)'].mean()
        
        # æ¦‚è§ˆæ•°æ®
        overview_data = [
            ['ğŸš€ === é«˜çº§IPåˆ†ææ¦‚è§ˆ ===', ''],
            ['', ''],
            
            ['ğŸ“Š === åŸºç¡€ç»Ÿè®¡ ===', ''],
            ['æ€»å¤„ç†è®°å½•æ•°', self.total_processed],
            ['å”¯ä¸€IPæ•°é‡', total_unique_ips],
            ['æ€»è¯·æ±‚æ•°', total_requests],
            ['å¹³å‡æ¯IPè¯·æ±‚æ•°', round(avg_requests_per_ip, 2)],
            ['', ''],
            
            ['ğŸ” === é£é™©åˆ†å¸ƒ ===', ''],
            ['é«˜é£é™©IPæ•°é‡ (>70)', high_risk_count],
            ['ä¸­ç­‰é£é™©IPæ•°é‡ (50-70)', medium_risk_count],
            ['ä½é£é™©IPæ•°é‡ (â‰¤50)', low_risk_count],
            ['', ''],
            
            ['âš ï¸ === å¼‚å¸¸æ£€æµ‹ ===', ''],
            ['ä¸¥é‡å¼‚å¸¸IP', severe_anomaly],
            ['ä¸­åº¦å¼‚å¸¸IP', moderate_anomaly],
            ['è½»å¾®å¼‚å¸¸IP', mild_anomaly],
            ['æ­£å¸¸IP', normal_count],
            ['', ''],
            
            ['ğŸŒ === IPç±»å‹åˆ†å¸ƒ ===', ''],
        ]
        
        # æ·»åŠ IPç±»å‹ç»Ÿè®¡
        for ip_type, count in ip_type_counts.items():
            overview_data.append([f'{ip_type}æ•°é‡', count])
        
        overview_data.extend([
            ['', ''],
            ['ğŸ‘¤ === è¡Œä¸ºæ¨¡å¼åˆ†å¸ƒ ===', ''],
        ])
        
        # æ·»åŠ è¡Œä¸ºæ¨¡å¼ç»Ÿè®¡
        for behavior, count in behavior_counts.items():
            overview_data.append([f'{behavior}æ•°é‡', count])
        
        overview_data.extend([
            ['', ''],
            ['ğŸ“ˆ === æ€§èƒ½ç»Ÿè®¡ ===', ''],
            ['å¹³å‡æˆåŠŸç‡(%)', round(avg_success_rate, 2)],
            ['å¹³å‡é”™è¯¯ç‡(%)', round(avg_error_rate, 2)],
            ['å¹³å‡å“åº”æ—¶é—´(ç§’)', round(avg_response_time, 3)],
            ['', ''],
            
            ['ğŸ† === TOPæŒ‡æ ‡ ===', ''],
            ['è¯·æ±‚é‡æœ€å¤§IP', ip_df.iloc[0]['IPåœ°å€'] if not ip_df.empty else 'N/A'],
            ['æœ€å¤§è¯·æ±‚é‡', ip_df.iloc[0]['æ€»è¯·æ±‚æ•°'] if not ip_df.empty else 0],
            ['æœ€é«˜é£é™©è¯„åˆ†IP', ip_df.loc[ip_df['é£é™©è¯„åˆ†'].idxmax(), 'IPåœ°å€'] if not ip_df.empty else 'N/A'],
            ['æœ€é«˜é£é™©è¯„åˆ†', ip_df['é£é™©è¯„åˆ†'].max() if not ip_df.empty else 0],
            ['', ''],
            
            ['ğŸ”§ === ä¼˜åŒ–è¯´æ˜ ===', ''],
            ['ç®—æ³•ä¼˜åŒ–', 'T-Digest + HyperLogLog + è“„æ°´æ± é‡‡æ ·'],
            ['å†…å­˜ä¼˜åŒ–', 'æµå¼ç®—æ³•ï¼Œæ”¯æŒ40G+æ•°æ®'],
            ['åˆ†æå¢å¼º', 'å¤šç»´é£é™©è¯„åˆ† + å¼‚å¸¸æ£€æµ‹ + è¡Œä¸ºåˆ†æ'],
        ])
        
        # å†™å…¥æ•°æ®
        for row_idx, (label, value) in enumerate(overview_data, start=1):
            cell_label = ws.cell(row=row_idx, column=1, value=label)
            cell_value = ws.cell(row=row_idx, column=2, value=value)
            
            if label.startswith('===') and label.endswith('==='):
                cell_label.font = Font(bold=True, size=12)
        
        # è®¾ç½®åˆ—å®½
        ws.column_dimensions['A'].width = 30
        ws.column_dimensions['B'].width = 25
        
        format_excel_sheet(ws)


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
def analyze_ip_sources(csv_path, output_path, top_n=100):
    """åˆ†ææ¥æºIP - å…¼å®¹æ¥å£ï¼Œä½¿ç”¨é«˜çº§åˆ†æå™¨"""
    analyzer = AdvancedIPAnalyzer()
    return analyzer.analyze_ip_sources(csv_path, output_path, top_n)


if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("ä½¿ç”¨æ–¹æ³•: python self_08_ip_analyzer_advanced.py <csv_path> <output_path>")
        sys.exit(1)
    
    csv_path = sys.argv[1]
    output_path = sys.argv[2]
    
    log_info("ğŸš€ å¯åŠ¨é«˜çº§IPåˆ†æå™¨...")
    result = analyze_ip_sources(csv_path, output_path, top_n=100)
    if not result.empty:
        log_info("âœ… é«˜çº§IPåˆ†æå®Œæˆï¼")
        print(result.head())
    else:
        log_info("âŒ åˆ†æå¤±è´¥æˆ–æ— æœ‰æ•ˆæ•°æ®")