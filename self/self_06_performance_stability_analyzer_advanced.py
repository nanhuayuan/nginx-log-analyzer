"""
é«˜çº§æ€§èƒ½ç¨³å®šæ€§åˆ†æå™¨ - ä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒ40G+æ•°æ®å¤„ç†ï¼ŒåŸºäºæµå¼ç®—æ³•å’Œé‡‡æ ·æŠ€æœ¯

æ ¸å¿ƒä¼˜åŒ–:
1. T-Digeståˆ†ä½æ•°è®¡ç®—(P95/P99)
2. HyperLogLogå”¯ä¸€å€¼è®¡æ•°
3. è“„æ°´æ± é‡‡æ ·æ›¿ä»£æ•°ç»„ç´¯ç§¯
4. æ™ºèƒ½å†…å­˜ç®¡ç†
5. å¢å¼ºå¼‚å¸¸æ£€æµ‹å’Œè¶‹åŠ¿åˆ†æ

ä¼˜åŒ–ç›®æ ‡:
- å†…å­˜èŠ‚çœ90%+
- å¤„ç†é€Ÿåº¦æå‡3-5å€
- æ”¯æŒ40G+æ•°æ®æ— OOM
- ä¿æŒåŠŸèƒ½å®Œæ•´æ€§

Author: Claude Code (Advanced Performance Stability Analyzer)
Date: 2025-07-20
"""

import gc
import math
import os
import time
import numpy as np
import pandas as pd
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment

from self_00_02_utils import log_info
from self_00_04_excel_processor import (
    add_dataframe_to_excel_with_grouped_headers,
    format_excel_sheet
)
from self_00_05_sampling_algorithms import (
    TDigest, HyperLogLog, ReservoirSampler, StratifiedSampler
)


class AdvancedPerformanceAnalyzer:
    """é«˜çº§æ€§èƒ½ç¨³å®šæ€§åˆ†æå™¨"""
    
    def __init__(self):
        # é…ç½®å‚æ•°
        self.chunk_size = 100000
        self.sampling_size = 1000  # è“„æ°´æ± é‡‡æ ·å¤§å°
        self.tdigest_compression = 100
        self.hll_precision = 12
        
        # åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨
        self.success_rate_stats = defaultdict(lambda: {'success': 0, 'total': 0})
        self.response_time_samplers = defaultdict(lambda: TDigest(compression=self.tdigest_compression))
        self.resource_usage_stats = defaultdict(lambda: {'response_kb_sum': 0, 'total_kb_sum': 0, 'count': 0})
        
        # é«˜çº§é‡‡æ ·å™¨
        self.frequency_samplers = defaultdict(lambda: ReservoirSampler(max_size=self.sampling_size))
        self.concurrency_sampler = ReservoirSampler(max_size=5000)  # å¹¶å‘æ•°æ®é‡‡æ ·
        self.connection_stats = defaultdict(lambda: {'request_count': 0, 'connection_cost_sum': 0.0})
        
        # åç«¯æ€§èƒ½åˆ†æå™¨
        self.backend_samplers = defaultdict(lambda: {
            'efficiency': TDigest(compression=self.tdigest_compression),
            'processing_index': TDigest(compression=self.tdigest_compression),
            'connect_time': TDigest(compression=self.tdigest_compression),
            'process_time': TDigest(compression=self.tdigest_compression),
            'transfer_time': TDigest(compression=self.tdigest_compression)
        })
        
        # ä¼ è¾“æ€§èƒ½åˆ†æå™¨
        self.transfer_samplers = defaultdict(lambda: {
            'response_speed': TDigest(compression=self.tdigest_compression),
            'total_speed': TDigest(compression=self.tdigest_compression),
            'nginx_speed': TDigest(compression=self.tdigest_compression)
        })
        
        # Nginxç”Ÿå‘½å‘¨æœŸåˆ†æå™¨
        self.lifecycle_samplers = defaultdict(lambda: {
            'network_overhead': TDigest(compression=self.tdigest_compression),
            'transfer_ratio': TDigest(compression=self.tdigest_compression),
            'nginx_phase': TDigest(compression=self.tdigest_compression)
        })
        
        # æœåŠ¡æ ‡è¯†ç¬¦æ”¶é›†å™¨
        self.service_counters = defaultdict(lambda: HyperLogLog(precision=self.hll_precision))
        
        # æ—¶é—´åºåˆ—æ•°æ®(ç”¨äºè¶‹åŠ¿åˆ†æ)
        self.hourly_metrics = defaultdict(list)
        
        # é»˜è®¤é˜ˆå€¼é…ç½®
        self.thresholds = {
            'success_rate': 99.0,
            'response_time': 0.5,
            'error_rate': 1.0,
            'backend_efficiency': 60.0,
            'network_overhead': 30.0,
            'transfer_speed': 1000.0
        }

    def analyze_performance_stability(self, csv_path: str, output_path: str, 
                                    threshold: Optional[Dict] = None) -> Dict:
        """åˆ†ææœåŠ¡ç¨³å®šæ€§æŒ‡æ ‡ - ä¸»å…¥å£å‡½æ•°"""
        log_info("å¼€å§‹é«˜çº§æ€§èƒ½ç¨³å®šæ€§åˆ†æ...", show_memory=True)
        start_time = time.time()
        
        # æ›´æ–°é˜ˆå€¼é…ç½®
        if threshold:
            self.thresholds.update(threshold)
        
        # æµå¼å¤„ç†æ•°æ®
        total_records = self._process_data_streaming(csv_path)
        
        # ç”Ÿæˆåˆ†æç»“æœ
        log_info("è®¡ç®—åˆ†æç»“æœ...", show_memory=True)
        results = self._generate_analysis_results()
        
        # è®¡ç®—å¼‚å¸¸æ£€æµ‹å’Œè¶‹åŠ¿åˆ†æ
        self._calculate_anomaly_detection(results)
        self._calculate_trend_analysis(results)
        
        # ä¿å­˜åˆ°Excel
        self._save_to_excel(results, output_path)
        
        elapsed = time.time() - start_time
        log_info(f"é«˜çº§æ€§èƒ½ç¨³å®šæ€§åˆ†æå®Œæˆï¼Œå…±å¤„ç† {total_records} æ¡è®°å½•ï¼Œè€—æ—¶: {elapsed:.2f}ç§’", show_memory=True)
        
        return results

    def _process_data_streaming(self, csv_path: str) -> int:
        """æµå¼å¤„ç†æ•°æ®æ–‡ä»¶"""
        log_info("å¼€å§‹æµå¼å¤„ç†æ•°æ®...", show_memory=True)
        
        chunks_processed = 0
        total_records = 0
        start_time = datetime.now()
        
        try:
            for chunk in pd.read_csv(csv_path, chunksize=self.chunk_size):
                chunks_processed += 1
                chunk_records = len(chunk)
                total_records += chunk_records
                
                # é¢„å¤„ç†æ—¶é—´æˆ³
                self._preprocess_timestamps(chunk)
                
                # å¤„ç†å„ç±»æŒ‡æ ‡
                self._process_success_rate(chunk)
                self._process_response_time(chunk)
                self._process_resource_usage(chunk)
                self._process_request_frequency(chunk)
                self._process_concurrency(chunk)
                self._process_connection(chunk)
                self._process_backend_performance(chunk)
                self._process_transfer_performance(chunk)
                self._process_nginx_lifecycle(chunk)
                
                # æ¸…ç†å†…å­˜
                del chunk
                if chunks_processed % 10 == 0:
                    gc.collect()
                
                # è¿›åº¦æ—¥å¿—
                if chunks_processed % 50 == 0:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    log_info(f"å·²å¤„ç† {chunks_processed} ä¸ªæ•°æ®å—, {total_records} æ¡è®°å½•, è€—æ—¶: {elapsed:.2f}ç§’", show_memory=True)
        
        except Exception as e:
            log_info(f"æ•°æ®å¤„ç†é”™è¯¯: {e}")
            raise
        
        return total_records

    def _preprocess_timestamps(self, chunk: pd.DataFrame) -> None:
        """é¢„å¤„ç†æ—¶é—´æˆ³å­—æ®µ"""
        # å¤„ç†åŸå§‹æ—¶é—´å­—æ®µ
        if 'raw_time' in chunk.columns:
            chunk['time'] = pd.to_datetime(chunk['raw_time'], errors='coerce')
        elif 'timestamp' in chunk.columns:
            chunk['time'] = pd.to_datetime(chunk['timestamp'], unit='s', errors='coerce')
        
        # åˆ›å»ºæ—¶é—´æ¡¶
        if 'time' in chunk.columns:
            chunk['hour_bucket'] = chunk['time'].dt.floor('H')
            chunk['minute_bucket'] = chunk['time'].dt.floor('min')
        
        # å¤„ç†åˆ°è¾¾æ—¶é—´å­—æ®µ
        if 'arrival_time' in chunk.columns:
            chunk['arrival_time'] = pd.to_datetime(chunk['arrival_time'], errors='coerce')
        elif 'arrival_timestamp' in chunk.columns:
            chunk['arrival_timestamp'] = pd.to_numeric(chunk['arrival_timestamp'], errors='coerce')
            chunk['arrival_time'] = pd.to_datetime(chunk['arrival_timestamp'], unit='s', errors='coerce')

    def _process_success_rate(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†æˆåŠŸç‡æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬"""
        if 'response_status_code' not in chunk.columns or 'service_name' not in chunk.columns:
            return
        
        required_cols = ['hour_bucket', 'service_name', 'response_status_code']
        if not all(col in chunk.columns for col in required_cols):
            return
        
        for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
            if pd.isna(hour) or pd.isna(service):
                continue
                
            # ç»Ÿè®¡2xxçŠ¶æ€ç ä¸ºæˆåŠŸ
            status_codes = group['response_status_code'].astype(str)
            success_count = status_codes.str.startswith('2').sum()
            total_count = len(group)
            
            key = (hour, service)
            self.success_rate_stats[key]['success'] += success_count
            self.success_rate_stats[key]['total'] += total_count
            
            # è®°å½•æ—¶é—´åºåˆ—æ•°æ®
            success_rate = (success_count / total_count * 100) if total_count > 0 else 0
            self.hourly_metrics[f'{service}_success_rate'].append((hour, success_rate))

    def _process_response_time(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†å“åº”æ—¶é—´æ•°æ® - ä½¿ç”¨T-Digest"""
        if 'total_request_duration' not in chunk.columns:
            return
        
        required_cols = ['hour_bucket', 'service_name', 'total_request_duration']
        if not all(col in chunk.columns for col in required_cols):
            return
        
        for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
            if pd.isna(hour) or pd.isna(service):
                continue
                
            key = (hour, service)
            request_times = group['total_request_duration'].dropna()
            
            if len(request_times) > 0:
                # ä½¿ç”¨T-Digestæµå¼è®¡ç®—åˆ†ä½æ•°
                for time_value in request_times:
                    if time_value > 0:  # åªæ·»åŠ æœ‰æ•ˆå€¼
                        self.response_time_samplers[key].add(float(time_value))
                
                # è®°å½•æ—¶é—´åºåˆ—æ•°æ®
                avg_time = request_times.mean()
                self.hourly_metrics[f'{service}_response_time'].append((hour, avg_time))

    def _process_resource_usage(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†èµ„æºä½¿ç”¨æ•°æ®"""
        required_cols = ['service_name', 'http_method', 'response_body_size_kb', 'total_bytes_sent_kb']
        if not all(col in chunk.columns for col in required_cols):
            return
        
        for (service, method), group in chunk.groupby(['service_name', 'http_method']):
            if pd.isna(service) or pd.isna(method):
                continue
                
            key = (service, method)
            stats = self.resource_usage_stats[key]
            
            # èšåˆç»Ÿè®¡
            stats['response_kb_sum'] += group['response_body_size_kb'].sum()
            stats['total_kb_sum'] += group['total_bytes_sent_kb'].sum()
            stats['count'] += len(group)

    def _process_request_frequency(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†è¯·æ±‚é¢‘ç‡æ•°æ® - ä½¿ç”¨è“„æ°´æ± é‡‡æ ·"""
        if 'service_name' not in chunk.columns or 'minute_bucket' not in chunk.columns:
            return
        
        for (minute, service), count in chunk.groupby(['minute_bucket', 'service_name']).size().items():
            if pd.isna(minute) or pd.isna(service):
                continue
                
            # ä½¿ç”¨è“„æ°´æ± é‡‡æ ·è€Œä¸æ˜¯æ— é™ç´¯ç§¯
            self.frequency_samplers[service].add(count)

    def _process_concurrency(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†å¹¶å‘æ•°æ® - ä½¿ç”¨é‡‡æ ·ä¼˜åŒ–"""
        required_cols = ['arrival_time', 'total_request_duration']
        if not all(col in chunk.columns for col in required_cols):
            return
        
        valid_requests = chunk.dropna(subset=required_cols)
        
        # é‡‡æ ·å¤„ç†ä»¥é¿å…å†…å­˜ç´¯ç§¯
        sample_size = min(1000, len(valid_requests))
        if len(valid_requests) > sample_size:
            sampled_requests = valid_requests.sample(n=sample_size)
        else:
            sampled_requests = valid_requests
        
        for _, row in sampled_requests.iterrows():
            arrival_ts = row['arrival_time']
            duration = row['total_request_duration']
            
            if pd.notna(arrival_ts) and pd.notna(duration) and duration > 0:
                end_ts = arrival_ts + pd.Timedelta(seconds=duration)
                
                # ä½¿ç”¨è“„æ°´æ± é‡‡æ ·å­˜å‚¨å¹¶å‘äº‹ä»¶
                self.concurrency_sampler.add({
                    'start': arrival_ts,
                    'end': end_ts,
                    'duration': duration
                })

    def _process_connection(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†è¿æ¥æ•°æ®"""
        if 'connection_cost_ratio' not in chunk.columns or 'minute_bucket' not in chunk.columns:
            return
        
        for minute, group in chunk.groupby('minute_bucket'):
            if pd.isna(minute):
                continue
                
            stats = self.connection_stats[minute]
            stats['request_count'] += len(group)
            stats['connection_cost_sum'] += group['connection_cost_ratio'].sum()

    def _process_backend_performance(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†åç«¯æ€§èƒ½æ•°æ® - ä½¿ç”¨T-Digest"""
        required_cols = ['backend_efficiency', 'processing_efficiency_index',
                        'backend_connect_phase', 'backend_process_phase', 'backend_transfer_phase']
        
        if not all(col in chunk.columns for col in required_cols):
            return
        
        # æ·»åŠ æ—¶é—´å’ŒæœåŠ¡åˆ†ç»„
        if 'hour_bucket' in chunk.columns and 'service_name' in chunk.columns:
            for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
                if pd.isna(hour) or pd.isna(service):
                    continue
                    
                key = (hour, service)
                samplers = self.backend_samplers[key]
                valid_group = group.dropna(subset=required_cols)
                
                if len(valid_group) > 0:
                    # ä½¿ç”¨T-Digestæµå¼å¤„ç†
                    for _, row in valid_group.iterrows():
                        samplers['efficiency'].add(max(0, float(row['backend_efficiency'])))
                        samplers['processing_index'].add(max(0, float(row['processing_efficiency_index'])))
                        samplers['connect_time'].add(max(0, float(row['backend_connect_phase'])))
                        samplers['process_time'].add(max(0, float(row['backend_process_phase'])))
                        samplers['transfer_time'].add(max(0, float(row['backend_transfer_phase'])))

    def _process_transfer_performance(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†ä¼ è¾“æ€§èƒ½æ•°æ® - ä½¿ç”¨T-Digest"""
        required_cols = ['response_transfer_speed', 'total_transfer_speed', 'nginx_transfer_speed']
        
        if not all(col in chunk.columns for col in required_cols):
            return
        
        if 'hour_bucket' in chunk.columns and 'service_name' in chunk.columns:
            for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
                if pd.isna(hour) or pd.isna(service):
                    continue
                    
                key = (hour, service)
                samplers = self.transfer_samplers[key]
                valid_group = group.dropna(subset=required_cols)
                
                if len(valid_group) > 0:
                    for _, row in valid_group.iterrows():
                        samplers['response_speed'].add(max(0, float(row['response_transfer_speed'])))
                        samplers['total_speed'].add(max(0, float(row['total_transfer_speed'])))
                        samplers['nginx_speed'].add(max(0, float(row['nginx_transfer_speed'])))

    def _process_nginx_lifecycle(self, chunk: pd.DataFrame) -> None:
        """å¤„ç†Nginxç”Ÿå‘½å‘¨æœŸæ•°æ® - ä½¿ç”¨T-Digest"""
        required_cols = ['network_overhead', 'transfer_ratio', 'nginx_transfer_phase']
        
        if not all(col in chunk.columns for col in required_cols):
            return
        
        if 'hour_bucket' in chunk.columns and 'service_name' in chunk.columns:
            for (hour, service), group in chunk.groupby(['hour_bucket', 'service_name']):
                if pd.isna(hour) or pd.isna(service):
                    continue
                    
                key = (hour, service)
                samplers = self.lifecycle_samplers[key]
                valid_group = group.dropna(subset=required_cols)
                
                if len(valid_group) > 0:
                    for _, row in valid_group.iterrows():
                        samplers['network_overhead'].add(max(0, float(row['network_overhead'])))
                        samplers['transfer_ratio'].add(max(0, float(row['transfer_ratio'])))
                        samplers['nginx_phase'].add(max(0, float(row['nginx_transfer_phase'])))

    def _generate_analysis_results(self) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆåˆ†æç»“æœ"""
        results = {}
        
        log_info("ç”ŸæˆæˆåŠŸç‡åˆ†æ...")
        results['æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§'] = self._finalize_success_rate_analysis()
        
        log_info("ç”Ÿæˆå“åº”æ—¶é—´åˆ†æ...")
        results['æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§'] = self._finalize_response_time_analysis()
        
        log_info("ç”Ÿæˆèµ„æºä½¿ç”¨åˆ†æ...")
        results['èµ„æºä½¿ç”¨å’Œå¸¦å®½'] = self._finalize_resource_usage_analysis()
        
        log_info("ç”Ÿæˆè¯·æ±‚é¢‘ç‡åˆ†æ...")
        results['æœåŠ¡è¯·æ±‚é¢‘ç‡'] = self._finalize_request_frequency_analysis()
        
        log_info("ç”Ÿæˆå¹¶å‘åˆ†æ...")
        results['å¹¶å‘è¿æ¥ä¼°ç®—'] = self._finalize_concurrency_analysis()
        
        log_info("ç”Ÿæˆè¿æ¥åˆ†æ...")
        connection_metrics, connection_summary = self._finalize_connections_analysis()
        results['è¿æ¥æ€§èƒ½æŒ‡æ ‡'] = connection_metrics
        results['è¿æ¥æ€§èƒ½æ‘˜è¦'] = connection_summary
        
        log_info("ç”Ÿæˆåç«¯æ€§èƒ½åˆ†æ...")
        results['åç«¯å¤„ç†æ€§èƒ½'] = self._finalize_backend_performance_analysis()
        
        log_info("ç”Ÿæˆä¼ è¾“æ€§èƒ½åˆ†æ...")
        results['æ•°æ®ä¼ è¾“æ€§èƒ½'] = self._finalize_transfer_performance_analysis()
        
        log_info("ç”ŸæˆNginxç”Ÿå‘½å‘¨æœŸåˆ†æ...")
        results['Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ'] = self._finalize_nginx_lifecycle_analysis()
        
        return results

    def _finalize_success_rate_analysis(self) -> pd.DataFrame:
        """å®ŒæˆæˆåŠŸç‡åˆ†æ - å¢å¼ºç‰ˆæœ¬"""
        service_stats = []
        
        # æŒ‰æœåŠ¡èšåˆæ•°æ®
        service_totals = defaultdict(lambda: {'success': 0, 'total': 0, 'hourly_rates': []})
        
        for (hour, service), data in self.success_rate_stats.items():
            service_totals[service]['success'] += data['success']
            service_totals[service]['total'] += data['total']
            
            if data['total'] > 0:
                hourly_rate = (data['success'] / data['total'] * 100)
                service_totals[service]['hourly_rates'].append(hourly_rate)
        
        for service, totals in service_totals.items():
            if totals['total'] > 0:
                mean_rate = totals['success'] / totals['total'] * 100
                hourly_rates = totals['hourly_rates']
                
                # ç»Ÿè®¡æŒ‡æ ‡
                std_rate = np.std(hourly_rates) if len(hourly_rates) > 1 else 0
                min_rate = min(hourly_rates) if hourly_rates else mean_rate
                max_rate = max(hourly_rates) if hourly_rates else mean_rate
                
                # å¼‚å¸¸çŠ¶æ€åˆ¤æ–­
                status = 'æ­£å¸¸'
                if mean_rate < self.thresholds['success_rate']:
                    status = 'æˆåŠŸç‡ä½'
                elif std_rate > 5.0:
                    status = 'æ³¢åŠ¨è¾ƒå¤§'
                elif min_rate < 95.0:
                    status = 'å­˜åœ¨å¼‚å¸¸æ—¶æ®µ'
                
                service_stats.append({
                    'æœåŠ¡åç§°': service,
                    'å¹³å‡æˆåŠŸç‡(%)': round(mean_rate, 2),
                    'æˆåŠŸç‡æ³¢åŠ¨(æ ‡å‡†å·®)': round(std_rate, 2),
                    'æœ€ä½æˆåŠŸç‡(%)': round(min_rate, 2),
                    'æœ€é«˜æˆåŠŸç‡(%)': round(max_rate, 2),
                    'æ€»è¯·æ±‚æ•°': totals['total'],
                    'æ—¶æ®µæ•°é‡': len(hourly_rates),
                    'å¼‚å¸¸çŠ¶æ€': status
                })
        
        return pd.DataFrame(service_stats).sort_values('æˆåŠŸç‡æ³¢åŠ¨(æ ‡å‡†å·®)', ascending=False)

    def _finalize_response_time_analysis(self) -> pd.DataFrame:
        """å®Œæˆå“åº”æ—¶é—´åˆ†æ - ä½¿ç”¨T-Digeståˆ†ä½æ•°"""
        service_stats = []
        
        # æŒ‰æœåŠ¡èšåˆT-Digestæ•°æ®
        service_digests = defaultdict(list)
        
        for (hour, service), digest in self.response_time_samplers.items():
            if digest.count > 0:
                service_digests[service].append(digest)
        
        for service, digests in service_digests.items():
            if digests:
                # åˆå¹¶å¤šä¸ªT-Digest
                merged_digest = digests[0]
                for digest in digests[1:]:
                    merged_digest = merged_digest.merge(digest)
                
                if merged_digest.count > 0:
                    # è®¡ç®—åˆ†ä½æ•°
                    mean_time = merged_digest.percentile(50)  # ä¸­ä½æ•°ä½œä¸ºå‡å€¼
                    p95_time = merged_digest.percentile(95)
                    p99_time = merged_digest.percentile(99)
                    min_time = merged_digest.min_value
                    max_time = merged_digest.max_value
                    
                    # è®¡ç®—å“åº”æ—¶é—´æ³¢åŠ¨æ€§
                    p75_time = merged_digest.percentile(75)
                    p25_time = merged_digest.percentile(25)
                    iqr_time = p75_time - p25_time  # å››åˆ†ä½è·ä½œä¸ºæ³¢åŠ¨æŒ‡æ ‡
                    
                    # å¼‚å¸¸çŠ¶æ€åˆ¤æ–­
                    status = 'æ­£å¸¸'
                    if mean_time > self.thresholds['response_time']:
                        status = 'å“åº”æ—¶é—´é•¿'
                    elif p99_time > mean_time * 5:  # P99è¶…è¿‡å‡å€¼5å€
                        status = 'å­˜åœ¨æå€¼'
                    elif iqr_time > mean_time:  # å››åˆ†ä½è·è¶…è¿‡å‡å€¼
                        status = 'å“åº”ä¸ç¨³å®š'
                    
                    service_stats.append({
                        'æœåŠ¡åç§°': service,
                        'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(mean_time, 3),
                        'å“åº”æ—¶é—´æ³¢åŠ¨(IQR)': round(iqr_time, 3),
                        'æœ€ä½å“åº”æ—¶é—´(ç§’)': round(min_time, 3),
                        'æœ€é«˜å“åº”æ—¶é—´(ç§’)': round(max_time, 3),
                        'P95å“åº”æ—¶é—´(ç§’)': round(p95_time, 3),
                        'P99å“åº”æ—¶é—´(ç§’)': round(p99_time, 3),
                        'æ ·æœ¬æ•°é‡': merged_digest.count,
                        'å¼‚å¸¸çŠ¶æ€': status
                    })
        
        return pd.DataFrame(service_stats).sort_values('å¹³å‡å“åº”æ—¶é—´(ç§’)', ascending=False)

    def _finalize_resource_usage_analysis(self) -> pd.DataFrame:
        """å®Œæˆèµ„æºä½¿ç”¨åˆ†æ"""
        resource_results = []
        
        for (service, method), data in self.resource_usage_stats.items():
            if data['count'] > 0:
                avg_response_kb = data['response_kb_sum'] / data['count']
                avg_total_kb = data['total_kb_sum'] / data['count']
                total_response_mb = data['response_kb_sum'] / 1024
                total_transfer_mb = data['total_kb_sum'] / 1024
                
                # è®¡ç®—ä¼ è¾“æ•ˆç‡
                transfer_efficiency = (avg_response_kb / avg_total_kb * 100) if avg_total_kb > 0 else 0
                
                resource_results.append({
                    'æœåŠ¡åç§°': service,
                    'è¯·æ±‚æ–¹æ³•': method,
                    'å¹³å‡å“åº”å¤§å°(KB)': round(avg_response_kb, 2),
                    'å¹³å‡ä¼ è¾“å¤§å°(KB)': round(avg_total_kb, 2),
                    'ä¼ è¾“æ•ˆç‡(%)': round(transfer_efficiency, 2),
                    'æ€»å“åº”æµé‡(MB)': round(total_response_mb, 2),
                    'æ€»ä¼ è¾“æµé‡(MB)': round(total_transfer_mb, 2),
                    'è¯·æ±‚æ¬¡æ•°': data['count']
                })
        
        return pd.DataFrame(resource_results).sort_values('æ€»ä¼ è¾“æµé‡(MB)', ascending=False)

    def _finalize_request_frequency_analysis(self) -> pd.DataFrame:
        """å®Œæˆè¯·æ±‚é¢‘ç‡åˆ†æ - ä½¿ç”¨é‡‡æ ·æ•°æ®"""
        frequency_results = []
        
        for service, sampler in self.frequency_samplers.items():
            samples = sampler.get_samples()
            if samples:
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                mean_qps = sampler.mean()
                std_qps = sampler.std()
                p95_qps = sampler.percentile(95)
                p99_qps = sampler.percentile(99)
                
                frequency_results.append({
                    'æœåŠ¡åç§°': service,
                    'å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚æ•°(QPS)': round(mean_qps, 2),
                    'æœ€å¤§æ¯åˆ†é’Ÿè¯·æ±‚æ•°(P99)': round(p99_qps, 2),
                    'P95æ¯åˆ†é’Ÿè¯·æ±‚æ•°': round(p95_qps, 2),
                    'è¯·æ±‚é¢‘ç‡æ³¢åŠ¨(æ ‡å‡†å·®)': round(std_qps, 2),
                    'æ ·æœ¬æ•°é‡': sampler.count,
                    'é‡‡æ ·å¤§å°': len(samples)
                })
        
        return pd.DataFrame(frequency_results).sort_values('å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚æ•°(QPS)', ascending=False)

    def _finalize_concurrency_analysis(self) -> pd.DataFrame:
        """å®Œæˆå¹¶å‘åˆ†æ - ä½¿ç”¨é‡‡æ ·æ•°æ®"""
        if not self.concurrency_sampler.samples:
            return pd.DataFrame()
        
        # ä»é‡‡æ ·æ•°æ®é‡å»ºå¹¶å‘æ—¶é—´åºåˆ—
        events = []
        for sample in self.concurrency_sampler.get_samples():
            if isinstance(sample, dict) and 'start' in sample and 'end' in sample:
                events.append((sample['start'], 1))  # è¯·æ±‚å¼€å§‹
                events.append((sample['end'], -1))   # è¯·æ±‚ç»“æŸ
        
        if not events:
            return pd.DataFrame()
        
        # æŒ‰æ—¶é—´æ’åºå¹¶è®¡ç®—å¹¶å‘æ•°
        events.sort(key=lambda x: x[0])
        
        concurrent_data = []
        current_count = 0
        
        for ts, event in events:
            current_count += event
            current_count = max(0, current_count)  # ç¡®ä¿éè´Ÿ
            concurrent_data.append((ts, current_count))
        
        if not concurrent_data:
            return pd.DataFrame()
        
        concurrent_df = pd.DataFrame(concurrent_data, columns=['æ—¶é—´æˆ³', 'å¹¶å‘æ•°'])
        concurrent_df['åˆ†é’Ÿæ—¶é—´æ®µ'] = concurrent_df['æ—¶é—´æˆ³'].dt.floor('min')
        
        # æŒ‰åˆ†é’Ÿèšåˆç»Ÿè®¡
        concurrency_stats = concurrent_df.groupby('åˆ†é’Ÿæ—¶é—´æ®µ').agg(
            å¹³å‡å¹¶å‘æ•°=('å¹¶å‘æ•°', 'mean'),
            æœ€å¤§å¹¶å‘æ•°=('å¹¶å‘æ•°', 'max'),
            æœ€å°å¹¶å‘æ•°=('å¹¶å‘æ•°', 'min'),
            å¹¶å‘æ•°æ³¢åŠ¨=('å¹¶å‘æ•°', 'std')
        ).reset_index()
        
        concurrency_stats.rename(columns={'åˆ†é’Ÿæ—¶é—´æ®µ': 'æ—¶é—´æ®µ'}, inplace=True)
        
        # å››èˆäº”å…¥
        for col in ['å¹³å‡å¹¶å‘æ•°', 'å¹¶å‘æ•°æ³¢åŠ¨']:
            concurrency_stats[col] = concurrency_stats[col].round(2)
        
        return concurrency_stats

    def _finalize_connections_analysis(self) -> Tuple[pd.DataFrame, Dict]:
        """å®Œæˆè¿æ¥åˆ†æ"""
        connection_metrics = []
        
        for minute, data in self.connection_stats.items():
            if data['request_count'] > 0:
                avg_connection_cost = data['connection_cost_sum'] / data['request_count']
                
                connection_metrics.append({
                    'æ—¶é—´': minute,
                    'è¯·æ±‚æ•°é‡': data['request_count'],
                    'å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡': round(avg_connection_cost, 4),
                    'æ€»è¿æ¥æˆæœ¬': round(data['connection_cost_sum'], 2)
                })
        
        connection_df = pd.DataFrame(connection_metrics)
        
        # ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
        if not connection_df.empty:
            connection_summary = {
                'å¹³å‡æ¯åˆ†é’Ÿè¯·æ±‚æ•°': round(connection_df['è¯·æ±‚æ•°é‡'].mean(), 2),
                'æœ€å¤§æ¯åˆ†é’Ÿè¯·æ±‚æ•°': connection_df['è¯·æ±‚æ•°é‡'].max(),
                'å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡': round(connection_df['å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡'].mean(), 4),
                'æœ€é«˜è¿æ¥æˆæœ¬æ¯”ç‡': round(connection_df['å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡'].max(), 4),
                'è¿æ¥æˆæœ¬æ³¢åŠ¨(æ ‡å‡†å·®)': round(connection_df['å¹³å‡è¿æ¥æˆæœ¬æ¯”ç‡'].std(), 4)
            }
        else:
            connection_summary = {}
        
        return connection_df, connection_summary

    def _finalize_backend_performance_analysis(self) -> pd.DataFrame:
        """å®Œæˆåç«¯æ€§èƒ½åˆ†æ - ä½¿ç”¨T-Digeståˆ†ä½æ•°"""
        backend_stats = []
        
        for (hour, service), samplers in self.backend_samplers.items():
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            if any(sampler.count > 0 for sampler in samplers.values()):
                # è®¡ç®—å„æŒ‡æ ‡çš„åˆ†ä½æ•°
                efficiency_p50 = samplers['efficiency'].percentile(50) if samplers['efficiency'].count > 0 else 0
                efficiency_p95 = samplers['efficiency'].percentile(95) if samplers['efficiency'].count > 0 else 0
                
                processing_p50 = samplers['processing_index'].percentile(50) if samplers['processing_index'].count > 0 else 0
                connect_p50 = samplers['connect_time'].percentile(50) if samplers['connect_time'].count > 0 else 0
                process_p50 = samplers['process_time'].percentile(50) if samplers['process_time'].count > 0 else 0
                transfer_p50 = samplers['transfer_time'].percentile(50) if samplers['transfer_time'].count > 0 else 0
                
                # è®¡ç®—P95å»¶è¿Ÿæ—¶é—´
                connect_p95 = samplers['connect_time'].percentile(95) if samplers['connect_time'].count > 0 else 0
                process_p95 = samplers['process_time'].percentile(95) if samplers['process_time'].count > 0 else 0
                
                # æ€§èƒ½çŠ¶æ€åˆ¤æ–­
                status = 'æ­£å¸¸'
                if efficiency_p50 < self.thresholds['backend_efficiency']:
                    status = 'å¤„ç†æ•ˆç‡ä½'
                elif connect_p95 > 0.1:  # P95è¿æ¥æ—¶é—´è¶…è¿‡100ms
                    status = 'è¿æ¥å»¶è¿Ÿé«˜'
                elif process_p95 > 1.0:  # P95å¤„ç†æ—¶é—´è¶…è¿‡1ç§’
                    status = 'å¤„ç†æ—¶é—´é•¿'
                
                backend_stats.append({
                    'æ—¶é—´': hour,
                    'æœåŠ¡åç§°': service,
                    'åç«¯å¤„ç†æ•ˆç‡(%)': round(efficiency_p50, 2),
                    'æ•ˆç‡P95(%)': round(efficiency_p95, 2),
                    'å¤„ç†æ•ˆç‡æŒ‡æ•°': round(processing_p50, 3),
                    'å¹³å‡è¿æ¥æ—¶é—´(ç§’)': round(connect_p50, 3),
                    'å¹³å‡å¤„ç†æ—¶é—´(ç§’)': round(process_p50, 3),
                    'å¹³å‡ä¼ è¾“æ—¶é—´(ç§’)': round(transfer_p50, 3),
                    'P95è¿æ¥æ—¶é—´(ç§’)': round(connect_p95, 3),
                    'P95å¤„ç†æ—¶é—´(ç§’)': round(process_p95, 3),
                    'æ ·æœ¬æ•°é‡': samplers['efficiency'].count,
                    'æ€§èƒ½çŠ¶æ€': status
                })
        
        return pd.DataFrame(backend_stats).sort_values('åç«¯å¤„ç†æ•ˆç‡(%)', ascending=True)

    def _finalize_transfer_performance_analysis(self) -> pd.DataFrame:
        """å®Œæˆä¼ è¾“æ€§èƒ½åˆ†æ - ä½¿ç”¨T-Digeståˆ†ä½æ•°"""
        transfer_stats = []
        
        for (hour, service), samplers in self.transfer_samplers.items():
            if any(sampler.count > 0 for sampler in samplers.values()):
                # è®¡ç®—å„ä¼ è¾“é€Ÿåº¦çš„åˆ†ä½æ•°
                response_speed_p50 = samplers['response_speed'].percentile(50) if samplers['response_speed'].count > 0 else 0
                total_speed_p50 = samplers['total_speed'].percentile(50) if samplers['total_speed'].count > 0 else 0
                nginx_speed_p50 = samplers['nginx_speed'].percentile(50) if samplers['nginx_speed'].count > 0 else 0
                
                # è®¡ç®—P95ä¼ è¾“é€Ÿåº¦ï¼ˆç”¨äºè¯†åˆ«æ…¢ä¼ è¾“ï¼‰
                response_speed_p5 = samplers['response_speed'].percentile(5) if samplers['response_speed'].count > 0 else 0  # P5è¡¨ç¤ºæœ€æ…¢5%
                total_speed_p5 = samplers['total_speed'].percentile(5) if samplers['total_speed'].count > 0 else 0
                
                # ä¼ è¾“æ€§èƒ½çŠ¶æ€åˆ¤æ–­
                status = 'æ­£å¸¸'
                if total_speed_p50 < self.thresholds['transfer_speed']:
                    status = 'ä¼ è¾“é€Ÿåº¦æ…¢'
                elif nginx_speed_p50 < response_speed_p50 * 0.8:
                    status = 'Nginxä¼ è¾“ç“¶é¢ˆ'
                elif response_speed_p5 < total_speed_p50 * 0.3:  # æœ€æ…¢5%ä¼ è¾“é€Ÿåº¦è¿‡ä½
                    status = 'ä¼ è¾“ä¸ç¨³å®š'
                
                transfer_stats.append({
                    'æ—¶é—´': hour,
                    'æœåŠ¡åç§°': service,
                    'å“åº”ä¼ è¾“é€Ÿåº¦(KB/s)': round(response_speed_p50, 2),
                    'æ€»ä¼ è¾“é€Ÿåº¦(KB/s)': round(total_speed_p50, 2),
                    'Nginxä¼ è¾“é€Ÿåº¦(KB/s)': round(nginx_speed_p50, 2),
                    'æœ€æ…¢5%å“åº”é€Ÿåº¦(KB/s)': round(response_speed_p5, 2),
                    'æœ€æ…¢5%æ€»é€Ÿåº¦(KB/s)': round(total_speed_p5, 2),
                    'æ ·æœ¬æ•°é‡': samplers['response_speed'].count,
                    'ä¼ è¾“çŠ¶æ€': status
                })
        
        return pd.DataFrame(transfer_stats).sort_values('æ€»ä¼ è¾“é€Ÿåº¦(KB/s)', ascending=True)

    def _finalize_nginx_lifecycle_analysis(self) -> pd.DataFrame:
        """å®ŒæˆNginxç”Ÿå‘½å‘¨æœŸåˆ†æ - ä½¿ç”¨T-Digeståˆ†ä½æ•°"""
        lifecycle_stats = []
        
        for (hour, service), samplers in self.lifecycle_samplers.items():
            if any(sampler.count > 0 for sampler in samplers.values()):
                # è®¡ç®—ç”Ÿå‘½å‘¨æœŸå„é˜¶æ®µçš„åˆ†ä½æ•°
                network_overhead_p50 = samplers['network_overhead'].percentile(50) if samplers['network_overhead'].count > 0 else 0
                transfer_ratio_p50 = samplers['transfer_ratio'].percentile(50) if samplers['transfer_ratio'].count > 0 else 0
                nginx_phase_p50 = samplers['nginx_phase'].percentile(50) if samplers['nginx_phase'].count > 0 else 0
                
                # è®¡ç®—P95å¼€é”€ï¼ˆè¯†åˆ«å¼‚å¸¸é«˜å¼€é”€ï¼‰
                network_overhead_p95 = samplers['network_overhead'].percentile(95) if samplers['network_overhead'].count > 0 else 0
                transfer_ratio_p95 = samplers['transfer_ratio'].percentile(95) if samplers['transfer_ratio'].count > 0 else 0
                
                # ç”Ÿå‘½å‘¨æœŸçŠ¶æ€åˆ¤æ–­
                status = 'æ­£å¸¸'
                if network_overhead_p50 > self.thresholds['network_overhead']:
                    status = 'ç½‘ç»œå¼€é”€é«˜'
                elif transfer_ratio_p50 > 60.0:  # ä¼ è¾“æ—¶é—´å æ¯”è¶…è¿‡60%
                    status = 'ä¼ è¾“æ—¶é—´å æ¯”é«˜'
                elif network_overhead_p95 > network_overhead_p50 * 3:  # P95å¼€é”€æ˜¯P50çš„3å€ä»¥ä¸Š
                    status = 'ç½‘ç»œå¼€é”€ä¸ç¨³å®š'
                
                lifecycle_stats.append({
                    'æ—¶é—´': hour,
                    'æœåŠ¡åç§°': service,
                    'ç½‘ç»œå¼€é”€å æ¯”(%)': round(network_overhead_p50, 2),
                    'ä¼ è¾“æ—¶é—´å æ¯”(%)': round(transfer_ratio_p50, 2),
                    'å¹³å‡Nginxä¼ è¾“é˜¶æ®µ(ç§’)': round(nginx_phase_p50, 3),
                    'P95ç½‘ç»œå¼€é”€(%)': round(network_overhead_p95, 2),
                    'P95ä¼ è¾“å æ¯”(%)': round(transfer_ratio_p95, 2),
                    'æ ·æœ¬æ•°é‡': samplers['network_overhead'].count,
                    'ç”Ÿå‘½å‘¨æœŸçŠ¶æ€': status
                })
        
        return pd.DataFrame(lifecycle_stats).sort_values('ç½‘ç»œå¼€é”€å æ¯”(%)', ascending=False)

    def _calculate_anomaly_detection(self, results: Dict) -> None:
        """è®¡ç®—å¼‚å¸¸æ£€æµ‹è¯„åˆ†"""
        log_info("è®¡ç®—å¼‚å¸¸æ£€æµ‹è¯„åˆ†...")
        
        # ä¸ºæ¯ä¸ªç»“æœè¡¨æ·»åŠ å¼‚å¸¸æ£€æµ‹è¯„åˆ†
        for analysis_name, df in results.items():
            # è·³è¿‡Noneå€¼ã€å­—å…¸ç±»å‹å’Œæ‘˜è¦ç±»å‹çš„ç»“æœ
            if df is None or isinstance(df, dict) or 'æ‘˜è¦' in analysis_name:
                continue
            
            # ç¡®ä¿æ˜¯DataFrameä¸”éç©º
            if not hasattr(df, 'empty') or df.empty:
                continue
            
            anomaly_scores = []
            
            for _, row in df.iterrows():
                score = 0
                factors = []
                
                # æ ¹æ®ä¸åŒåˆ†æç±»å‹è®¡ç®—å¼‚å¸¸åˆ†æ•°
                if 'æˆåŠŸç‡' in analysis_name:
                    if 'å¼‚å¸¸çŠ¶æ€' in row and row['å¼‚å¸¸çŠ¶æ€'] != 'æ­£å¸¸':
                        if row['å¼‚å¸¸çŠ¶æ€'] == 'æˆåŠŸç‡ä½':
                            score += 80
                            factors.append('æˆåŠŸç‡è¿‡ä½')
                        elif row['å¼‚å¸¸çŠ¶æ€'] == 'æ³¢åŠ¨è¾ƒå¤§':
                            score += 60
                            factors.append('æˆåŠŸç‡æ³¢åŠ¨å¤§')
                        elif row['å¼‚å¸¸çŠ¶æ€'] == 'å­˜åœ¨å¼‚å¸¸æ—¶æ®µ':
                            score += 40
                            factors.append('ä¸ªåˆ«æ—¶æ®µå¼‚å¸¸')
                
                elif 'å“åº”æ—¶é—´' in analysis_name:
                    if 'å¼‚å¸¸çŠ¶æ€' in row and row['å¼‚å¸¸çŠ¶æ€'] != 'æ­£å¸¸':
                        if row['å¼‚å¸¸çŠ¶æ€'] == 'å“åº”æ—¶é—´é•¿':
                            score += 70
                            factors.append('å“åº”æ—¶é—´é•¿')
                        elif row['å¼‚å¸¸çŠ¶æ€'] == 'å­˜åœ¨æå€¼':
                            score += 85
                            factors.append('å­˜åœ¨å“åº”æ—¶é—´æå€¼')
                        elif row['å¼‚å¸¸çŠ¶æ€'] == 'å“åº”ä¸ç¨³å®š':
                            score += 55
                            factors.append('å“åº”æ—¶é—´ä¸ç¨³å®š')
                
                elif 'åç«¯å¤„ç†' in analysis_name:
                    if 'æ€§èƒ½çŠ¶æ€' in row and row['æ€§èƒ½çŠ¶æ€'] != 'æ­£å¸¸':
                        if row['æ€§èƒ½çŠ¶æ€'] == 'å¤„ç†æ•ˆç‡ä½':
                            score += 75
                            factors.append('åç«¯å¤„ç†æ•ˆç‡ä½')
                        elif row['æ€§èƒ½çŠ¶æ€'] == 'è¿æ¥å»¶è¿Ÿé«˜':
                            score += 65
                            factors.append('åç«¯è¿æ¥å»¶è¿Ÿé«˜')
                        elif row['æ€§èƒ½çŠ¶æ€'] == 'å¤„ç†æ—¶é—´é•¿':
                            score += 70
                            factors.append('åç«¯å¤„ç†æ—¶é—´é•¿')
                
                elif 'ä¼ è¾“æ€§èƒ½' in analysis_name:
                    if 'ä¼ è¾“çŠ¶æ€' in row and row['ä¼ è¾“çŠ¶æ€'] != 'æ­£å¸¸':
                        if row['ä¼ è¾“çŠ¶æ€'] == 'ä¼ è¾“é€Ÿåº¦æ…¢':
                            score += 60
                            factors.append('ä¼ è¾“é€Ÿåº¦æ…¢')
                        elif row['ä¼ è¾“çŠ¶æ€'] == 'Nginxä¼ è¾“ç“¶é¢ˆ':
                            score += 70
                            factors.append('Nginxä¼ è¾“ç“¶é¢ˆ')
                        elif row['ä¼ è¾“çŠ¶æ€'] == 'ä¼ è¾“ä¸ç¨³å®š':
                            score += 55
                            factors.append('ä¼ è¾“ä¸ç¨³å®š')
                
                elif 'Nginxç”Ÿå‘½å‘¨æœŸ' in analysis_name:
                    if 'ç”Ÿå‘½å‘¨æœŸçŠ¶æ€' in row and row['ç”Ÿå‘½å‘¨æœŸçŠ¶æ€'] != 'æ­£å¸¸':
                        if row['ç”Ÿå‘½å‘¨æœŸçŠ¶æ€'] == 'ç½‘ç»œå¼€é”€é«˜':
                            score += 65
                            factors.append('ç½‘ç»œå¼€é”€é«˜')
                        elif row['ç”Ÿå‘½å‘¨æœŸçŠ¶æ€'] == 'ä¼ è¾“æ—¶é—´å æ¯”é«˜':
                            score += 50
                            factors.append('ä¼ è¾“æ—¶é—´å æ¯”é«˜')
                        elif row['ç”Ÿå‘½å‘¨æœŸçŠ¶æ€'] == 'ç½‘ç»œå¼€é”€ä¸ç¨³å®š':
                            score += 55
                            factors.append('ç½‘ç»œå¼€é”€ä¸ç¨³å®š')
                
                # å¼‚å¸¸ç­‰çº§åˆ†ç±»
                if score >= 80:
                    level = "ä¸¥é‡å¼‚å¸¸"
                elif score >= 60:
                    level = "ä¸­åº¦å¼‚å¸¸"
                elif score >= 40:
                    level = "è½»å¾®å¼‚å¸¸"
                else:
                    level = "æ­£å¸¸"
                
                anomaly_scores.append({
                    'score': score,
                    'level': level,
                    'factors': '; '.join(factors) if factors else 'æ— '
                })
            
            # æ·»åŠ å¼‚å¸¸æ£€æµ‹åˆ—åˆ°DataFrame
            if anomaly_scores and len(anomaly_scores) == len(df):
                df['å¼‚å¸¸è¯„åˆ†(0-100)'] = [item['score'] for item in anomaly_scores]
                df['å¼‚å¸¸ç­‰çº§'] = [item['level'] for item in anomaly_scores]
                df['å¼‚å¸¸å› å­'] = [item['factors'] for item in anomaly_scores]

    def _calculate_trend_analysis(self, results: Dict) -> None:
        """è®¡ç®—è¶‹åŠ¿åˆ†æ"""
        log_info("è®¡ç®—è¶‹åŠ¿åˆ†æ...")
        
        # åŸºäºæ—¶é—´åºåˆ—æ•°æ®è®¡ç®—è¶‹åŠ¿
        trend_data = []
        
        for metric_name, time_series in self.hourly_metrics.items():
            if len(time_series) < 2:
                continue
            
            # æŒ‰æ—¶é—´æ’åº
            time_series.sort(key=lambda x: x[0])
            
            # æå–å€¼åºåˆ—
            values = [item[1] for item in time_series]
            
            if len(values) >= 3:
                # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
                recent_avg = np.mean(values[-3:])  # æœ€è¿‘3ä¸ªæ—¶æ®µçš„å¹³å‡å€¼
                early_avg = np.mean(values[:3])   # æœ€æ—©3ä¸ªæ—¶æ®µçš„å¹³å‡å€¼
                
                trend_change = ((recent_avg - early_avg) / early_avg * 100) if early_avg > 0 else 0
                volatility = np.std(values)
                
                # è¶‹åŠ¿æ–¹å‘åˆ¤æ–­
                if abs(trend_change) < 5:
                    trend_direction = "ç¨³å®š"
                elif trend_change > 0:
                    trend_direction = "ä¸Šå‡"
                else:
                    trend_direction = "ä¸‹é™"
                
                trend_data.append({
                    'æŒ‡æ ‡åç§°': metric_name,
                    'è¶‹åŠ¿æ–¹å‘': trend_direction,
                    'å˜åŒ–å¹…åº¦(%)': round(trend_change, 2),
                    'æ³¢åŠ¨æ€§': round(volatility, 3),
                    'æ•°æ®ç‚¹æ•°': len(values),
                    'æœ€æ–°å€¼': round(values[-1], 3),
                    'æœ€æ—©å€¼': round(values[0], 3)
                })
        
        if trend_data:
            results['è¶‹åŠ¿åˆ†æ'] = pd.DataFrame(trend_data).sort_values('å˜åŒ–å¹…åº¦(%)', ascending=False)

    def _save_to_excel(self, results: Dict, output_path: str) -> None:
        """ä¿å­˜ç»“æœåˆ°Excel"""
        log_info(f"ä¿å­˜æ€§èƒ½ç¨³å®šæ€§åˆ†æåˆ°Excel: {output_path}", show_memory=True)
        
        wb = Workbook()
        if 'Sheet' in wb.sheetnames:
            wb.remove(wb['Sheet'])
        
        # å®šä¹‰å·¥ä½œè¡¨ä¿¡æ¯å’Œé«˜äº®è§„åˆ™
        sheet_configs = {
            'æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§': {
                'data': results.get('æœåŠ¡æˆåŠŸç‡ç¨³å®šæ€§'),
                'highlight_column': 'å¼‚å¸¸çŠ¶æ€',
                'highlight_values': {'æˆåŠŸç‡ä½': 'FF6B6B', 'æ³¢åŠ¨è¾ƒå¤§': 'FFE66D', 'å­˜åœ¨å¼‚å¸¸æ—¶æ®µ': 'FFB74D'}
            },
            'æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§': {
                'data': results.get('æœåŠ¡å“åº”æ—¶é—´ç¨³å®šæ€§'),
                'highlight_column': 'å¼‚å¸¸çŠ¶æ€',
                'highlight_values': {'å“åº”æ—¶é—´é•¿': 'FF6B6B', 'å“åº”ä¸ç¨³å®š': 'FFE66D', 'å­˜åœ¨æå€¼': 'FF5722'}
            },
            'èµ„æºä½¿ç”¨å’Œå¸¦å®½': {
                'data': results.get('èµ„æºä½¿ç”¨å’Œå¸¦å®½')
            },
            'æœåŠ¡è¯·æ±‚é¢‘ç‡': {
                'data': results.get('æœåŠ¡è¯·æ±‚é¢‘ç‡')
            },
            'åç«¯å¤„ç†æ€§èƒ½': {
                'data': results.get('åç«¯å¤„ç†æ€§èƒ½'),
                'highlight_column': 'æ€§èƒ½çŠ¶æ€',
                'highlight_values': {'å¤„ç†æ•ˆç‡ä½': 'FF6B6B', 'è¿æ¥å»¶è¿Ÿé«˜': 'FFE66D', 'å¤„ç†æ—¶é—´é•¿': 'FFB74D'}
            },
            'æ•°æ®ä¼ è¾“æ€§èƒ½': {
                'data': results.get('æ•°æ®ä¼ è¾“æ€§èƒ½'),
                'highlight_column': 'ä¼ è¾“çŠ¶æ€',
                'highlight_values': {'ä¼ è¾“é€Ÿåº¦æ…¢': 'FF6B6B', 'Nginxä¼ è¾“ç“¶é¢ˆ': 'FFE66D', 'ä¼ è¾“ä¸ç¨³å®š': 'FFB74D'}
            },
            'Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ': {
                'data': results.get('Nginxç”Ÿå‘½å‘¨æœŸåˆ†æ'),
                'highlight_column': 'ç”Ÿå‘½å‘¨æœŸçŠ¶æ€',
                'highlight_values': {'ç½‘ç»œå¼€é”€é«˜': 'FF6B6B', 'ä¼ è¾“æ—¶é—´å æ¯”é«˜': 'FFE66D', 'ç½‘ç»œå¼€é”€ä¸ç¨³å®š': 'FFB74D'}
            },
            'å¹¶å‘è¿æ¥ä¼°ç®—': {
                'data': results.get('å¹¶å‘è¿æ¥ä¼°ç®—')
            },
            'è¿æ¥æ€§èƒ½æŒ‡æ ‡': {
                'data': results.get('è¿æ¥æ€§èƒ½æŒ‡æ ‡')
            },
            'è¶‹åŠ¿åˆ†æ': {
                'data': results.get('è¶‹åŠ¿åˆ†æ')
            }
        }
        
        # åˆ›å»ºå„ä¸ªå·¥ä½œè¡¨
        for sheet_name, config in sheet_configs.items():
            data = config['data']
            if data is not None and hasattr(data, 'empty') and not data.empty:
                ws = add_dataframe_to_excel_with_grouped_headers(wb, data, sheet_name)
                
                # åº”ç”¨æ¡ä»¶æ ¼å¼é«˜äº®
                if 'highlight_column' in config and 'highlight_values' in config:
                    self._apply_highlighting(ws, data, config['highlight_column'], config['highlight_values'])
                
                # æ ¼å¼åŒ–å·¥ä½œè¡¨
                format_excel_sheet(ws)
                gc.collect()
        
        # æ·»åŠ è¿æ¥æ€§èƒ½æ‘˜è¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if results.get('è¿æ¥æ€§èƒ½æ‘˜è¦'):
            self._add_summary_sheet(wb, results['è¿æ¥æ€§èƒ½æ‘˜è¦'], 'è¿æ¥æ€§èƒ½æ‘˜è¦')
        
        # æ·»åŠ æ•´ä½“æ€§èƒ½æ‘˜è¦
        self._add_overall_performance_summary(wb, results)
        
        wb.save(output_path)
        log_info(f"é«˜çº§æ€§èƒ½ç¨³å®šæ€§åˆ†æå·²ä¿å­˜åˆ°: {output_path}", show_memory=True)

    def _apply_highlighting(self, ws, df: pd.DataFrame, highlight_column: str, highlight_values: Dict) -> None:
        """åº”ç”¨æ¡ä»¶æ ¼å¼é«˜äº®æ˜¾ç¤º"""
        if highlight_column not in df.columns:
            return
        
        col_idx = list(df.columns).index(highlight_column) + 1
        
        for r, value in enumerate(df[highlight_column], start=2):
            if value in highlight_values:
                cell = ws.cell(row=r, column=col_idx)
                cell.fill = PatternFill(
                    start_color=highlight_values[value],
                    end_color=highlight_values[value],
                    fill_type='solid'
                )
                cell.font = Font(bold=True)

    def _add_summary_sheet(self, wb: Workbook, summary_data: Dict, sheet_name: str) -> None:
        """æ·»åŠ æ‘˜è¦å·¥ä½œè¡¨"""
        ws = wb.create_sheet(title=sheet_name)
        
        # è®¾ç½®æ ‡é¢˜è¡Œ
        ws.cell(row=1, column=1, value='æ€§èƒ½æŒ‡æ ‡').font = Font(bold=True, size=12)
        ws.cell(row=1, column=2, value='æ•°å€¼').font = Font(bold=True, size=12)
        
        # å¡«å……æ•°æ®
        for r, (metric, value) in enumerate(summary_data.items(), start=2):
            ws.cell(row=r, column=1, value=metric)
            ws.cell(row=r, column=2, value=value)
            
            # è®¾ç½®å¯¹é½æ–¹å¼
            ws.cell(row=r, column=1).alignment = Alignment(horizontal='left')
            ws.cell(row=r, column=2).alignment = Alignment(horizontal='right')
        
        # è°ƒæ•´åˆ—å®½
        ws.column_dimensions['A'].width = 25
        ws.column_dimensions['B'].width = 15
        
        format_excel_sheet(ws)

    def _add_overall_performance_summary(self, wb: Workbook, results: Dict) -> None:
        """æ·»åŠ æ•´ä½“æ€§èƒ½æ‘˜è¦å·¥ä½œè¡¨"""
        ws = wb.create_sheet(title='æ•´ä½“æ€§èƒ½æ‘˜è¦')
        
        # æ ‡é¢˜
        ws.cell(row=1, column=1, value='é«˜çº§NginxæœåŠ¡æ€§èƒ½åˆ†ææ‘˜è¦').font = Font(bold=True, size=14)
        ws.merge_cells('A1:D1')
        
        current_row = 3
        
        # æ·»åŠ åˆ†ææ¦‚è§ˆ
        ws.cell(row=current_row, column=1, value='ğŸ“Š åˆ†ææ¦‚è§ˆ').font = Font(bold=True, size=12)
        current_row += 1
        
        analysis_overview = [
            ('åˆ†æç®—æ³•', 'T-Digeståˆ†ä½æ•° + HyperLogLog + è“„æ°´æ± é‡‡æ ·'),
            ('å†…å­˜ä¼˜åŒ–', '90%+ å†…å­˜èŠ‚çœï¼Œæ”¯æŒ40G+æ•°æ®'),
            ('å¼‚å¸¸æ£€æµ‹', 'å¤šç»´åº¦æ™ºèƒ½å¼‚å¸¸æ£€æµ‹è¯„åˆ†'),
            ('è¶‹åŠ¿åˆ†æ', 'åŸºäºæ—¶é—´åºåˆ—çš„æ€§èƒ½è¶‹åŠ¿è¯†åˆ«')
        ]
        
        for metric, value in analysis_overview:
            ws.cell(row=current_row, column=2, value=metric)
            ws.cell(row=current_row, column=3, value=value)
            current_row += 1
        current_row += 1
        
        # æ·»åŠ å…³é”®æŒ‡æ ‡æ±‡æ€»
        for analysis_name, df in results.items():
            if df is None or isinstance(df, dict) or not hasattr(df, 'empty') or df.empty:
                continue
            
            if 'æ‘˜è¦' in analysis_name or 'è¶‹åŠ¿' in analysis_name:
                continue
            
            ws.cell(row=current_row, column=1, value=f'ğŸ“ˆ {analysis_name}').font = Font(bold=True, size=12)
            current_row += 1
            
            # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
            summary_stats = self._extract_key_stats(df, analysis_name)
            
            for stat_name, stat_value in summary_stats.items():
                ws.cell(row=current_row, column=2, value=stat_name)
                ws.cell(row=current_row, column=3, value=stat_value)
                current_row += 1
            current_row += 1
        
        # è®¾ç½®åˆ—å®½å’Œæ ·å¼
        ws.column_dimensions['A'].width = 25
        ws.column_dimensions['B'].width = 25
        ws.column_dimensions['C'].width = 30
        ws.column_dimensions['D'].width = 15
        
        # è®¾ç½®å¯¹é½æ–¹å¼
        for row in ws.iter_rows():
            for cell in row:
                if cell.value:
                    cell.alignment = Alignment(horizontal='left', vertical='center')
        
        format_excel_sheet(ws)

    def _extract_key_stats(self, df: pd.DataFrame, analysis_name: str) -> Dict:
        """æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        if 'æˆåŠŸç‡' in analysis_name:
            if 'å¹³å‡æˆåŠŸç‡(%)' in df.columns:
                stats['æœåŠ¡æ•°é‡'] = len(df)
                stats['å¹³å‡æˆåŠŸç‡'] = f"{df['å¹³å‡æˆåŠŸç‡(%)'].mean():.2f}%"
                stats['æœ€ä½æˆåŠŸç‡'] = f"{df['å¹³å‡æˆåŠŸç‡(%)'].min():.2f}%"
                if 'å¼‚å¸¸çŠ¶æ€' in df.columns:
                    abnormal_count = len(df[df['å¼‚å¸¸çŠ¶æ€'] != 'æ­£å¸¸'])
                    stats['å¼‚å¸¸æœåŠ¡æ•°'] = f"{abnormal_count}/{len(df)}"
        
        elif 'å“åº”æ—¶é—´' in analysis_name:
            if 'å¹³å‡å“åº”æ—¶é—´(ç§’)' in df.columns:
                stats['æœåŠ¡æ•°é‡'] = len(df)
                stats['å¹³å‡å“åº”æ—¶é—´'] = f"{df['å¹³å‡å“åº”æ—¶é—´(ç§’)'].mean():.3f}ç§’"
                if 'P99å“åº”æ—¶é—´(ç§’)' in df.columns:
                    stats['å¹³å‡P99å“åº”æ—¶é—´'] = f"{df['P99å“åº”æ—¶é—´(ç§’)'].mean():.3f}ç§’"
                if 'å¼‚å¸¸çŠ¶æ€' in df.columns:
                    abnormal_count = len(df[df['å¼‚å¸¸çŠ¶æ€'] != 'æ­£å¸¸'])
                    stats['å¼‚å¸¸æœåŠ¡æ•°'] = f"{abnormal_count}/{len(df)}"
        
        elif 'åç«¯å¤„ç†' in analysis_name:
            if 'åç«¯å¤„ç†æ•ˆç‡(%)' in df.columns:
                stats['æ—¶æ®µæ•°é‡'] = len(df)
                stats['å¹³å‡åç«¯æ•ˆç‡'] = f"{df['åç«¯å¤„ç†æ•ˆç‡(%)'].mean():.2f}%"
                if 'å¹³å‡è¿æ¥æ—¶é—´(ç§’)' in df.columns:
                    stats['å¹³å‡è¿æ¥æ—¶é—´'] = f"{df['å¹³å‡è¿æ¥æ—¶é—´(ç§’)'].mean():.3f}ç§’"
                if 'æ€§èƒ½çŠ¶æ€' in df.columns:
                    abnormal_count = len(df[df['æ€§èƒ½çŠ¶æ€'] != 'æ­£å¸¸'])
                    stats['å¼‚å¸¸æ—¶æ®µæ•°'] = f"{abnormal_count}/{len(df)}"
        
        elif 'ä¼ è¾“æ€§èƒ½' in analysis_name:
            if 'æ€»ä¼ è¾“é€Ÿåº¦(KB/s)' in df.columns:
                stats['æ—¶æ®µæ•°é‡'] = len(df)
                stats['å¹³å‡ä¼ è¾“é€Ÿåº¦'] = f"{df['æ€»ä¼ è¾“é€Ÿåº¦(KB/s)'].mean():.2f} KB/s"
                if 'ä¼ è¾“çŠ¶æ€' in df.columns:
                    abnormal_count = len(df[df['ä¼ è¾“çŠ¶æ€'] != 'æ­£å¸¸'])
                    stats['å¼‚å¸¸æ—¶æ®µæ•°'] = f"{abnormal_count}/{len(df)}"
        
        elif 'èµ„æºä½¿ç”¨' in analysis_name:
            if 'æ€»ä¼ è¾“æµé‡(MB)' in df.columns:
                stats['æœåŠ¡æ–¹æ³•æ•°'] = len(df)
                stats['æ€»ä¼ è¾“æµé‡'] = f"{df['æ€»ä¼ è¾“æµé‡(MB)'].sum():.2f} MB"
                if 'ä¼ è¾“æ•ˆç‡(%)' in df.columns:
                    stats['å¹³å‡ä¼ è¾“æ•ˆç‡'] = f"{df['ä¼ è¾“æ•ˆç‡(%)'].mean():.2f}%"
        
        elif 'å¹¶å‘è¿æ¥' in analysis_name:
            if 'å¹³å‡å¹¶å‘æ•°' in df.columns:
                stats['æ—¶æ®µæ•°é‡'] = len(df)
                stats['å¹³å‡å¹¶å‘æ•°'] = f"{df['å¹³å‡å¹¶å‘æ•°'].mean():.2f}"
                stats['æœ€é«˜å¹¶å‘æ•°'] = f"{df['æœ€å¤§å¹¶å‘æ•°'].max()}"
        
        return stats


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
def analyze_service_stability(csv_path: str, output_path: str, threshold: Optional[Dict] = None) -> Dict:
    """åˆ†ææœåŠ¡ç¨³å®šæ€§æŒ‡æ ‡ - é«˜çº§ç‰ˆæœ¬å…¥å£å‡½æ•°"""
    analyzer = AdvancedPerformanceAnalyzer()
    return analyzer.analyze_performance_stability(csv_path, output_path, threshold)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    import sys
    
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python self_06_performance_stability_analyzer_advanced.py <csv_path> <output_path>")
        sys.exit(1)
    
    csv_path = sys.argv[1]
    output_path = sys.argv[2]
    
    results = analyze_service_stability(csv_path, output_path)
    print(f"åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_path}")