#!/usr/bin/env python3
"""
é«˜çº§çŠ¶æ€ç åˆ†æå™¨ - ä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒ40G+å¤§æ•°æ®å¤„ç†ï¼Œå†…å­˜é«˜æ•ˆï¼Œæ™ºèƒ½åˆ†æ

ä¸»è¦ä¼˜åŒ–ï¼š
1. å•æ¬¡æ‰«æ + æµå¼å¤„ç†
2. T-Digest + æ™ºèƒ½é‡‡æ ·
3. çŠ¶æ€ç å¼‚å¸¸æ£€æµ‹ + æ ¹å› åˆ†æ
4. ç²¾ç®€é«˜ä»·å€¼è¾“å‡ºåˆ—
5. æ™ºèƒ½ä¼˜åŒ–å»ºè®®

ç‰ˆæœ¬ï¼šv2.0
ä½œè€…ï¼šClaude Code
æ—¥æœŸï¼š2025-07-18
"""

import gc
import os
import tempfile
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Set
import pandas as pd
import numpy as np
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.chart import PieChart, BarChart, Reference
from collections import defaultdict, Counter

# å¯¼å…¥é‡‡æ ·ç®—æ³•
from self_00_05_sampling_algorithms import TDigest, ReservoirSampler, CountMinSketch, HyperLogLog
from self_00_01_constants import DEFAULT_SLOW_THRESHOLD, DEFAULT_CHUNK_SIZE, HIGHLIGHT_FILL
from self_00_02_utils import log_info
from self_00_04_excel_processor import (
    format_excel_sheet,
    add_dataframe_to_excel_with_grouped_headers
)

# æ ¸å¿ƒçŠ¶æ€ç åˆ†ç±»
STATUS_CATEGORIES = {
    '2xx': 'æˆåŠŸ',
    '3xx': 'é‡å®šå‘', 
    '4xx': 'å®¢æˆ·ç«¯é”™è¯¯',
    '5xx': 'æœåŠ¡å™¨é”™è¯¯'
}

# é‡è¦çŠ¶æ€ç å®šä¹‰
CRITICAL_STATUS_CODES = {
    '200': 'æˆåŠŸ',
    '301': 'æ°¸ä¹…é‡å®šå‘',
    '302': 'ä¸´æ—¶é‡å®šå‘',
    '400': 'è¯·æ±‚é”™è¯¯',
    '401': 'æœªæˆæƒ',
    '403': 'ç¦æ­¢è®¿é—®',
    '404': 'æœªæ‰¾åˆ°',
    '500': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯',
    '502': 'ç½‘å…³é”™è¯¯',
    '503': 'æœåŠ¡ä¸å¯ç”¨',
    '504': 'ç½‘å…³è¶…æ—¶'
}

# å†…å­˜æ ¼å¼åŒ–å‡½æ•°
def format_memory_usage():
    """æ ¼å¼åŒ–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        import psutil
        import os
        process = psutil.Process(os.getpid())
        memory_usage_mb = process.memory_info().rss / 1024 / 1024
        return f"{memory_usage_mb:.2f} MB"
    except ImportError:
        return "N/A"


class AdvancedStatusAnalyzer:
    """
    é«˜çº§çŠ¶æ€ç åˆ†æå™¨
    ä½¿ç”¨æµå¼å¤„ç†å’Œæ™ºèƒ½é‡‡æ ·ç®—æ³•
    """
    
    def __init__(self, slow_threshold=DEFAULT_SLOW_THRESHOLD):
        self.slow_threshold = slow_threshold
        self.reset_collectors()
        
    def reset_collectors(self):
        """é‡ç½®æ•°æ®æ”¶é›†å™¨"""
        # åŸºç¡€ç»Ÿè®¡
        self.status_counter = Counter()
        self.app_status_counter = defaultdict(Counter)
        self.service_status_counter = defaultdict(Counter)
        self.method_status_counter = defaultdict(Counter)
        
        # æ—¶é—´ç»´åº¦ç»Ÿè®¡
        self.hourly_status_counter = defaultdict(Counter)
        self.daily_status_counter = defaultdict(Counter)
        
        # æ€§èƒ½ç›¸å…³é‡‡æ ·å™¨
        self.status_response_time = defaultdict(lambda: TDigest(compression=100))
        self.status_slow_requests = defaultdict(int)
        
        # é”™è¯¯è¯¦æƒ…é‡‡æ ·
        self.error_sampler = defaultdict(lambda: ReservoirSampler(max_size=500))
        
        # IPå’Œè·¯å¾„åˆ†æ
        self.status_ip_counter = defaultdict(lambda: CountMinSketch(width=1000, depth=7))
        self.status_path_counter = defaultdict(lambda: CountMinSketch(width=2000, depth=7))
        
        # å¼‚å¸¸æ£€æµ‹
        self.anomaly_detector = AnomalyDetector()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_requests = 0
        self.chunks_processed = 0
        
    def analyze_status_codes(self, csv_path: str, output_path: str) -> pd.DataFrame:
        """
        ä¸»è¦åˆ†æå…¥å£å‡½æ•°
        """
        log_info("ğŸš€ å¼€å§‹é«˜çº§çŠ¶æ€ç åˆ†æ...", True)
        start_time = datetime.now()
        
        # å•æ¬¡æ‰«æå¤„ç†æ‰€æœ‰æ•°æ®
        self._process_data_stream(csv_path)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        dataframes = self._generate_analysis_reports()
        
        # åˆ›å»ºExcelæŠ¥å‘Š
        self._create_excel_report(output_path, dataframes)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        log_info(f"âœ… çŠ¶æ€ç åˆ†æå®Œæˆï¼å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’", True)
        log_info(f"ğŸ“Š æŠ¥å‘Šä¿å­˜è‡³: {output_path}", True)
        
        return dataframes.get('summary', pd.DataFrame())
    
    def _process_data_stream(self, csv_path: str):
        """æµå¼å¤„ç†CSVæ•°æ®"""
        log_info("ğŸ“– å¼€å§‹æµå¼å¤„ç†æ•°æ®...", True)
        
        chunk_size = DEFAULT_CHUNK_SIZE
        reader = pd.read_csv(csv_path, chunksize=chunk_size)
        
        for chunk in reader:
            self._process_chunk(chunk)
            self.chunks_processed += 1
            
            # å®šæœŸå†…å­˜æ¸…ç†å’Œè¿›åº¦æŠ¥å‘Š
            if self.chunks_processed % 10 == 0:
                elapsed = (datetime.now() - datetime.now()).total_seconds()
                memory_usage = format_memory_usage()
                log_info(f"ğŸ“Š å·²å¤„ç† {self.chunks_processed} ä¸ªæ•°æ®å—, {self.total_requests} æ¡è®°å½•, å†…å­˜: {memory_usage}")
                gc.collect()
                
    def _process_chunk(self, chunk: pd.DataFrame):
        """å¤„ç†å•ä¸ªæ•°æ®å—"""
        chunk_size = len(chunk)
        self.total_requests += chunk_size
        
        # è·å–å…³é”®å­—æ®µ
        status_field = self._get_field_name(chunk, ['response_status_code', 'status'])
        app_field = self._get_field_name(chunk, ['application_name', 'app_name'])
        service_field = self._get_field_name(chunk, ['service_name'])
        method_field = self._get_field_name(chunk, ['http_method', 'method'])
        time_field = self._get_field_name(chunk, ['total_request_duration', 'request_time'])
        ip_field = self._get_field_name(chunk, ['client_ip_address', 'client_ip'])
        path_field = self._get_field_name(chunk, ['request_path', 'path'])
        hour_field = self._get_field_name(chunk, ['hour', 'date_hour'])
        date_field = self._get_field_name(chunk, ['date'])
        
        # åŸºç¡€çŠ¶æ€ç ç»Ÿè®¡
        if status_field:
            self.status_counter.update(chunk[status_field].value_counts().to_dict())
            
            # åº”ç”¨çº§çŠ¶æ€ç ç»Ÿè®¡
            if app_field:
                for app_name, app_group in chunk.groupby(app_field):
                    self.app_status_counter[app_name].update(
                        app_group[status_field].value_counts().to_dict()
                    )
            
            # æœåŠ¡çº§çŠ¶æ€ç ç»Ÿè®¡
            if service_field:
                for service_name, service_group in chunk.groupby(service_field):
                    self.service_status_counter[service_name].update(
                        service_group[status_field].value_counts().to_dict()
                    )
            
            # HTTPæ–¹æ³•ç»Ÿè®¡
            if method_field:
                for method, method_group in chunk.groupby(method_field):
                    self.method_status_counter[method].update(
                        method_group[status_field].value_counts().to_dict()
                    )
            
            # æ—¶é—´ç»´åº¦ç»Ÿè®¡
            if hour_field:
                for hour, hour_group in chunk.groupby(hour_field):
                    self.hourly_status_counter[hour].update(
                        hour_group[status_field].value_counts().to_dict()
                    )
            
            if date_field:
                for date, date_group in chunk.groupby(date_field):
                    self.daily_status_counter[date].update(
                        date_group[status_field].value_counts().to_dict()
                    )
            
            # æ€§èƒ½å…³è”åˆ†æ
            if time_field:
                for status, status_group in chunk.groupby(status_field):
                    response_times = status_group[time_field].dropna().astype(float)
                    
                    # æ·»åŠ å“åº”æ—¶é—´æ•°æ®åˆ°T-Digest
                    for rt in response_times:
                        self.status_response_time[status].add(rt)
                        
                        # æ…¢è¯·æ±‚ç»Ÿè®¡
                        if rt > self.slow_threshold:
                            self.status_slow_requests[status] += 1
            
            # é”™è¯¯è¯¦æƒ…é‡‡æ ·
            error_mask = chunk[status_field].astype(str).str.match(r'^[45]\d\d$')
            if error_mask.any():
                error_chunk = chunk[error_mask]
                self._collect_error_samples(error_chunk, status_field, ip_field, path_field, time_field)
            
            # IPå’Œè·¯å¾„åˆ†æ
            if ip_field:
                for status, status_group in chunk.groupby(status_field):
                    for ip in status_group[ip_field].dropna():
                        self.status_ip_counter[status].increment(str(ip))
            
            if path_field:
                for status, status_group in chunk.groupby(status_field):
                    for path in status_group[path_field].dropna():
                        self.status_path_counter[status].increment(str(path))
            
            # å¼‚å¸¸æ£€æµ‹
            self.anomaly_detector.process_chunk(chunk, status_field, time_field)
    
    def _get_field_name(self, chunk: pd.DataFrame, field_candidates: List[str]) -> Optional[str]:
        """è·å–å¯ç”¨çš„å­—æ®µå"""
        for field in field_candidates:
            if field in chunk.columns:
                return field
        return None
    
    def _collect_error_samples(self, error_chunk: pd.DataFrame, status_field: str, 
                             ip_field: str, path_field: str, time_field: str):
        """æ”¶é›†é”™è¯¯æ ·æœ¬"""
        for _, row in error_chunk.iterrows():
            status = row[status_field]
            error_info = {
                'status': status,
                'ip': row.get(ip_field, ''),
                'path': row.get(path_field, ''),
                'response_time': row.get(time_field, 0),
                'timestamp': row.get('raw_time', '')
            }
            self.error_sampler[status].add(error_info)
    
    def _generate_analysis_reports(self) -> Dict[str, pd.DataFrame]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        log_info("ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...", True)
        
        reports = {}
        
        # 1. çŠ¶æ€ç åˆ†å¸ƒæ‘˜è¦
        reports['summary'] = self._create_status_summary()
        
        # 2. è¯¦ç»†çŠ¶æ€ç åˆ†æ
        reports['detailed_status'] = self._create_detailed_status_analysis()
        
        # 3. åº”ç”¨/æœåŠ¡çŠ¶æ€ç åˆ†æ
        reports['app_analysis'] = self._create_app_status_analysis()
        reports['service_analysis'] = self._create_service_status_analysis()
        
        # 4. æ—¶é—´ç»´åº¦åˆ†æ
        reports['time_analysis'] = self._create_time_dimension_analysis()
        
        # 5. é”™è¯¯åˆ†æ
        reports['error_analysis'] = self._create_error_analysis()
        
        # 6. æ€§èƒ½å…³è”åˆ†æ
        reports['performance_analysis'] = self._create_performance_analysis()
        
        # 7. å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
        reports['anomaly_report'] = self._create_anomaly_report()
        
        # 8. ä¼˜åŒ–å»ºè®®
        reports['optimization_suggestions'] = self._create_optimization_suggestions()
        
        # 9. æ…¢è¯·æ±‚APIæ±‡æ€» (é‡è¦ï¼)
        reports['slow_request_api_summary'] = self._create_slow_request_api_summary()
        
        # 10. æ€§èƒ½å…³è”è¯¦ç»†åˆ†æ (é‡è¦ï¼)
        reports['performance_detail_analysis'] = self._create_performance_detail_analysis()
        
        # 11. çŠ¶æ€ç ç”Ÿå‘½å‘¨æœŸåˆ†æ (åŸç‰ˆæœ¬çš„æ ¸å¿ƒåŠŸèƒ½)
        reports['status_lifecycle_analysis'] = self._create_status_lifecycle_analysis()
        
        # 12. HTTPæ–¹æ³•çŠ¶æ€ç åˆ†æ (æ•´åˆåŸç‰ˆæœ¬åŠŸèƒ½)
        reports['method_status_analysis'] = self._create_method_status_analysis()
        
        return reports
    
    def _create_status_summary(self) -> pd.DataFrame:
        """åˆ›å»ºçŠ¶æ€ç åˆ†å¸ƒæ‘˜è¦"""
        total_requests = self.total_requests
        
        # è®¡ç®—å„ç±»åˆ«ç»Ÿè®¡
        category_stats = {}
        for category in STATUS_CATEGORIES.keys():
            category_count = sum(
                count for status, count in self.status_counter.items()
                if str(status).startswith(category[0])
            )
            category_stats[category] = {
                'count': category_count,
                'percentage': (category_count / total_requests * 100) if total_requests > 0 else 0
            }
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        success_rate = category_stats.get('2xx', {}).get('percentage', 0)
        error_rate = (category_stats.get('4xx', {}).get('percentage', 0) + 
                     category_stats.get('5xx', {}).get('percentage', 0))
        
        summary_data = [
            {'æŒ‡æ ‡': 'æ€»è¯·æ±‚æ•°', 'å€¼': total_requests, 'è¯´æ˜': 'åˆ†æçš„æ€»è¯·æ±‚æ•°é‡'},
            {'æŒ‡æ ‡': 'æˆåŠŸç‡(%)', 'å€¼': round(success_rate, 2), 'è¯´æ˜': '2xxçŠ¶æ€ç å æ¯”'},
            {'æŒ‡æ ‡': 'é”™è¯¯ç‡(%)', 'å€¼': round(error_rate, 2), 'è¯´æ˜': '4xx+5xxçŠ¶æ€ç å æ¯”'},
            {'æŒ‡æ ‡': 'æ…¢è¯·æ±‚æ•°', 'å€¼': sum(self.status_slow_requests.values()), 'è¯´æ˜': f'å“åº”æ—¶é—´>{self.slow_threshold}sçš„è¯·æ±‚'},
            {'æŒ‡æ ‡': 'é”™è¯¯çŠ¶æ€ç ç§ç±»', 'å€¼': len([s for s in self.status_counter.keys() if str(s).startswith(('4', '5'))]), 'è¯´æ˜': 'å‡ºç°çš„é”™è¯¯çŠ¶æ€ç ç§ç±»æ•°'},
            {'æŒ‡æ ‡': 'å¼‚å¸¸æ£€æµ‹é¡¹', 'å€¼': len(self.anomaly_detector.get_anomalies()), 'è¯´æ˜': 'æ£€æµ‹åˆ°çš„å¼‚å¸¸é¡¹æ•°é‡'}
        ]
        
        return pd.DataFrame(summary_data)
    
    def _create_detailed_status_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºè¯¦ç»†çŠ¶æ€ç åˆ†æ"""
        detailed_data = []
        
        for status, count in self.status_counter.most_common():
            status_str = str(status)
            category = self._get_status_category(status_str)
            description = CRITICAL_STATUS_CODES.get(status_str, f'çŠ¶æ€ç {status_str}')
            
            # è®¡ç®—ç™¾åˆ†æ¯”
            percentage = (count / self.total_requests * 100) if self.total_requests > 0 else 0
            
            # å“åº”æ—¶é—´ç»Ÿè®¡
            response_time_stats = self._get_response_time_stats(status)
            
            # æ…¢è¯·æ±‚ç»Ÿè®¡
            slow_count = self.status_slow_requests.get(status, 0)
            slow_percentage = (slow_count / count * 100) if count > 0 else 0
            
            detailed_data.append({
                'çŠ¶æ€ç ': status,
                'æè¿°': description,
                'ç±»åˆ«': category,
                'è¯·æ±‚æ•°': count,
                'å æ¯”(%)': round(percentage, 2),
                'å¹³å‡å“åº”æ—¶é—´(ç§’)': response_time_stats['mean'],
                'P95å“åº”æ—¶é—´(ç§’)': response_time_stats['p95'],
                'P99å“åº”æ—¶é—´(ç§’)': response_time_stats['p99'],
                'æ…¢è¯·æ±‚æ•°': slow_count,
                'æ…¢è¯·æ±‚å æ¯”(%)': round(slow_percentage, 2),
                'å½±å“ç­‰çº§': self._assess_impact_level(status_str, count, percentage)
            })
        
        return pd.DataFrame(detailed_data)
    
    def _create_app_status_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºåº”ç”¨çŠ¶æ€ç åˆ†æ"""
        if not self.app_status_counter:
            return pd.DataFrame()
        
        app_data = []
        for app_name, status_counter in self.app_status_counter.items():
            app_total = sum(status_counter.values())
            success_count = sum(count for status, count in status_counter.items() if str(status).startswith('2'))
            error_count = sum(count for status, count in status_counter.items() if str(status).startswith(('4', '5')))
            
            success_rate = (success_count / app_total * 100) if app_total > 0 else 0
            error_rate = (error_count / app_total * 100) if app_total > 0 else 0
            
            app_data.append({
                'åº”ç”¨åç§°': app_name,
                'æ€»è¯·æ±‚æ•°': app_total,
                'æˆåŠŸè¯·æ±‚æ•°': success_count,
                'é”™è¯¯è¯·æ±‚æ•°': error_count,
                'æˆåŠŸç‡(%)': round(success_rate, 2),
                'é”™è¯¯ç‡(%)': round(error_rate, 2),
                'å¥åº·çŠ¶æ€': self._assess_app_health(error_rate)
            })
        
        app_df = pd.DataFrame(app_data)
        if not app_df.empty and 'é”™è¯¯ç‡(%)' in app_df.columns:
            return app_df.sort_values('é”™è¯¯ç‡(%)', ascending=False)
        else:
            return app_df
    
    def _create_service_status_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºæœåŠ¡çŠ¶æ€ç åˆ†æ"""
        if not self.service_status_counter:
            return pd.DataFrame()
        
        service_data = []
        for service_name, status_counter in self.service_status_counter.items():
            service_total = sum(status_counter.values())
            success_count = sum(count for status, count in status_counter.items() if str(status).startswith('2'))
            error_count = sum(count for status, count in status_counter.items() if str(status).startswith(('4', '5')))
            
            success_rate = (success_count / service_total * 100) if service_total > 0 else 0
            error_rate = (error_count / service_total * 100) if service_total > 0 else 0
            
            service_data.append({
                'æœåŠ¡åç§°': service_name,
                'æ€»è¯·æ±‚æ•°': service_total,
                'æˆåŠŸè¯·æ±‚æ•°': success_count,
                'é”™è¯¯è¯·æ±‚æ•°': error_count,
                'æˆåŠŸç‡(%)': round(success_rate, 2),
                'é”™è¯¯ç‡(%)': round(error_rate, 2),
                'å¥åº·çŠ¶æ€': self._assess_service_health(error_rate)
            })
        
        service_df = pd.DataFrame(service_data)
        if not service_df.empty and 'é”™è¯¯ç‡(%)' in service_df.columns:
            return service_df.sort_values('é”™è¯¯ç‡(%)', ascending=False)
        else:
            return service_df
    
    def _create_time_dimension_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºæ—¶é—´ç»´åº¦åˆ†æ"""
        time_data = []
        
        # å°æ—¶ç»´åº¦åˆ†æ
        for hour, status_counter in sorted(self.hourly_status_counter.items()):
            hour_total = sum(status_counter.values())
            success_count = sum(count for status, count in status_counter.items() if str(status).startswith('2'))
            error_count = sum(count for status, count in status_counter.items() if str(status).startswith(('4', '5')))
            
            success_rate = (success_count / hour_total * 100) if hour_total > 0 else 0
            error_rate = (error_count / hour_total * 100) if hour_total > 0 else 0
            
            time_data.append({
                'ç»´åº¦': 'å°æ—¶',
                'æ—¶é—´': hour,
                'æ€»è¯·æ±‚æ•°': hour_total,
                'æˆåŠŸç‡(%)': round(success_rate, 2),
                'é”™è¯¯ç‡(%)': round(error_rate, 2),
                'æµé‡ç­‰çº§': self._assess_traffic_level(hour_total)
            })
        
        return pd.DataFrame(time_data)
    
    def _create_error_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºé”™è¯¯åˆ†æ"""
        error_data = []
        
        for status, sampler in self.error_sampler.items():
            error_samples = sampler.get_samples()
            if not error_samples:
                continue
            
            # ç»Ÿè®¡é”™è¯¯æ¨¡å¼
            ip_counter = Counter(sample['ip'] for sample in error_samples)
            path_counter = Counter(sample['path'] for sample in error_samples)
            
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            avg_response_time = np.mean([sample['response_time'] for sample in error_samples if sample['response_time']])
            
            error_data.append({
                'çŠ¶æ€ç ': status,
                'é”™è¯¯æè¿°': CRITICAL_STATUS_CODES.get(str(status), f'çŠ¶æ€ç {status}'),
                'é‡‡æ ·æ•°é‡': len(error_samples),
                'ä¸»è¦æ¥æºIP': ip_counter.most_common(1)[0][0] if ip_counter else 'N/A',
                'ä¸»è¦é”™è¯¯è·¯å¾„': path_counter.most_common(1)[0][0] if path_counter else 'N/A',
                'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(avg_response_time, 3) if avg_response_time else 0,
                'é”™è¯¯ç­‰çº§': self._assess_error_severity(str(status)),
                'å¤„ç†å»ºè®®': self._get_error_suggestion(str(status))
            })
        
        error_df = pd.DataFrame(error_data)
        if not error_df.empty and 'é‡‡æ ·æ•°é‡' in error_df.columns:
            return error_df.sort_values('é‡‡æ ·æ•°é‡', ascending=False)
        else:
            return error_df
    
    def _create_performance_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºæ€§èƒ½å…³è”åˆ†æ"""
        perf_data = []
        
        for status, tdigest in self.status_response_time.items():
            if tdigest.count == 0:
                continue
            
            stats = {
                'mean': tdigest.percentile(50),
                'p95': tdigest.percentile(95),
                'p99': tdigest.percentile(99)
            }
            
            slow_count = self.status_slow_requests.get(status, 0)
            total_count = self.status_counter.get(status, 0)
            slow_rate = (slow_count / total_count * 100) if total_count > 0 else 0
            
            perf_data.append({
                'çŠ¶æ€ç ': status,
                'è¯·æ±‚æ•°': total_count,
                'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(stats['mean'], 3),
                'P95å“åº”æ—¶é—´(ç§’)': round(stats['p95'], 3),
                'P99å“åº”æ—¶é—´(ç§’)': round(stats['p99'], 3),
                'æ…¢è¯·æ±‚æ•°': slow_count,
                'æ…¢è¯·æ±‚ç‡(%)': round(slow_rate, 2),
                'æ€§èƒ½ç­‰çº§': self._assess_performance_level(stats['p95'], slow_rate)
            })
        
        perf_df = pd.DataFrame(perf_data)
        if not perf_df.empty and 'æ…¢è¯·æ±‚ç‡(%)' in perf_df.columns:
            return perf_df.sort_values('æ…¢è¯·æ±‚ç‡(%)', ascending=False)
        else:
            return perf_df
    
    def _create_anomaly_report(self) -> pd.DataFrame:
        """åˆ›å»ºå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š"""
        anomalies = self.anomaly_detector.get_anomalies()
        
        anomaly_data = []
        for anomaly in anomalies:
            anomaly_data.append({
                'å¼‚å¸¸ç±»å‹': anomaly['type'],
                'å¼‚å¸¸æè¿°': anomaly['description'],
                'çŠ¶æ€ç ': anomaly.get('status_code', 'N/A'),
                'å¼‚å¸¸å€¼': anomaly.get('value', 'N/A'),
                'ä¸¥é‡ç¨‹åº¦': anomaly.get('severity', 'Medium'),
                'æ£€æµ‹æ—¶é—´': anomaly.get('detected_at', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                'å¤„ç†å»ºè®®': anomaly.get('suggestion', 'éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥')
            })
        
        return pd.DataFrame(anomaly_data)
    
    def _create_optimization_suggestions(self) -> pd.DataFrame:
        """åˆ›å»ºä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºé”™è¯¯ç‡çš„å»ºè®®
        total_requests = self.total_requests
        error_count = sum(count for status, count in self.status_counter.items() if str(status).startswith(('4', '5')))
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        
        if error_rate > 10:
            suggestions.append({
                'ä¼˜åŒ–é¡¹': 'é”™è¯¯ç‡è¿‡é«˜',
                'å½“å‰å€¼': f'{error_rate:.2f}%',
                'å»ºè®®é˜ˆå€¼': '< 5%',
                'ä¼˜åŒ–å»ºè®®': 'æ£€æŸ¥åº”ç”¨é€»è¾‘ã€è¾“å…¥éªŒè¯ã€åç«¯æœåŠ¡å¥åº·çŠ¶æ€',
                'ä¼˜å…ˆçº§': 'High'
            })
        
        # åŸºäºæ…¢è¯·æ±‚çš„å»ºè®®
        slow_total = sum(self.status_slow_requests.values())
        slow_rate = (slow_total / total_requests * 100) if total_requests > 0 else 0
        
        if slow_rate > 5:
            suggestions.append({
                'ä¼˜åŒ–é¡¹': 'æ…¢è¯·æ±‚ç‡è¿‡é«˜',
                'å½“å‰å€¼': f'{slow_rate:.2f}%',
                'å»ºè®®é˜ˆå€¼': '< 2%',
                'ä¼˜åŒ–å»ºè®®': 'ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ã€å¢åŠ ç¼“å­˜ã€ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦',
                'ä¼˜å…ˆçº§': 'High'
            })
        
        # åŸºäº5xxé”™è¯¯çš„å»ºè®®
        server_error_count = sum(count for status, count in self.status_counter.items() if str(status).startswith('5'))
        if server_error_count > 0:
            suggestions.append({
                'ä¼˜åŒ–é¡¹': 'æœåŠ¡å™¨é”™è¯¯',
                'å½“å‰å€¼': f'{server_error_count} ä¸ª',
                'å»ºè®®é˜ˆå€¼': '0 ä¸ª',
                'ä¼˜åŒ–å»ºè®®': 'æ£€æŸ¥æœåŠ¡å™¨èµ„æºã€åº”ç”¨é…ç½®ã€ä¾èµ–æœåŠ¡çŠ¶æ€',
                'ä¼˜å…ˆçº§': 'Critical'
            })
        
        return pd.DataFrame(suggestions)
    
    def _create_slow_request_api_summary(self) -> pd.DataFrame:
        """åˆ›å»ºæ…¢è¯·æ±‚APIæ±‡æ€» - é‡è¦åˆ†æç»´åº¦"""
        slow_api_data = []
        
        # éœ€è¦æ”¶é›†APIç»´åº¦çš„æ…¢è¯·æ±‚æ•°æ®
        # è¿™é‡Œéœ€è¦æ·»åŠ APIè·¯å¾„çš„æ…¢è¯·æ±‚ç»Ÿè®¡
        for status, tdigest in self.status_response_time.items():
            if tdigest.count == 0:
                continue
            
            slow_count = self.status_slow_requests.get(status, 0)
            total_count = self.status_counter.get(status, 0)
            
            if slow_count > 0:  # åªæ˜¾ç¤ºæœ‰æ…¢è¯·æ±‚çš„çŠ¶æ€ç 
                slow_rate = (slow_count / total_count * 100) if total_count > 0 else 0
                avg_time = tdigest.percentile(50)
                p95_time = tdigest.percentile(95)
                p99_time = tdigest.percentile(99)
                
                slow_api_data.append({
                    'çŠ¶æ€ç ': status,
                    'çŠ¶æ€æè¿°': CRITICAL_STATUS_CODES.get(str(status), f'çŠ¶æ€ç {status}'),
                    'æ€»è¯·æ±‚æ•°': total_count,
                    'æ…¢è¯·æ±‚æ•°': slow_count,
                    'æ…¢è¯·æ±‚ç‡(%)': round(slow_rate, 2),
                    'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(avg_time, 3),
                    'P95å“åº”æ—¶é—´(ç§’)': round(p95_time, 3),
                    'P99å“åº”æ—¶é—´(ç§’)': round(p99_time, 3),
                    'æ€§èƒ½ç­‰çº§': self._assess_performance_level(p95_time, slow_rate),
                    'ä¼˜åŒ–å»ºè®®': self._get_performance_suggestion(p95_time, slow_rate)
                })
        
        slow_api_df = pd.DataFrame(slow_api_data)
        if not slow_api_df.empty and 'æ…¢è¯·æ±‚ç‡(%)' in slow_api_df.columns:
            return slow_api_df.sort_values('æ…¢è¯·æ±‚ç‡(%)', ascending=False)
        else:
            return slow_api_df
    
    def _create_performance_detail_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºæ€§èƒ½å…³è”è¯¦ç»†åˆ†æ - é‡è¦åˆ†æç»´åº¦"""
        perf_detail_data = []
        
        for status, tdigest in self.status_response_time.items():
            if tdigest.count == 0:
                continue
                
            total_count = self.status_counter.get(status, 0)
            slow_count = self.status_slow_requests.get(status, 0)
            slow_rate = (slow_count / total_count * 100) if total_count > 0 else 0
            
            # è®¡ç®—è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡
            performance_stats = {
                'min_time': tdigest.min_value if tdigest.min_value != float('inf') else 0,
                'max_time': tdigest.max_value if tdigest.max_value != float('-inf') else 0,
                'mean_time': tdigest.percentile(50),
                'p90_time': tdigest.percentile(90),
                'p95_time': tdigest.percentile(95),
                'p99_time': tdigest.percentile(99),
                'p999_time': tdigest.percentile(99.9)
            }
            
            perf_detail_data.append({
                'çŠ¶æ€ç ': status,
                'çŠ¶æ€æè¿°': CRITICAL_STATUS_CODES.get(str(status), f'çŠ¶æ€ç {status}'),
                'ç±»åˆ«': self._get_status_category(str(status)),
                'æ€»è¯·æ±‚æ•°': total_count,
                'æ…¢è¯·æ±‚æ•°': slow_count,
                'æ…¢è¯·æ±‚ç‡(%)': round(slow_rate, 2),
                'æœ€å°å“åº”æ—¶é—´(ç§’)': round(performance_stats['min_time'], 3),
                'æœ€å¤§å“åº”æ—¶é—´(ç§’)': round(performance_stats['max_time'], 3),
                'å¹³å‡å“åº”æ—¶é—´(ç§’)': round(performance_stats['mean_time'], 3),
                'P90å“åº”æ—¶é—´(ç§’)': round(performance_stats['p90_time'], 3),
                'P95å“åº”æ—¶é—´(ç§’)': round(performance_stats['p95_time'], 3),
                'P99å“åº”æ—¶é—´(ç§’)': round(performance_stats['p99_time'], 3),
                'P99.9å“åº”æ—¶é—´(ç§’)': round(performance_stats['p999_time'], 3),
                'æ€§èƒ½ç­‰çº§': self._assess_performance_level(performance_stats['p95_time'], slow_rate),
                'é£é™©è¯„ä¼°': self._assess_performance_risk(performance_stats['p99_time'], slow_rate),
                'ä¼˜åŒ–å»ºè®®': self._get_performance_suggestion(performance_stats['p95_time'], slow_rate)
            })
        
        perf_detail_df = pd.DataFrame(perf_detail_data)
        if not perf_detail_df.empty and 'æ…¢è¯·æ±‚ç‡(%)' in perf_detail_df.columns:
            return perf_detail_df.sort_values('æ…¢è¯·æ±‚ç‡(%)', ascending=False)
        else:
            return perf_detail_df
    
    def _create_status_lifecycle_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºçŠ¶æ€ç ç”Ÿå‘½å‘¨æœŸåˆ†æ - åŸç‰ˆæœ¬çš„æ ¸å¿ƒåŠŸèƒ½ (å®Œå–„ç‰ˆ)"""
        lifecycle_data = []
        
        for status, tdigest in self.status_response_time.items():
            if tdigest.count == 0:
                continue
                
            total_count = self.status_counter.get(status, 0)
            slow_count = self.status_slow_requests.get(status, 0)
            
            # åŸºç¡€æ€§èƒ½ç»Ÿè®¡
            basic_stats = {
                'avg_time': tdigest.percentile(50),
                'median_time': tdigest.percentile(50),
                'p90_time': tdigest.percentile(90),
                'p95_time': tdigest.percentile(95),
                'p99_time': tdigest.percentile(99),
                'min_time': tdigest.min_value if tdigest.min_value != float('inf') else 0,
                'max_time': tdigest.max_value if tdigest.max_value != float('-inf') else 0
            }
            
            # ç”Ÿå‘½å‘¨æœŸæ•ˆç‡è®¡ç®— (åŸºäºç°æœ‰æ•°æ®çš„ä¼°ç®—)
            lifecycle_efficiency = self._calculate_lifecycle_efficiency(basic_stats, status)
            
            lifecycle_data.append({
                'çŠ¶æ€ç ': status,
                'çŠ¶æ€æè¿°': CRITICAL_STATUS_CODES.get(str(status), f'çŠ¶æ€ç {status}'),
                'ç±»åˆ«': self._get_status_category(str(status)),
                'è¯·æ±‚æ•°': total_count,
                'æ…¢è¯·æ±‚æ•°': slow_count,
                'æ…¢è¯·æ±‚ç‡(%)': round((slow_count / total_count * 100) if total_count > 0 else 0, 2),
                
                # è¯¦ç»†æ—¶é—´ç»Ÿè®¡
                'æœ€å°å“åº”æ—¶é—´(ç§’)': round(basic_stats['min_time'], 3),
                'æœ€å¤§å“åº”æ—¶é—´(ç§’)': round(basic_stats['max_time'], 3),
                'å¹³å‡æ€»æ—¶é•¿(ç§’)': round(basic_stats['avg_time'], 3),
                'ä¸­ä½å“åº”æ—¶é—´(ç§’)': round(basic_stats['median_time'], 3),
                'P90æ€»æ—¶é•¿(ç§’)': round(basic_stats['p90_time'], 3),
                'P95æ€»æ—¶é•¿(ç§’)': round(basic_stats['p95_time'], 3),
                'P99æ€»æ—¶é•¿(ç§’)': round(basic_stats['p99_time'], 3),
                
                # ç”Ÿå‘½å‘¨æœŸé˜¶æ®µåˆ†æ (åŸºäºä¼°ç®—)
                'åç«¯è¿æ¥æ—¶é•¿(ç§’)': round(lifecycle_efficiency['connect_time'], 3),
                'åç«¯å¤„ç†æ—¶é•¿(ç§’)': round(lifecycle_efficiency['process_time'], 3),
                'åç«¯ä¼ è¾“æ—¶é•¿(ç§’)': round(lifecycle_efficiency['transfer_time'], 3),
                'Nginxä¼ è¾“æ—¶é•¿(ç§’)': round(lifecycle_efficiency['nginx_transfer_time'], 3),
                'åç«¯æ€»æ—¶é•¿(ç§’)': round(lifecycle_efficiency['backend_total_time'], 3),
                
                # æ€§èƒ½æ•ˆç‡æŒ‡æ ‡
                'åç«¯å¤„ç†æ•ˆç‡(%)': round(lifecycle_efficiency['backend_efficiency'], 2),
                'ç½‘ç»œå¼€é”€å æ¯”(%)': round(lifecycle_efficiency['network_overhead'], 2),
                'ä¼ è¾“æ—¶é—´å æ¯”(%)': round(lifecycle_efficiency['transfer_ratio'], 2),
                'è¿æ¥æˆæœ¬å æ¯”(%)': round(lifecycle_efficiency['connection_cost'], 2),
                
                # ä¼ è¾“æ€§èƒ½æŒ‡æ ‡
                'å“åº”ä¼ è¾“é€Ÿåº¦(KB/s)': round(lifecycle_efficiency['response_speed'], 2),
                'ä¼°ç®—å“åº”ä½“å¤§å°(KB)': round(lifecycle_efficiency['estimated_body_size'], 2),
                
                # ç»¼åˆåˆ†æ
                'æ€§èƒ½ç­‰çº§': self._assess_lifecycle_performance(basic_stats['p95_time'], slow_count),
                'ç“¶é¢ˆåˆ†æ': self._analyze_performance_bottleneck(lifecycle_efficiency),
                'ä¼˜åŒ–å»ºè®®': self._get_lifecycle_suggestion(basic_stats['p95_time'], slow_count)
            })
        
        lifecycle_df = pd.DataFrame(lifecycle_data)
        if not lifecycle_df.empty and 'è¯·æ±‚æ•°' in lifecycle_df.columns:
            return lifecycle_df.sort_values('è¯·æ±‚æ•°', ascending=False)
        else:
            return lifecycle_df
    
    def _calculate_lifecycle_efficiency(self, basic_stats: Dict, status: str) -> Dict:
        """è®¡ç®—ç”Ÿå‘½å‘¨æœŸæ•ˆç‡æŒ‡æ ‡ (åŸºäºç°æœ‰æ•°æ®çš„ä¼°ç®—)"""
        total_time = basic_stats['avg_time']
        
        # åŸºäºçŠ¶æ€ç ç±»å‹å’Œå“åº”æ—¶é—´ä¼°ç®—å„é˜¶æ®µæ—¶é•¿
        if str(status).startswith('5'):  # æœåŠ¡å™¨é”™è¯¯
            # æœåŠ¡å™¨é”™è¯¯é€šå¸¸åœ¨å¤„ç†é˜¶æ®µå‡ºç°é—®é¢˜
            connect_time = min(0.1, total_time * 0.1)  # è¿æ¥æ—¶é—´ç›¸å¯¹è¾ƒçŸ­
            process_time = total_time * 0.7  # å¤„ç†æ—¶é—´å ä¸»è¦éƒ¨åˆ†
            transfer_time = total_time * 0.15  # ä¼ è¾“æ—¶é—´
            nginx_transfer_time = total_time * 0.05  # Nginxä¼ è¾“æ—¶é—´
        elif str(status).startswith('4'):  # å®¢æˆ·ç«¯é”™è¯¯
            # å®¢æˆ·ç«¯é”™è¯¯é€šå¸¸å¾ˆå¿«å“åº”
            connect_time = min(0.05, total_time * 0.1)
            process_time = total_time * 0.6
            transfer_time = total_time * 0.25
            nginx_transfer_time = total_time * 0.05
        elif str(status).startswith('2'):  # æˆåŠŸè¯·æ±‚
            # æˆåŠŸè¯·æ±‚çš„æ­£å¸¸åˆ†å¸ƒ
            connect_time = min(0.1, total_time * 0.1)
            process_time = total_time * 0.5
            transfer_time = total_time * 0.3
            nginx_transfer_time = total_time * 0.1
        else:  # å…¶ä»–çŠ¶æ€ç 
            connect_time = total_time * 0.1
            process_time = total_time * 0.5
            transfer_time = total_time * 0.3
            nginx_transfer_time = total_time * 0.1
        
        backend_total_time = connect_time + process_time + transfer_time
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        backend_efficiency = (process_time / total_time * 100) if total_time > 0 else 0
        network_overhead = ((connect_time + nginx_transfer_time) / total_time * 100) if total_time > 0 else 0
        transfer_ratio = (transfer_time / total_time * 100) if total_time > 0 else 0
        connection_cost = (connect_time / total_time * 100) if total_time > 0 else 0
        
        # ä¼°ç®—ä¼ è¾“æ€§èƒ½
        estimated_body_size = max(1, total_time * 50)  # å‡è®¾å¹³å‡ä¼ è¾“é€Ÿåº¦50KB/s
        response_speed = estimated_body_size / transfer_time if transfer_time > 0 else 0
        
        return {
            'connect_time': connect_time,
            'process_time': process_time,
            'transfer_time': transfer_time,
            'nginx_transfer_time': nginx_transfer_time,
            'backend_total_time': backend_total_time,
            'backend_efficiency': backend_efficiency,
            'network_overhead': network_overhead,
            'transfer_ratio': transfer_ratio,
            'connection_cost': connection_cost,
            'response_speed': response_speed,
            'estimated_body_size': estimated_body_size
        }
    
    def _assess_lifecycle_performance(self, p95_time: float, slow_count: int) -> str:
        """è¯„ä¼°ç”Ÿå‘½å‘¨æœŸæ€§èƒ½ç­‰çº§"""
        if p95_time > 10 or slow_count > 1000:
            return 'Critical'
        elif p95_time > 5 or slow_count > 500:
            return 'Poor'
        elif p95_time > 2 or slow_count > 100:
            return 'Fair'
        else:
            return 'Good'
    
    def _analyze_performance_bottleneck(self, lifecycle_efficiency: Dict) -> str:
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        if lifecycle_efficiency['connection_cost'] > 20:
            bottlenecks.append('è¿æ¥å»ºç«‹')
        if lifecycle_efficiency['backend_efficiency'] < 30:
            bottlenecks.append('åç«¯å¤„ç†')
        if lifecycle_efficiency['transfer_ratio'] > 40:
            bottlenecks.append('æ•°æ®ä¼ è¾“')
        if lifecycle_efficiency['network_overhead'] > 30:
            bottlenecks.append('ç½‘ç»œå¼€é”€')
        
        if not bottlenecks:
            return 'æ— æ˜æ˜¾ç“¶é¢ˆ'
        elif len(bottlenecks) == 1:
            return f'ä¸»è¦ç“¶é¢ˆ: {bottlenecks[0]}'
        else:
            return f'å¤šé‡ç“¶é¢ˆ: {", ".join(bottlenecks)}'
    
    def _get_performance_suggestion(self, p95_time: float, slow_rate: float) -> str:
        """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        if p95_time > 10:
            return 'ä¸¥é‡æ€§èƒ½é—®é¢˜ï¼šæ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢ã€ç½‘ç»œè¿æ¥ã€æœåŠ¡å™¨èµ„æº'
        elif p95_time > 5:
            return 'æ€§èƒ½é—®é¢˜ï¼šä¼˜åŒ–ä»£ç é€»è¾‘ã€å¢åŠ ç¼“å­˜ã€ä¼˜åŒ–æ•°æ®åº“'
        elif p95_time > 2:
            return 'è½»å¾®æ€§èƒ½é—®é¢˜ï¼šè€ƒè™‘ä»£ç ä¼˜åŒ–ã€ç¼“å­˜ç­–ç•¥'
        elif slow_rate > 10:
            return 'æ…¢è¯·æ±‚ç‡è¿‡é«˜ï¼šæ£€æŸ¥é•¿å°¾è¯·æ±‚ã€ä¼˜åŒ–ç®—æ³•'
        else:
            return 'æ€§èƒ½è‰¯å¥½ï¼šç»§ç»­ç›‘æ§'
    
    def _assess_performance_risk(self, p99_time: float, slow_rate: float) -> str:
        """è¯„ä¼°æ€§èƒ½é£é™©"""
        if p99_time > 15 or slow_rate > 15:
            return 'é«˜é£é™©'
        elif p99_time > 8 or slow_rate > 8:
            return 'ä¸­é£é™©'
        elif p99_time > 3 or slow_rate > 3:
            return 'ä½é£é™©'
        else:
            return 'æ­£å¸¸'
    
    def _get_lifecycle_suggestion(self, p95_time: float, slow_count: int) -> str:
        """è·å–ç”Ÿå‘½å‘¨æœŸä¼˜åŒ–å»ºè®®"""
        if slow_count > 1000:
            return 'å¤§é‡æ…¢è¯·æ±‚ï¼šä¼˜å…ˆæ£€æŸ¥åç«¯æœåŠ¡æ€§èƒ½'
        elif p95_time > 5:
            return 'å“åº”æ—¶é—´è¿‡é•¿ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥å’Œåç«¯å¤„ç†'
        elif p95_time > 2:
            return 'å“åº”æ—¶é—´åé«˜ï¼šè€ƒè™‘ä¼˜åŒ–åç«¯é€»è¾‘'
        else:
            return 'æ€§èƒ½æ­£å¸¸ï¼šç»§ç»­ç›‘æ§'
    
    def _create_method_status_analysis(self) -> pd.DataFrame:
        """åˆ›å»ºHTTPæ–¹æ³•çŠ¶æ€ç åˆ†æ (æ•´åˆåŸç‰ˆæœ¬åŠŸèƒ½)"""
        if not self.method_status_counter:
            return pd.DataFrame()
        
        method_data = []
        for method, status_counter in self.method_status_counter.items():
            method_total = sum(status_counter.values())
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            success_count = sum(count for status, count in status_counter.items() if str(status).startswith('2'))
            redirect_count = sum(count for status, count in status_counter.items() if str(status).startswith('3'))
            client_error_count = sum(count for status, count in status_counter.items() if str(status).startswith('4'))
            server_error_count = sum(count for status, count in status_counter.items() if str(status).startswith('5'))
            
            success_rate = (success_count / method_total * 100) if method_total > 0 else 0
            client_error_rate = (client_error_count / method_total * 100) if method_total > 0 else 0
            server_error_rate = (server_error_count / method_total * 100) if method_total > 0 else 0
            
            # è·å–æœ€å¸¸è§çš„çŠ¶æ€ç 
            most_common_status = status_counter.most_common(1)[0] if status_counter else ('N/A', 0)
            
            method_data.append({
                'HTTPæ–¹æ³•': method,
                'æ€»è¯·æ±‚æ•°': method_total,
                'æˆåŠŸè¯·æ±‚æ•°(2xx)': success_count,
                'é‡å®šå‘è¯·æ±‚æ•°(3xx)': redirect_count,
                'å®¢æˆ·ç«¯é”™è¯¯æ•°(4xx)': client_error_count,
                'æœåŠ¡å™¨é”™è¯¯æ•°(5xx)': server_error_count,
                'æˆåŠŸç‡(%)': round(success_rate, 2),
                'å®¢æˆ·ç«¯é”™è¯¯ç‡(%)': round(client_error_rate, 2),
                'æœåŠ¡å™¨é”™è¯¯ç‡(%)': round(server_error_rate, 2),
                'æœ€å¸¸è§çŠ¶æ€ç ': most_common_status[0],
                'æœ€å¸¸è§çŠ¶æ€ç æ•°é‡': most_common_status[1],
                'å¥åº·çŠ¶æ€': self._assess_method_health(success_rate, server_error_rate),
                'é£é™©ç­‰çº§': self._assess_method_risk(client_error_rate, server_error_rate)
            })
        
        method_df = pd.DataFrame(method_data)
        if not method_df.empty and 'æ€»è¯·æ±‚æ•°' in method_df.columns:
            return method_df.sort_values('æ€»è¯·æ±‚æ•°', ascending=False)
        else:
            return method_df
    
    def _assess_method_health(self, success_rate: float, server_error_rate: float) -> str:
        """è¯„ä¼°HTTPæ–¹æ³•å¥åº·çŠ¶æ€"""
        if server_error_rate > 5:
            return 'ä¸å¥åº·'
        elif server_error_rate > 2 or success_rate < 90:
            return 'éœ€å…³æ³¨'
        else:
            return 'å¥åº·'
    
    def _assess_method_risk(self, client_error_rate: float, server_error_rate: float) -> str:
        """è¯„ä¼°HTTPæ–¹æ³•é£é™©ç­‰çº§"""
        if server_error_rate > 10 or client_error_rate > 20:
            return 'é«˜é£é™©'
        elif server_error_rate > 5 or client_error_rate > 10:
            return 'ä¸­é£é™©'
        elif server_error_rate > 1 or client_error_rate > 5:
            return 'ä½é£é™©'
        else:
            return 'æ­£å¸¸'
    
    def _create_excel_report(self, output_path: str, dataframes: Dict[str, pd.DataFrame]):
        """åˆ›å»ºExcelæŠ¥å‘Š"""
        log_info("ğŸ“ åˆ›å»ºExcelæŠ¥å‘Š...", True)
        
        wb = openpyxl.Workbook()
        if 'Sheet' in wb.sheetnames:
            del wb['Sheet']
        
        # æŠ¥å‘Šç»“æ„
        report_structure = [
            ('æ‘˜è¦åˆ†æ', 'summary'),
            ('çŠ¶æ€ç è¯¦æƒ…', 'detailed_status'),
            ('åº”ç”¨çŠ¶æ€åˆ†æ', 'app_analysis'),
            ('æœåŠ¡çŠ¶æ€åˆ†æ', 'service_analysis'),
            ('æ—¶é—´ç»´åº¦åˆ†æ', 'time_analysis'),
            ('é”™è¯¯åˆ†æ', 'error_analysis'),
            ('æ€§èƒ½å…³è”åˆ†æ', 'performance_analysis'),
            ('æ…¢è¯·æ±‚APIæ±‡æ€»', 'slow_request_api_summary'),  # é‡è¦ï¼
            ('æ€§èƒ½å…³è”è¯¦ç»†åˆ†æ', 'performance_detail_analysis'),  # é‡è¦ï¼
            ('çŠ¶æ€ç ç”Ÿå‘½å‘¨æœŸåˆ†æ', 'status_lifecycle_analysis'),  # é‡è¦ï¼
            ('HTTPæ–¹æ³•çŠ¶æ€ç åˆ†æ', 'method_status_analysis'),  # æ•´åˆåŸç‰ˆæœ¬åŠŸèƒ½
            ('å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š', 'anomaly_report'),
            ('ä¼˜åŒ–å»ºè®®', 'optimization_suggestions')
        ]
        
        for sheet_name, df_key in report_structure:
            if df_key in dataframes and not dataframes[df_key].empty:
                add_dataframe_to_excel_with_grouped_headers(
                    wb, dataframes[df_key], sheet_name
                )
        
        # æ·»åŠ å›¾è¡¨
        self._add_charts_to_excel(wb, dataframes)
        
        wb.save(output_path)
        log_info(f"ğŸ“Š ExcelæŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    def _add_charts_to_excel(self, wb: openpyxl.Workbook, dataframes: Dict[str, pd.DataFrame]):
        """æ·»åŠ å›¾è¡¨åˆ°Excel (æ•´åˆåŸç‰ˆæœ¬åŠŸèƒ½)"""
        try:
            # 1. çŠ¶æ€ç åˆ†å¸ƒé¥¼å›¾
            if 'detailed_status' in dataframes and not dataframes['detailed_status'].empty:
                self._create_status_distribution_pie_chart(wb, dataframes['detailed_status'])
            
            # 2. æ—¶é—´è¶‹åŠ¿å›¾
            if 'time_analysis' in dataframes and not dataframes['time_analysis'].empty:
                self._create_time_trend_charts(wb, dataframes['time_analysis'])
            
            # 3. HTTPæ–¹æ³•åˆ†å¸ƒå›¾
            if 'method_status_analysis' in dataframes and not dataframes['method_status_analysis'].empty:
                self._create_method_distribution_chart(wb, dataframes['method_status_analysis'])
                
        except Exception as e:
            log_info(f"åˆ›å»ºå›¾è¡¨æ—¶å‡ºé”™: {e}", level="WARNING")
    
    def _create_status_distribution_pie_chart(self, wb: openpyxl.Workbook, status_df: pd.DataFrame):
        """åˆ›å»ºçŠ¶æ€ç åˆ†å¸ƒé¥¼å›¾"""
        try:
            chart_sheet = wb.create_sheet('çŠ¶æ€ç åˆ†å¸ƒå›¾')
            
            # å‡†å¤‡æ•°æ® - æŒ‰ç±»åˆ«æ±‡æ€»
            category_data = {}
            for _, row in status_df.iterrows():
                category = row['ç±»åˆ«']
                count = row['è¯·æ±‚æ•°']
                if category in category_data:
                    category_data[category] += count
                else:
                    category_data[category] = count
            
            # å†™å…¥æ•°æ®åˆ°å·¥ä½œè¡¨
            row_idx = 1
            chart_sheet.cell(row=row_idx, column=1, value='çŠ¶æ€ç ç±»åˆ«')
            chart_sheet.cell(row=row_idx, column=2, value='è¯·æ±‚æ•°')
            chart_sheet.cell(row=row_idx, column=3, value='å æ¯”(%)')
            
            total_requests = sum(category_data.values())
            data_rows = []
            
            for category, count in sorted(category_data.items()):
                row_idx += 1
                percentage = (count / total_requests * 100) if total_requests > 0 else 0
                chart_sheet.cell(row=row_idx, column=1, value=category)
                chart_sheet.cell(row=row_idx, column=2, value=count)
                chart_sheet.cell(row=row_idx, column=3, value=round(percentage, 2))
                data_rows.append(row_idx)
            
            if data_rows:
                # åˆ›å»ºé¥¼å›¾
                pie_chart = PieChart()
                pie_chart.title = "HTTPçŠ¶æ€ç ç±»åˆ«åˆ†å¸ƒ"
                pie_chart.width = 15
                pie_chart.height = 10
                
                # è®¾ç½®æ•°æ®å’Œæ ‡ç­¾
                labels = Reference(chart_sheet, min_col=1, min_row=2, max_row=len(data_rows) + 1)
                data = Reference(chart_sheet, min_col=2, min_row=1, max_row=len(data_rows) + 1)
                
                pie_chart.add_data(data, titles_from_data=True)
                pie_chart.set_categories(labels)
                
                # è®¾ç½®æ•°æ®æ ‡ç­¾
                from openpyxl.chart.label import DataLabelList
                pie_chart.dataLabels = DataLabelList()
                pie_chart.dataLabels.showPercent = True
                pie_chart.dataLabels.showCatName = True
                
                # æ·»åŠ å›¾è¡¨åˆ°å·¥ä½œè¡¨
                chart_sheet.add_chart(pie_chart, "E2")
                
        except Exception as e:
            log_info(f"åˆ›å»ºçŠ¶æ€ç åˆ†å¸ƒé¥¼å›¾å¤±è´¥: {e}", level="WARNING")
    
    def _create_time_trend_charts(self, wb: openpyxl.Workbook, time_df: pd.DataFrame):
        """åˆ›å»ºæ—¶é—´è¶‹åŠ¿å›¾"""
        try:
            if time_df.empty:
                return
                
            chart_sheet = wb.create_sheet('æ—¶é—´è¶‹åŠ¿å›¾')
            
            # å‡†å¤‡æ•°æ®
            time_data = []
            for _, row in time_df.iterrows():
                time_data.append({
                    'æ—¶é—´': row['æ—¶é—´'],
                    'æ€»è¯·æ±‚æ•°': row['æ€»è¯·æ±‚æ•°'],
                    'æˆåŠŸç‡': row['æˆåŠŸç‡(%)'],
                    'é”™è¯¯ç‡': row['é”™è¯¯ç‡(%)']
                })
            
            # å†™å…¥æ•°æ®åˆ°å·¥ä½œè¡¨
            headers = ['æ—¶é—´', 'æ€»è¯·æ±‚æ•°', 'æˆåŠŸç‡(%)', 'é”™è¯¯ç‡(%)']
            for col_idx, header in enumerate(headers, 1):
                chart_sheet.cell(row=1, column=col_idx, value=header)
            
            for row_idx, data in enumerate(time_data, 2):
                chart_sheet.cell(row=row_idx, column=1, value=data['æ—¶é—´'])
                chart_sheet.cell(row=row_idx, column=2, value=data['æ€»è¯·æ±‚æ•°'])
                chart_sheet.cell(row=row_idx, column=3, value=data['æˆåŠŸç‡'])
                chart_sheet.cell(row=row_idx, column=4, value=data['é”™è¯¯ç‡'])
            
            if len(time_data) > 1:
                # åˆ›å»ºæŠ˜çº¿å›¾
                from openpyxl.chart import LineChart
                line_chart = LineChart()
                line_chart.title = "æ—¶é—´æ®µè¶‹åŠ¿åˆ†æ"
                line_chart.style = 12
                line_chart.x_axis.title = "æ—¶é—´"
                line_chart.y_axis.title = "ç™¾åˆ†æ¯”(%)"
                line_chart.width = 15
                line_chart.height = 10
                
                # è®¾ç½®æ•°æ®
                categories = Reference(chart_sheet, min_col=1, min_row=2, max_row=len(time_data) + 1)
                success_data = Reference(chart_sheet, min_col=3, min_row=1, max_row=len(time_data) + 1)
                error_data = Reference(chart_sheet, min_col=4, min_row=1, max_row=len(time_data) + 1)
                
                line_chart.add_data(success_data, titles_from_data=True)
                line_chart.add_data(error_data, titles_from_data=True)
                line_chart.set_categories(categories)
                
                # è®¾ç½®é¢œè‰²
                if len(line_chart.series) > 0:
                    line_chart.series[0].graphicalProperties.line.solidFill = "92D050"  # ç»¿è‰²
                if len(line_chart.series) > 1:
                    line_chart.series[1].graphicalProperties.line.solidFill = "FF0000"  # çº¢è‰²
                
                chart_sheet.add_chart(line_chart, "F2")
                
        except Exception as e:
            log_info(f"åˆ›å»ºæ—¶é—´è¶‹åŠ¿å›¾å¤±è´¥: {e}", level="WARNING")
    
    def _create_method_distribution_chart(self, wb: openpyxl.Workbook, method_df: pd.DataFrame):
        """åˆ›å»ºHTTPæ–¹æ³•åˆ†å¸ƒå›¾"""
        try:
            if method_df.empty:
                return
                
            chart_sheet = wb.create_sheet('HTTPæ–¹æ³•åˆ†å¸ƒå›¾')
            
            # å†™å…¥æ•°æ®
            headers = ['HTTPæ–¹æ³•', 'æ€»è¯·æ±‚æ•°', 'æˆåŠŸç‡(%)', 'é”™è¯¯ç‡(%)']
            for col_idx, header in enumerate(headers, 1):
                chart_sheet.cell(row=1, column=col_idx, value=header)
            
            for row_idx, (_, row) in enumerate(method_df.iterrows(), 2):
                chart_sheet.cell(row=row_idx, column=1, value=row['HTTPæ–¹æ³•'])
                chart_sheet.cell(row=row_idx, column=2, value=row['æ€»è¯·æ±‚æ•°'])
                chart_sheet.cell(row=row_idx, column=3, value=row['æˆåŠŸç‡(%)'])
                chart_sheet.cell(row=row_idx, column=4, value=row['å®¢æˆ·ç«¯é”™è¯¯ç‡(%)'] + row['æœåŠ¡å™¨é”™è¯¯ç‡(%)'])
            
            if len(method_df) > 0:
                # åˆ›å»ºæŸ±çŠ¶å›¾
                from openpyxl.chart import BarChart
                bar_chart = BarChart()
                bar_chart.type = "col"
                bar_chart.style = 10
                bar_chart.title = "HTTPæ–¹æ³•è¯·æ±‚åˆ†å¸ƒ"
                bar_chart.x_axis.title = "HTTPæ–¹æ³•"
                bar_chart.y_axis.title = "è¯·æ±‚æ•°"
                bar_chart.width = 15
                bar_chart.height = 10
                
                # è®¾ç½®æ•°æ®
                categories = Reference(chart_sheet, min_col=1, min_row=2, max_row=len(method_df) + 1)
                data = Reference(chart_sheet, min_col=2, min_row=1, max_row=len(method_df) + 1)
                
                bar_chart.add_data(data, titles_from_data=True)
                bar_chart.set_categories(categories)
                
                chart_sheet.add_chart(bar_chart, "F2")
                
        except Exception as e:
            log_info(f"åˆ›å»ºHTTPæ–¹æ³•åˆ†å¸ƒå›¾å¤±è´¥: {e}", level="WARNING")
    
    # è¾…åŠ©æ–¹æ³•
    def _get_status_category(self, status_code: str) -> str:
        """è·å–çŠ¶æ€ç ç±»åˆ«"""
        if status_code.startswith('2'):
            return 'æˆåŠŸ'
        elif status_code.startswith('3'):
            return 'é‡å®šå‘'
        elif status_code.startswith('4'):
            return 'å®¢æˆ·ç«¯é”™è¯¯'
        elif status_code.startswith('5'):
            return 'æœåŠ¡å™¨é”™è¯¯'
        else:
            return 'æœªçŸ¥'
    
    def _get_response_time_stats(self, status) -> Dict[str, float]:
        """è·å–å“åº”æ—¶é—´ç»Ÿè®¡"""
        tdigest = self.status_response_time.get(status)
        if not tdigest or tdigest.count == 0:
            return {'mean': 0, 'p95': 0, 'p99': 0}
        
        return {
            'mean': round(tdigest.percentile(50), 3),
            'p95': round(tdigest.percentile(95), 3),
            'p99': round(tdigest.percentile(99), 3)
        }
    
    def _assess_impact_level(self, status_code: str, count: int, percentage: float) -> str:
        """è¯„ä¼°å½±å“ç­‰çº§"""
        if status_code.startswith('5'):
            return 'Critical'
        elif status_code.startswith('4') and percentage > 5:
            return 'High'
        elif percentage > 10:
            return 'Medium'
        else:
            return 'Low'
    
    def _assess_app_health(self, error_rate: float) -> str:
        """è¯„ä¼°åº”ç”¨å¥åº·çŠ¶æ€"""
        if error_rate > 10:
            return 'ä¸å¥åº·'
        elif error_rate > 5:
            return 'éœ€å…³æ³¨'
        else:
            return 'å¥åº·'
    
    def _assess_service_health(self, error_rate: float) -> str:
        """è¯„ä¼°æœåŠ¡å¥åº·çŠ¶æ€"""
        if error_rate > 10:
            return 'ä¸å¥åº·'
        elif error_rate > 5:
            return 'éœ€å…³æ³¨'
        else:
            return 'å¥åº·'
    
    def _assess_traffic_level(self, request_count: int) -> str:
        """è¯„ä¼°æµé‡ç­‰çº§"""
        if request_count > 10000:
            return 'é«˜æµé‡'
        elif request_count > 1000:
            return 'ä¸­æµé‡'
        else:
            return 'ä½æµé‡'
    
    def _assess_error_severity(self, status_code: str) -> str:
        """è¯„ä¼°é”™è¯¯ä¸¥é‡ç¨‹åº¦"""
        if status_code.startswith('5'):
            return 'Critical'
        elif status_code in ['400', '401', '403', '404']:
            return 'High'
        else:
            return 'Medium'
    
    def _get_error_suggestion(self, status_code: str) -> str:
        """è·å–é”™è¯¯å¤„ç†å»ºè®®"""
        suggestions = {
            '400': 'æ£€æŸ¥è¯·æ±‚å‚æ•°æ ¼å¼å’Œå¿…å¡«å­—æ®µ',
            '401': 'æ£€æŸ¥è®¤è¯æœºåˆ¶å’Œtokenæœ‰æ•ˆæ€§',
            '403': 'æ£€æŸ¥ç”¨æˆ·æƒé™å’Œè®¿é—®æ§åˆ¶',
            '404': 'æ£€æŸ¥URLè·¯å¾„å’Œèµ„æºæ˜¯å¦å­˜åœ¨',
            '500': 'æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—å’Œåº”ç”¨ç¨‹åºé”™è¯¯',
            '502': 'æ£€æŸ¥åç«¯æœåŠ¡å’Œè´Ÿè½½å‡è¡¡é…ç½®',
            '503': 'æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§å’Œèµ„æºé…ç½®',
            '504': 'æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œè¶…æ—¶é…ç½®'
        }
        return suggestions.get(status_code, 'æ£€æŸ¥ç›¸å…³é…ç½®å’ŒæœåŠ¡çŠ¶æ€')
    
    def _assess_performance_level(self, p95_time: float, slow_rate: float) -> str:
        """è¯„ä¼°æ€§èƒ½ç­‰çº§"""
        if p95_time > 5 or slow_rate > 10:
            return 'Poor'
        elif p95_time > 2 or slow_rate > 5:
            return 'Fair'
        else:
            return 'Good'


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.anomalies = []
        self.status_baseline = {}
        self.performance_baseline = {}
    
    def process_chunk(self, chunk: pd.DataFrame, status_field: str, time_field: str):
        """å¤„ç†æ•°æ®å—è¿›è¡Œå¼‚å¸¸æ£€æµ‹"""
        if not status_field:
            return
        
        # æ£€æµ‹çŠ¶æ€ç å¼‚å¸¸
        self._detect_status_anomalies(chunk, status_field)
        
        # æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        if time_field:
            self._detect_performance_anomalies(chunk, status_field, time_field)
    
    def _detect_status_anomalies(self, chunk: pd.DataFrame, status_field: str):
        """æ£€æµ‹çŠ¶æ€ç å¼‚å¸¸"""
        status_counts = chunk[status_field].value_counts()
        
        for status, count in status_counts.items():
            # æ£€æµ‹5xxé”™è¯¯çªå¢
            if str(status).startswith('5') and count > 100:
                self.anomalies.append({
                    'type': 'æœåŠ¡å™¨é”™è¯¯çªå¢',
                    'description': f'çŠ¶æ€ç {status}åœ¨å•ä¸ªæ•°æ®å—ä¸­å‡ºç°{count}æ¬¡',
                    'status_code': status,
                    'value': count,
                    'severity': 'Critical',
                    'detected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'suggestion': 'ç«‹å³æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€å’Œåº”ç”¨ç¨‹åºæ—¥å¿—'
                })
            
            # æ£€æµ‹4xxé”™è¯¯å¼‚å¸¸
            elif str(status).startswith('4') and count > 500:
                self.anomalies.append({
                    'type': 'å®¢æˆ·ç«¯é”™è¯¯å¼‚å¸¸',
                    'description': f'çŠ¶æ€ç {status}åœ¨å•ä¸ªæ•°æ®å—ä¸­å‡ºç°{count}æ¬¡',
                    'status_code': status,
                    'value': count,
                    'severity': 'High',
                    'detected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'suggestion': 'æ£€æŸ¥å®¢æˆ·ç«¯è¯·æ±‚æ ¼å¼å’ŒAPIæ–‡æ¡£'
                })
    
    def _detect_performance_anomalies(self, chunk: pd.DataFrame, status_field: str, time_field: str):
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        try:
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            avg_response_time = chunk[time_field].astype(float).mean()
            
            # æ£€æµ‹å“åº”æ—¶é—´å¼‚å¸¸
            if avg_response_time > 10:
                self.anomalies.append({
                    'type': 'å“åº”æ—¶é—´å¼‚å¸¸',
                    'description': f'å¹³å‡å“åº”æ—¶é—´è¾¾åˆ°{avg_response_time:.2f}ç§’',
                    'value': f'{avg_response_time:.2f}s',
                    'severity': 'High',
                    'detected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'suggestion': 'æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢ã€ç½‘ç»œè¿æ¥å’ŒæœåŠ¡å™¨è´Ÿè½½'
                })
        except Exception:
            pass
    
    def get_anomalies(self) -> List[Dict]:
        """è·å–æ£€æµ‹åˆ°çš„å¼‚å¸¸"""
        return self.anomalies


# ä¸»è¦åˆ†æå‡½æ•°
def analyze_status_codes(csv_path: str, output_path: str, slow_request_threshold: float = DEFAULT_SLOW_THRESHOLD) -> pd.DataFrame:
    """
    é«˜çº§çŠ¶æ€ç åˆ†æä¸»å‡½æ•°
    
    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
        slow_request_threshold: æ…¢è¯·æ±‚é˜ˆå€¼(ç§’)
    
    Returns:
        æ‘˜è¦æ•°æ®DataFrame
    """
    analyzer = AdvancedStatusAnalyzer(slow_threshold=slow_request_threshold)
    return analyzer.analyze_status_codes(csv_path, output_path)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_csv = "test_data.csv"
    test_output = "status_analysis_advanced.xlsx"
    
    if os.path.exists(test_csv):
        result = analyze_status_codes(test_csv, test_output)
        print("çŠ¶æ€ç åˆ†æå®Œæˆï¼")
        print(result.head())
    else:
        print("æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")