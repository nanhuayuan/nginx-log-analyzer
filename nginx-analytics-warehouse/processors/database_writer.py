#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“å†™å…¥å™¨æ¨¡å— - ä¸“é—¨è´Ÿè´£æ•°æ®å†™å…¥
Database Writer Module - Specialized for Data Insertion

ä¸“é—¨è´Ÿè´£å°†å¤„ç†åçš„æ•°æ®å†™å…¥ClickHouseæ•°æ®åº“
æ”¯æŒODSå’ŒDWDå±‚çš„æ•°æ®å†™å…¥ï¼ŒåŒ…æ‹¬æ‰¹é‡å†™å…¥ã€é”™è¯¯å¤„ç†ã€çŠ¶æ€è·Ÿè¸ªç­‰
"""

import json
import time
from datetime import datetime, date
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import logging
import clickhouse_connect
from clickhouse_connect.driver.exceptions import ClickHouseError

class DatabaseWriter:
    """æ•°æ®åº“å†™å…¥å™¨"""
    
    def __init__(self, host: str = 'localhost', port: int = 8123, 
                 database: str = 'nginx_analytics', user: str = 'analytics_user', password: str = 'analytics_password'):
        self.config = {
            'host': host,
            'port': port,
            'username': user,
            'password': password,
            'database': database
        }
        self.database = database
        self.client = None
        self.logger = logging.getLogger(__name__)
        
        # æ‰¹é‡å†™å…¥é…ç½®
        self.batch_size = 1000
        self.max_retries = 3
        self.retry_delay = 1.0
        
        # è¡¨ç»“æ„æ˜ å°„ - åŸºäºå®é™…DDL
        self.ods_fields = [
            'id', 'log_time', 'server_name', 'client_ip', 'client_port', 'xff_ip',
            'remote_user', 'request_method', 'request_uri', 'request_full_uri',
            'http_protocol', 'response_status_code', 'response_body_size',
            'response_referer', 'user_agent', 'upstream_addr', 'upstream_connect_time',
            'upstream_header_time', 'upstream_response_time', 'total_request_time',
            'total_bytes_sent', 'query_string', 'connection_requests', 'trace_id',
            'business_sign', 'application_name', 'service_name', 'cache_status',
            'cluster_node', 'log_source_file', 'created_at'
        ]
        
        self.dwd_fields = [
            'id', 'ods_id', 'log_time', 'date_partition', 'hour_partition',
            'minute_partition', 'second_partition', 'client_ip', 'client_port',
            'xff_ip', 'server_name', 'request_method', 'request_uri',
            'request_uri_normalized', 'request_full_uri', 'query_parameters',
            'http_protocol_version', 'response_status_code', 'response_body_size',
            'response_body_size_kb', 'total_bytes_sent', 'total_bytes_sent_kb',
            'total_request_duration', 'upstream_connect_time', 'upstream_header_time',
            'upstream_response_time', 'backend_connect_phase', 'backend_process_phase',
            'backend_transfer_phase', 'nginx_transfer_phase', 'backend_total_phase',
            'network_phase', 'processing_phase', 'transfer_phase',
            'response_transfer_speed', 'total_transfer_speed', 'nginx_transfer_speed',
            'backend_efficiency', 'network_overhead', 'transfer_ratio',
            'connection_cost_ratio', 'processing_efficiency_index', 'platform',
            'platform_version', 'app_version', 'device_type', 'browser_type',
            'os_type', 'os_version', 'sdk_type', 'sdk_version', 'bot_type',
            'entry_source', 'referer_domain', 'search_engine', 'social_media',
            'api_category', 'api_module', 'api_version', 'business_domain',
            'access_type', 'client_category', 'application_name', 'service_name',
            'trace_id', 'business_sign', 'cluster_node', 'upstream_server',
            'connection_requests', 'cache_status', 'referer_url', 'user_agent_string',
            'log_source_file', 'is_success', 'is_business_success', 'is_slow',
            'is_very_slow', 'is_error', 'is_client_error', 'is_server_error',
            'has_anomaly', 'anomaly_type', 'user_experience_level',
            'apdex_classification', 'api_importance', 'business_value_score',
            'data_quality_score', 'parsing_errors', 'client_region', 'client_isp',
            'ip_risk_level', 'is_internal_ip', 'created_at', 'updated_at'
        ]
    
    def connect(self) -> bool:
        """è¿æ¥æ•°æ®åº“"""
        try:
            # å…ˆè¿æ¥é»˜è®¤æ•°æ®åº“æµ‹è¯•è¿æ¥
            temp_config = self.config.copy()
            temp_config['database'] = 'default'
            
            self.client = clickhouse_connect.get_client(**temp_config)
            # æµ‹è¯•è¿æ¥
            self.client.command("SELECT 1")
            self.logger.info(f"æˆåŠŸè¿æ¥åˆ°ClickHouse: {self.config['host']}:{self.config['port']}")
            return True
            
        except Exception as e:
            self.logger.error(f"è¿æ¥ClickHouseå¤±è´¥: {e}")
            return False
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            try:
                self.client.close()
                self.logger.info("ClickHouseè¿æ¥å·²å…³é—­")
            except:
                pass
    
    def prepare_ods_data(self, processed_record: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡ODSå±‚æ•°æ®"""
        ods_data = {}
        
        # å­—æ®µæ˜ å°„å’Œè½¬æ¢ - åŸºäºå®é™…DDLå­—æ®µ
        field_mapping = {
            'id': processed_record.get('id'),
            'log_time': processed_record.get('log_time'),
            'server_name': processed_record.get('http_host', ''),
            'client_ip': processed_record.get('remote_addr', ''),
            'client_port': 0,  # æš‚æ—¶æ²¡æœ‰ç«¯å£ä¿¡æ¯
            'xff_ip': processed_record.get('xff', ''),
            'remote_user': '',  # æš‚æ—¶æ²¡æœ‰è¿œç¨‹ç”¨æˆ·ä¿¡æ¯
            'request_method': processed_record.get('method', ''),
            'request_uri': processed_record.get('uri_path', ''),
            'request_full_uri': processed_record.get('uri', ''),
            'http_protocol': processed_record.get('protocol', ''),
            'response_status_code': processed_record.get('code', ''),
            'response_body_size': processed_record.get('body_int', 0),
            'response_referer': processed_record.get('referer', ''),
            'user_agent': processed_record.get('agent', ''),
            'upstream_addr': '',  # æš‚æ—¶æ²¡æœ‰ä¸Šæ¸¸åœ°å€
            'upstream_connect_time': processed_record.get('upstream_connect_time_float', 0.0),
            'upstream_header_time': processed_record.get('upstream_header_time_float', 0.0),
            'upstream_response_time': processed_record.get('upstream_response_time_float', 0.0),
            'total_request_time': processed_record.get('ar_time_float', 0.0),
            'total_bytes_sent': processed_record.get('body_int', 0),
            'query_string': processed_record.get('query_string', ''),
            'connection_requests': 1,  # é»˜è®¤1ä¸ªè¿æ¥è¯·æ±‚
            'trace_id': '',  # æš‚æ—¶æ²¡æœ‰è·Ÿè¸ªID
            'business_sign': processed_record.get('api_category', ''),
            'application_name': processed_record.get('platform', ''),
            'service_name': processed_record.get('api_module', ''),
            'cache_status': '',  # æš‚æ—¶æ²¡æœ‰ç¼“å­˜çŠ¶æ€
            'cluster_node': '',  # æš‚æ—¶æ²¡æœ‰é›†ç¾¤èŠ‚ç‚¹
            'log_source_file': processed_record.get('source_file', ''),
            'created_at': datetime.now()
        }
        
        # åªåŒ…å«ODSè¡¨éœ€è¦çš„å­—æ®µ
        for field in self.ods_fields:
            ods_data[field] = field_mapping.get(field)
            
            # å¤„ç†Noneå€¼
            if ods_data[field] is None:
                if field in ['id', 'response_body_size', 'total_bytes_sent', 'client_port', 'connection_requests']:
                    ods_data[field] = 0
                elif field in ['total_request_time', 'upstream_connect_time', 
                             'upstream_header_time', 'upstream_response_time']:
                    ods_data[field] = 0.0
                elif field in ['created_at']:
                    ods_data[field] = datetime.now()
                else:
                    ods_data[field] = ''
        
        return ods_data
    
    def prepare_dwd_data(self, processed_record: Dict[str, Any], ods_id: int) -> Dict[str, Any]:
        """å‡†å¤‡DWDå±‚æ•°æ®"""
        dwd_data = {}
        
        # åŸºç¡€å­—æ®µèµ‹å€¼
        for field in self.dwd_fields:
            value = processed_record.get(field)
            
            # ç‰¹æ®Šå­—æ®µå¤„ç†
            if field == 'ods_id':
                value = ods_id
            elif field == 'request_uri_normalized':
                value = processed_record.get('uri_path', '')
            elif field == 'total_bytes_sent_kb':
                value = processed_record.get('response_body_size', 0) / 1024.0
            elif field == 'parsing_errors':
                errors = processed_record.get('parsing_errors', [])
                value = errors if isinstance(errors, list) else []
            
            # å¤„ç†Noneå€¼
            if value is None:
                if field in ['id', 'ods_id', 'response_body_size', 'total_bytes_sent', 
                           'client_port', 'connection_requests', 'business_value_score']:
                    value = 0
                elif field in ['total_request_duration', 'upstream_connect_time', 
                             'upstream_header_time', 'upstream_response_time',
                             'backend_connect_phase', 'backend_process_phase',
                             'backend_transfer_phase', 'nginx_transfer_phase',
                             'backend_total_phase', 'network_phase', 'processing_phase',
                             'transfer_phase', 'response_transfer_speed',
                             'total_transfer_speed', 'nginx_transfer_speed',
                             'backend_efficiency', 'network_overhead', 'transfer_ratio',
                             'connection_cost_ratio', 'processing_efficiency_index',
                             'response_body_size_kb', 'total_bytes_sent_kb',
                             'data_quality_score']:
                    value = 0.0
                elif field in ['is_success', 'is_business_success', 'is_slow',
                             'is_very_slow', 'is_error', 'is_client_error',
                             'is_server_error', 'has_anomaly', 'is_internal_ip']:
                    value = False
                elif field in ['created_at', 'updated_at']:
                    value = datetime.now()
                elif field in ['date_partition']:
                    log_time = processed_record.get('log_time')
                    value = log_time.date() if log_time else date.today()
                elif field in ['hour_partition', 'minute_partition', 'second_partition']:
                    log_time = processed_record.get('log_time')
                    if log_time:
                        if field == 'hour_partition':
                            value = log_time.hour
                        elif field == 'minute_partition':
                            value = log_time.minute
                        else:
                            value = log_time.second
                    else:
                        value = 0
                elif field == 'parsing_errors':
                    value = []
                else:
                    value = ''
            
            dwd_data[field] = value
        
        return dwd_data
    
    def insert_ods_batch(self, ods_records: List[Dict[str, Any]]) -> Tuple[bool, int, str]:
        """æ‰¹é‡æ’å…¥ODSæ•°æ®"""
        if not ods_records:
            return True, 0, "æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥"
        
        try:
            # å‡†å¤‡æ’å…¥æ•°æ® - è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            insert_data = []
            for record in ods_records:
                row_data = []
                for field in self.ods_fields:
                    value = record.get(field)
                    row_data.append(value)
                insert_data.append(row_data)
            
            # æ‰§è¡Œæ’å…¥
            start_time = time.time()
            self.client.insert(
                f'{self.database}.ods_nginx_raw',
                insert_data,
                column_names=self.ods_fields
            )
            duration = time.time() - start_time
            
            self.logger.info(f"æˆåŠŸæ’å…¥ODSæ•°æ® {len(insert_data)} æ¡ï¼Œè€—æ—¶ {duration:.2f}s")
            return True, len(insert_data), f"æ’å…¥æˆåŠŸï¼Œè€—æ—¶ {duration:.2f}s"
            
        except Exception as e:
            error_str = str(e)
            if "filesystem error" in error_str or "Code: 1001" in error_str:
                error_msg = f"ClickHouseç£ç›˜ç©ºé—´ä¸è¶³æˆ–æƒé™é—®é¢˜ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®: {error_str}"
            else:
                error_msg = f"ODSæ•°æ®æ’å…¥å¤±è´¥: {error_str}"
            self.logger.error(error_msg)
            return False, 0, error_msg
    
    def insert_dwd_batch(self, dwd_records: List[Dict[str, Any]]) -> Tuple[bool, int, str]:
        """æ‰¹é‡æ’å…¥DWDæ•°æ®"""
        if not dwd_records:
            return True, 0, "æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥"
        
        try:
            # å‡†å¤‡æ’å…¥æ•°æ®
            insert_data = []
            for record in dwd_records:
                row_data = []
                for field in self.dwd_fields:
                    value = record.get(field)
                    row_data.append(value)
                insert_data.append(row_data)
            
            # æ‰§è¡Œæ’å…¥
            start_time = time.time()
            self.client.insert(
                f'{self.database}.dwd_nginx_enriched_v2',
                insert_data,
                column_names=self.dwd_fields
            )
            duration = time.time() - start_time
            
            self.logger.info(f"æˆåŠŸæ’å…¥DWDæ•°æ® {len(insert_data)} æ¡ï¼Œè€—æ—¶ {duration:.2f}s")
            return True, len(insert_data), f"æ’å…¥æˆåŠŸï¼Œè€—æ—¶ {duration:.2f}s"
            
        except Exception as e:
            error_str = str(e)
            if "filesystem error" in error_str or "Code: 1001" in error_str:
                error_msg = f"ClickHouseç£ç›˜ç©ºé—´ä¸è¶³æˆ–æƒé™é—®é¢˜ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨é…ç½®: {error_str}"
            else:
                error_msg = f"DWDæ•°æ®æ’å…¥å¤±è´¥: {error_str}"
            self.logger.error(error_msg)
            return False, 0, error_msg
    
    def write_processed_records(self, processed_records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å†™å…¥å¤„ç†åçš„è®°å½•åˆ°ODSå’ŒDWDå±‚"""
        if not processed_records:
            return {
                'success': False,
                'message': 'æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥',
                'ods_count': 0,
                'dwd_count': 0,
                'errors': []
            }
        
        start_time = time.time()
        errors = []
        ods_count = 0
        dwd_count = 0
        
        try:
            # åˆ†æ‰¹å¤„ç†
            batches = [processed_records[i:i + self.batch_size] 
                      for i in range(0, len(processed_records), self.batch_size)]
            
            self.logger.info(f"å¼€å§‹å†™å…¥æ•°æ®ï¼Œå…± {len(processed_records)} æ¡è®°å½•ï¼Œåˆ† {len(batches)} æ‰¹å¤„ç†")
            
            for batch_idx, batch in enumerate(batches, 1):
                self.logger.info(f"å¤„ç†ç¬¬ {batch_idx}/{len(batches)} æ‰¹ï¼Œ{len(batch)} æ¡è®°å½•")
                
                # å‡†å¤‡ODSæ•°æ®
                ods_batch = []
                for record in batch:
                    ods_data = self.prepare_ods_data(record)
                    ods_batch.append(ods_data)
                
                # æ’å…¥ODSæ•°æ®
                ods_success, ods_inserted, ods_message = self.insert_ods_batch(ods_batch)
                if not ods_success:
                    errors.append(f"æ‰¹æ¬¡ {batch_idx} ODSæ’å…¥å¤±è´¥: {ods_message}")
                    continue
                
                ods_count += ods_inserted
                
                # å‡†å¤‡DWDæ•°æ®
                dwd_batch = []
                for i, record in enumerate(batch):
                    # ä½¿ç”¨è®°å½•IDä½œä¸ºODS_ID
                    ods_id = record.get('id', i + 1)
                    dwd_data = self.prepare_dwd_data(record, ods_id)
                    dwd_batch.append(dwd_data)
                
                # æ’å…¥DWDæ•°æ®
                dwd_success, dwd_inserted, dwd_message = self.insert_dwd_batch(dwd_batch)
                if not dwd_success:
                    errors.append(f"æ‰¹æ¬¡ {batch_idx} DWDæ’å…¥å¤±è´¥: {dwd_message}")
                    continue
                
                dwd_count += dwd_inserted
                
                self.logger.info(f"æ‰¹æ¬¡ {batch_idx} å®Œæˆ: ODS {ods_inserted} æ¡, DWD {dwd_inserted} æ¡")
            
            duration = time.time() - start_time
            success = len(errors) == 0
            
            result = {
                'success': success,
                'message': f"æ•°æ®å†™å…¥å®Œæˆï¼Œè€—æ—¶ {duration:.2f}s",
                'ods_count': ods_count,
                'dwd_count': dwd_count,
                'duration': duration,
                'errors': errors
            }
            
            if success:
                self.logger.info(f"æ•°æ®å†™å…¥æˆåŠŸ: ODS {ods_count} æ¡, DWD {dwd_count} æ¡, è€—æ—¶ {duration:.2f}s")
            else:
                self.logger.warning(f"æ•°æ®å†™å…¥éƒ¨åˆ†å¤±è´¥: {len(errors)} ä¸ªé”™è¯¯")
            
            return result
            
        except Exception as e:
            error_msg = f"æ•°æ®å†™å…¥è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}"
            self.logger.error(error_msg)
            return {
                'success': False,
                'message': error_msg,
                'ods_count': ods_count,
                'dwd_count': dwd_count,
                'duration': time.time() - start_time,
                'errors': errors + [error_msg]
            }
    
    def clear_data(self, table_names: Optional[List[str]] = None, 
                   date_filter: Optional[str] = None) -> Dict[str, Any]:
        """æ¸…ç†æ•°æ®"""
        if table_names is None:
            table_names = ['ods_nginx_raw', 'dwd_nginx_enriched_v2']
        
        results = {}
        
        for table_name in table_names:
            try:
                if date_filter:
                    # æŒ‰æ—¥æœŸåˆ é™¤
                    sql = f"DELETE FROM {self.database}.{table_name} WHERE date = '{date_filter}'"
                    self.logger.info(f"æ¸…ç†è¡¨ {table_name} ä¸­æ—¥æœŸä¸º {date_filter} çš„æ•°æ®")
                else:
                    # æ¸…ç©ºæ•´è¡¨
                    sql = f"TRUNCATE TABLE {self.database}.{table_name}"
                    self.logger.info(f"æ¸…ç©ºè¡¨ {table_name}")
                
                start_time = time.time()
                self.client.command(sql)
                duration = time.time() - start_time
                
                results[table_name] = {
                    'success': True,
                    'message': f"æ¸…ç†æˆåŠŸï¼Œè€—æ—¶ {duration:.2f}s",
                    'duration': duration
                }
                
            except Exception as e:
                error_msg = f"æ¸…ç†è¡¨ {table_name} å¤±è´¥: {str(e)}"
                self.logger.error(error_msg)
                results[table_name] = {
                    'success': False,
                    'message': error_msg,
                    'duration': 0
                }
        
        return results
    
    def get_data_counts(self) -> Dict[str, int]:
        """è·å–å„è¡¨æ•°æ®é‡"""
        counts = {}
        
        tables = ['ods_nginx_raw', 'dwd_nginx_enriched_v2']
        
        for table in tables:
            try:
                result = self.client.query(f"SELECT COUNT(*) FROM {self.database}.{table}")
                counts[table] = result.result_rows[0][0] if result.result_rows else 0
            except Exception as e:
                self.logger.error(f"è·å–è¡¨ {table} æ•°æ®é‡å¤±è´¥: {e}")
                counts[table] = -1
        
        return counts
    
    def check_table_status(self) -> Dict[str, Any]:
        """æ£€æŸ¥è¡¨çŠ¶æ€"""
        status = {}
        
        tables = ['ods_nginx_raw', 'dwd_nginx_enriched_v2']
        
        for table in tables:
            try:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                result = self.client.query(f"EXISTS TABLE {self.database}.{table}")
                exists = result.result_rows[0][0] if result.result_rows else False
                
                if exists:
                    # è·å–è¡¨ä¿¡æ¯
                    count_result = self.client.query(f"SELECT COUNT(*) FROM {self.database}.{table}")
                    count = count_result.result_rows[0][0] if count_result.result_rows else 0
                    
                    # è·å–æœ€æ–°æ•°æ®æ—¶é—´
                    try:
                        time_result = self.client.query(f"SELECT MAX(created_at) FROM {self.database}.{table}")
                        last_update = time_result.result_rows[0][0] if time_result.result_rows and time_result.result_rows[0][0] else None
                    except:
                        last_update = None
                    
                    status[table] = {
                        'exists': True,
                        'count': count,
                        'last_update': last_update,
                        'status': 'OK'
                    }
                else:
                    status[table] = {
                        'exists': False,
                        'count': 0,
                        'last_update': None,
                        'status': 'NOT_EXISTS'
                    }
                    
            except Exception as e:
                status[table] = {
                    'exists': False,
                    'count': -1,
                    'last_update': None,
                    'status': f'ERROR: {str(e)}'
                }
        
        return status

def test_writer():
    """æµ‹è¯•æ•°æ®åº“å†™å…¥å™¨"""
    writer = DatabaseWriter()
    
    print("=" * 60)
    print("æ•°æ®åº“å†™å…¥å™¨æµ‹è¯•")
    print("=" * 60)
    
    # è¿æ¥æµ‹è¯•
    if not writer.connect():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
        return
    
    print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    # è¡¨çŠ¶æ€æ£€æŸ¥
    status = writer.check_table_status()
    print(f"ğŸ“Š è¡¨çŠ¶æ€:")
    for table, info in status.items():
        print(f"   {table}: {'å­˜åœ¨' if info['exists'] else 'ä¸å­˜åœ¨'} - {info['count']} æ¡è®°å½•")
    
    # æµ‹è¯•æ•°æ®
    test_records = [
        {
            'id': 123456789,
            'log_time': datetime.now(),
            'http_host': 'test.example.com',
            'remote_addr': '192.168.1.1',
            'method': 'GET',
            'uri_path': '/api/test',
            'uri': '/api/test?param=1',
            'query_string': 'param=1',
            'protocol': 'HTTP/1.1',
            'code': '200',
            'body_int': 1024,
            'ar_time_float': 0.123,
            'agent': 'Test-Agent/1.0',
            'platform': 'Test',
            'device_type': 'Desktop',
            'api_category': 'test',
            'api_module': 'test',
            'is_success': True,
            'data_quality_score': 0.9
        }
    ]
    
    # å†™å…¥æµ‹è¯•
    print(f"ğŸ”„ å¼€å§‹å†™å…¥æµ‹è¯•æ•°æ® ({len(test_records)} æ¡)...")
    result = writer.write_processed_records(test_records)
    
    if result['success']:
        print(f"âœ… å†™å…¥æˆåŠŸ:")
        print(f"   ODS: {result['ods_count']} æ¡")
        print(f"   DWD: {result['dwd_count']} æ¡")
        print(f"   è€—æ—¶: {result['duration']:.2f}s")
    else:
        print(f"âŒ å†™å…¥å¤±è´¥: {result['message']}")
        for error in result.get('errors', []):
            print(f"   é”™è¯¯: {error}")
    
    writer.close()

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    test_writer()