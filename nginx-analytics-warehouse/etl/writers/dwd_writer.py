#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DWDå±‚æ•°æ®å†™å…¥å™¨ - æ”¯æŒæ–°è¡¨ç»“æ„ (ä¿®å¤ç‰ˆ)
DWD Writer - Supports new table structure (Fixed version)

è´Ÿè´£å°†æ˜ å°„åçš„æ•°æ®å†™å…¥dwd_nginx_enriched_v3è¡¨
"""

import logging
import threading
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
import clickhouse_connect
from clickhouse_connect.driver.exceptions import ClickHouseError

class DWDWriter:
    """DWDå±‚æ•°æ®å†™å…¥å™¨"""

    def __init__(self, host: str = 'localhost', port: int = 8123,
                 database: str = 'nginx_analytics', username: str = 'analytics_user',
                 password: str = 'analytics_password_change_in_prod'):
        """
        åˆå§‹åŒ–DWDå†™å…¥å™¨

        Args:
            host: ClickHouseä¸»æœº
            port: ClickHouseç«¯å£
            database: æ•°æ®åº“å
            username: ç”¨æˆ·å
            password: å¯†ç 
        """
        self.config = {
            'host': host,
            'port': port,
            'database': database,
            'username': username,
            'password': password
        }

        self.client = None
        self.logger = logging.getLogger(__name__)
        self._write_lock = threading.Lock()  # æ·»åŠ å†™å…¥é”é˜²æ­¢å¹¶å‘å†™å…¥åŒä¸€è¿æ¥

        # æ ¹æ®æ•°æ®åº“å®é™…ç»“æ„ç”Ÿæˆçš„284ä¸ªå­—æ®µï¼ˆ2025-09-14ï¼‰- å®Œæ•´å­—æ®µåˆ—è¡¨
        # æ’é™¤MATERIALIZEDå­—æ®µï¼šdate, hour, minute, second, quarter, week, day_of_year,
        # date_hour, date_hour_minute, weekday, is_weekend, time_period, business_hours_type, traffic_pattern
        self.dwd_fields = [
            'id',
            'ods_id',
            'log_time',
            'date_partition',
            'hour_partition',
            'minute_partition',
            'second_partition',
            'quarter_partition',
            'week_partition',
            'tenant_code',
            'team_code',
            'environment',
            'data_sensitivity',
            'cost_center',
            'business_unit',
            'region_code',
            'compliance_zone',
            'client_ip',
            'client_port',
            'xff_ip',
            'client_real_ip',
            'client_ip_type',
            'client_ip_classification',
            'client_country',
            'client_region',
            'client_city',
            'client_isp',
            'client_org',
            'client_asn',
            'server_name',
            'server_port',
            'server_protocol',
            'load_balancer_node',
            'edge_location',
            'datacenter',
            'availability_zone',
            'cluster_node',
            'instance_id',
            'pod_name',
            'container_id',
            'request_method',
            'request_uri',
            'request_uri_normalized',
            'request_full_uri',
            'request_path',
            'query_parameters',
            'query_params_count',
            'request_body_size',
            'http_protocol_version',
            'content_type',
            'accept_language',
            'accept_encoding',
            'response_status_code',
            'response_status_class',
            'response_body_size',
            'response_body_size_kb',
            'response_content_type',
            'response_content_encoding',
            'response_cache_control',
            'response_etag',
            'total_bytes_sent',
            'total_bytes_sent_kb',
            'bytes_received',
            'total_request_duration',
            'request_processing_time',
            'response_send_time',
            'upstream_connect_time',
            'upstream_header_time',
            'upstream_response_time',
            'backend_connect_phase',
            'backend_process_phase',
            'backend_transfer_phase',
            'nginx_transfer_phase',
            'backend_total_phase',
            'network_phase',
            'processing_phase',
            'transfer_phase',
            'response_transfer_speed',
            'total_transfer_speed',
            'nginx_transfer_speed',
            'backend_efficiency',
            'network_overhead',
            'transfer_ratio',
            'connection_cost_ratio',
            'processing_efficiency_index',
            'performance_score',
            'latency_percentile',
            'platform',
            'platform_version',
            'platform_category',
            'app_version',
            'app_build_number',
            'device_type',
            'device_model',
            'device_manufacturer',
            'screen_resolution',
            'browser_type',
            'browser_version',
            'browser_engine',
            'os_type',
            'os_version',
            'os_architecture',
            'sdk_type',
            'sdk_version',
            'integration_type',
            'framework_type',
            'framework_version',
            'access_entry_point',
            'entry_source',
            'entry_source_detail',
            'client_channel',
            'traffic_source',
            'campaign_id',
            'utm_source',
            'utm_medium',
            'utm_campaign',
            'utm_content',
            'utm_term',
            'referer_url',
            'referer_domain',
            'referer_domain_type',
            'search_engine',
            'search_keywords',
            'social_media',
            'social_media_type',
            'bot_type',
            'bot_name',
            'is_bot',
            'bot_probability',
            'crawler_category',
            'api_category',
            'api_subcategory',
            'api_module',
            'api_submodule',
            'api_version',
            'api_endpoint_type',
            'business_domain',
            'business_subdomain',
            'functional_area',
            'service_tier',
            'business_operation_type',
            'business_operation_subtype',
            'user_journey_stage',
            'user_session_stage',
            'transaction_type',
            'workflow_step',
            'process_stage',
            'access_type',
            'access_method',
            'client_category',
            'client_type',
            'client_classification',
            'integration_pattern',
            'user_id',
            'session_id',
            'user_type',
            'user_tier',
            'user_segment',
            'authentication_method',
            'authorization_level',
            'application_name',
            'application_version',
            'service_name',
            'service_version',
            'microservice_name',
            'service_mesh_name',
            'upstream_server',
            'upstream_service',
            'downstream_service',
            'trace_id',
            'span_id',
            'parent_span_id',
            'correlation_id',
            'request_id',
            'transaction_id',
            'business_transaction_id',
            'batch_id',
            'cache_status',
            'cache_layer',
            'cache_key',
            'cache_age',
            'cache_hit_ratio',
            'connection_requests',
            'connection_id',
            'connection_type',
            'ssl_session_reused',
            'business_sign',
            'feature_flag',
            'ab_test_group',
            'experiment_id',
            'custom_tags',
            'business_tags',
            'user_agent_string',
            'custom_headers',
            'security_headers',
            'cookie_count',
            'header_size',
            'log_source_file',
            'log_format_version',
            'log_level',
            'raw_log_entry',
            'error_code_group',
            'http_error_class',
            'error_severity_level',
            'error_category',
            'error_subcategory',
            'error_source',
            'error_propagation_path',
            'upstream_status_code',
            'error_correlation_id',
            'error_chain',
            'root_cause_analysis',
            'is_success',
            'is_business_success',
            'is_error',
            'is_client_error',
            'is_server_error',
            'is_retry',
            'has_anomaly',
            'is_slow',
            'is_very_slow',
            'perf_attention',
            'perf_warning',
            'perf_slow',
            'perf_very_slow',
            'perf_timeout',
            'performance_level',
            'anomaly_type',
            'anomaly_severity',
            'user_experience_level',
            'apdex_classification',
            'performance_rating',
            'sla_compliance',
            'sla_violation_type',
            'api_importance_level',
            'business_criticality',
            'business_value_score',
            'revenue_impact_level',
            'customer_impact_level',
            'data_quality_score',
            'data_completeness',
            'parsing_errors',
            'validation_errors',
            'enrichment_status',
            'security_risk_score',
            'security_risk_level',
            'threat_category',
            'attack_signature',
            'ip_reputation',
            'geo_anomaly',
            'access_pattern_anomaly',
            'rate_limit_hit',
            'blocked_by_waf',
            'fraud_score',
            'network_type',
            'ip_risk_level',
            'is_internal_ip',
            'is_tor_exit',
            'is_proxy',
            'is_vpn',
            'is_datacenter',
            'is_holiday',
            'created_at',
            'updated_at',
            'data_version',
            'last_processed_at',
            'processing_flags',
            'custom_dimensions',
            'custom_metrics',
            'metadata'
        ]

        # å†™å…¥ç»Ÿè®¡
        self.stats = {
            'total_records': 0,
            'success_records': 0,
            'failed_records': 0,
            'batch_count': 0
        }

    def connect(self) -> bool:
        """è¿æ¥ClickHouse - ç¡®ä¿æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹è¿æ¥"""
        try:
            import uuid
            import time

            # ä¸ºæ¯ä¸ªè¿æ¥åˆ›å»ºç‹¬ç«‹çš„é…ç½®ï¼Œé¿å…ä¼šè¯å†²çª
            connection_config = self.config.copy()
            # ä½¿ç”¨UUIDå’Œæ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€çš„ä¼šè¯ID
            unique_suffix = f"{uuid.uuid4().hex[:8]}_{int(time.time() * 1000000)}"
            thread_id = threading.current_thread().ident or 0
            connection_config['session_id'] = f"etl_session_{thread_id}_{unique_suffix}"
            connection_config['connect_timeout'] = 10
            connection_config['send_receive_timeout'] = 30

            self.client = clickhouse_connect.get_client(**connection_config)
            self.client.ping()
            self.logger.info(f"æˆåŠŸè¿æ¥åˆ°ClickHouse: {self.config['host']}:{self.config['port']} (ä¼šè¯: {connection_config['session_id'][:20]}...)")
            return True
        except Exception as e:
            self.logger.error(f"è¿æ¥ClickHouseå¤±è´¥: {e}")
            self.client = None
            return False

    def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            try:
                self.client.close()
                self.logger.info("ClickHouseè¿æ¥å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"å…³é—­è¿æ¥å¤±è´¥: {e}")
            finally:
                self.client = None

    def write_batch(self, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰¹é‡å†™å…¥æ•°æ®åˆ°DWDè¡¨"""
        # ä½¿ç”¨å†™å…¥é”é˜²æ­¢å¹¶å‘å†™å…¥åŒä¸€è¿æ¥
        with self._write_lock:
            if not self.client:
                if not self.connect():
                    return self._create_error_result("æ•°æ®åº“è¿æ¥å¤±è´¥")

            if not records:
                return self._create_success_result(0, "æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥")

            try:
                self.stats['batch_count'] += 1
                record_count_safe = self._safe_len(records, 1)
                self.logger.info(f"å¼€å§‹å†™å…¥æ‰¹æ¬¡ {self.stats['batch_count']}: {record_count_safe} æ¡è®°å½•")

                # å°†æ•°æ®è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                dict_data = []
                for record in records:
                    if not isinstance(record, dict):
                        self.logger.error(f"è®°å½•ä¸æ˜¯å­—å…¸ç±»å‹: {type(record)}")
                        continue

                    row_dict = {}
                    for field_name in self.dwd_fields:
                        value = record.get(field_name)
                        # ç‰¹åˆ«å¤„ç†ç©ºå­—ç¬¦ä¸²ï¼Œåº”å½“è§†ä¸ºNone
                        if value == '':
                            value = None
                        processed_value = self._process_field_value(field_name, value)
                        row_dict[field_name] = processed_value

                    dict_data.append(row_dict)

                if not dict_data:
                    return self._create_error_result("æ²¡æœ‰æœ‰æ•ˆçš„è®°å½•å¯å†™å…¥")

                dict_count_safe = self._safe_len(dict_data, 0)
                self.logger.info(f"å‡†å¤‡å†™å…¥ {dict_count_safe} æ¡è®°å½•åˆ°æ•°æ®åº“")

                # å°è¯•ä½¿ç”¨åˆ—è¡¨æ ¼å¼æ’å…¥
                table_name = f"{self.config['database']}.dwd_nginx_enriched_v3"

                # å°†å­—å…¸è½¬æ¢ä¸ºæŒ‰å­—æ®µé¡ºåºçš„åˆ—è¡¨
                list_data = []
                for record in dict_data:
                    row_list = []
                    for field_name in self.dwd_fields:
                        row_list.append(record[field_name])
                    list_data.append(row_list)

                result = self.client.insert(
                    table=table_name,
                    data=list_data
                    # ä¸æŒ‡å®šcolumn_namesï¼Œè®©ClickHouseä½¿ç”¨è¡¨çš„é»˜è®¤å­—æ®µé¡ºåº
                )

                # æ›´æ–°ç»Ÿè®¡
                success_count = dict_count_safe  # ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„å®‰å…¨è®¡æ•°
                self.stats['total_records'] += success_count
                self.stats['success_records'] += success_count

                self.logger.info(f"æ‰¹æ¬¡å†™å…¥æˆåŠŸ: {success_count} æ¡è®°å½•")
                return self._create_success_result(success_count, f"æˆåŠŸå†™å…¥ {success_count} æ¡è®°å½•")

            except ClickHouseError as e:
                self.logger.error(f"ClickHouseå†™å…¥é”™è¯¯: {e}")
                # ä½¿ç”¨å®‰å…¨çš„è®°å½•è®¡æ•°
                record_count = self._safe_len(records, 1)
                self.stats['failed_records'] += record_count
                return self._create_error_result(f"ClickHouseé”™è¯¯: {str(e)}")

            except Exception as e:
                self.logger.error(f"å†™å…¥å¼‚å¸¸: {e}")
                # ä½¿ç”¨å®‰å…¨çš„è®°å½•è®¡æ•°
                record_count = self._safe_len(records, 1)
                self.stats['failed_records'] += record_count
                return self._create_error_result(f"æœªçŸ¥é”™è¯¯: {str(e)}")

    def _process_field_value(self, field_name: str, value: Any) -> Any:
        """å¤„ç†å­—æ®µå€¼ï¼Œç¡®ä¿ç±»å‹åŒ¹é…"""
        if value is None:
            return self._get_default_value(field_name)

        # æ—¶é—´ç±»å‹å­—æ®µ - ç§»é™¤æ—¶åŒºä¿¡æ¯
        if field_name in ['log_time', 'last_processed_at', 'created_at', 'updated_at']:
            if isinstance(value, datetime):
                # ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼ŒClickHouseé€šå¸¸ä¸éœ€è¦æ—¶åŒº
                if value.tzinfo is not None:
                    return value.replace(tzinfo=None)
                return value
            return datetime.now()

        # DateTimeå­—æ®µ (date_hour, date_hour_minute)
        if field_name in ['date_hour', 'date_hour_minute']:
            if isinstance(value, datetime):
                return value.replace(tzinfo=None) if value.tzinfo else value
            return datetime.now().replace(second=0, microsecond=0) if field_name == 'date_hour_minute' else datetime.now().replace(minute=0, second=0, microsecond=0)

        # æ—¥æœŸç±»å‹å­—æ®µ
        if field_name in ['date_partition', 'date']:
            if hasattr(value, 'date'):
                return value.date()
            return datetime.now().date()

        # æ•´æ•°å­—æ®µ - æ”¹è¿›ç©ºå­—ç¬¦ä¸²å¤„ç†
        if field_name in ['hour_partition', 'minute_partition', 'second_partition', 'quarter_partition', 'week_partition',
                         'client_port', 'server_port', 'client_asn', 'query_params_count', 'request_body_size',
                         'response_body_size', 'total_bytes_sent', 'bytes_received', 'total_request_duration',
                         'request_processing_time', 'response_send_time', 'upstream_connect_time',
                         'upstream_header_time', 'upstream_response_time', 'backend_connect_phase',
                         'backend_process_phase', 'backend_transfer_phase', 'nginx_transfer_phase',
                         'backend_total_phase', 'network_phase', 'processing_phase', 'transfer_phase',
                         'cache_age', 'connection_requests', 'cookie_count', 'header_size',
                         'business_value_score', 'data_sensitivity', 'performance_level', 'security_risk_score',
                         'data_version']:
            if value is None or value == '' or (isinstance(value, str) and value.strip() == ''):
                return 0
            try:
                return int(float(value))  # å…ˆè½¬floatå†è½¬intï¼Œå¤„ç†"3.0"è¿™ç§æƒ…å†µ
            except (ValueError, TypeError):
                return 0

        # æµ®ç‚¹æ•°å­—æ®µ
        if field_name in ['response_body_size_kb', 'total_bytes_sent_kb', 'response_transfer_speed',
                         'total_transfer_speed', 'nginx_transfer_speed', 'backend_efficiency',
                         'network_overhead', 'transfer_ratio', 'connection_cost_ratio',
                         'processing_efficiency_index', 'performance_score', 'latency_percentile',
                         'cache_hit_ratio', 'bot_probability', 'fraud_score', 'data_quality_score',
                         'data_completeness']:
            return float(value) if value is not None else 0.0

        # å¸ƒå°”å­—æ®µ
        if field_name in ['is_success', 'is_business_success', 'is_slow', 'is_very_slow',
                         'perf_attention', 'perf_warning', 'perf_slow', 'perf_very_slow', 'perf_timeout',
                         'is_error', 'is_client_error', 'is_server_error', 'is_retry',
                         'has_anomaly', 'is_internal_ip', 'is_tor_exit', 'is_proxy', 'is_vpn',
                         'is_datacenter', 'is_bot', 'ssl_session_reused', 'geo_anomaly',
                         'access_pattern_anomaly', 'rate_limit_hit', 'blocked_by_waf',
                         'sla_compliance', 'is_holiday']:
            return bool(value) if value is not None else False

        # Mapç±»å‹å­—æ®µ
        if field_name in ['metadata', 'custom_dimensions', 'custom_metrics', 'custom_headers', 'security_headers']:
            if isinstance(value, dict):
                return value
            return {}

        # æ•°ç»„å­—æ®µ
        if field_name in ['parsing_errors', 'validation_errors', 'error_chain', 'custom_tags', 'business_tags', 'processing_flags']:
            if isinstance(value, list):
                return value
            return []

        # å­—ç¬¦ä¸²å­—æ®µï¼ˆé»˜è®¤ï¼‰
        return str(value) if value is not None else ''

    def _get_default_value(self, field_name: str) -> Any:
        """è·å–å­—æ®µçš„é»˜è®¤å€¼"""
        # æ—¶é—´ç±»å‹å­—æ®µ
        if field_name in ['log_time', 'last_processed_at', 'created_at', 'updated_at']:
            return datetime.now()
        elif field_name in ['date_partition', 'date']:
            return datetime.now().date()
        elif field_name in ['date_hour', 'date_hour_minute']:
            return datetime.now().replace(second=0, microsecond=0) if field_name == 'date_hour_minute' else datetime.now().replace(minute=0, second=0, microsecond=0)

        # å¸ƒå°”å­—æ®µ
        elif field_name.startswith('is_') or field_name.startswith('has_') or field_name.startswith('perf_') or field_name in ['ssl_session_reused', 'geo_anomaly', 'access_pattern_anomaly', 'rate_limit_hit', 'blocked_by_waf', 'sla_compliance', 'is_holiday']:
            return False

        # Mapç±»å‹å­—æ®µ
        elif field_name in ['metadata', 'custom_dimensions', 'custom_metrics', 'custom_headers', 'security_headers']:
            return {}

        # æ•°ç»„å­—æ®µ
        elif field_name in ['parsing_errors', 'validation_errors', 'error_chain', 'custom_tags', 'business_tags', 'processing_flags']:
            return []

        # æ•´æ•°å­—æ®µ (åŒ…å«æ–°å¢å­—æ®µ)
        elif field_name in ['id', 'ods_id', 'hour_partition', 'minute_partition', 'second_partition', 'quarter_partition', 'week_partition',
                           'client_port', 'server_port', 'client_asn', 'query_params_count', 'request_body_size',
                           'response_body_size', 'total_bytes_sent', 'bytes_received', 'total_request_duration',
                           'request_processing_time', 'response_send_time', 'upstream_connect_time',
                           'upstream_header_time', 'upstream_response_time', 'backend_connect_phase',
                           'backend_process_phase', 'backend_transfer_phase', 'nginx_transfer_phase',
                           'backend_total_phase', 'network_phase', 'processing_phase', 'transfer_phase',
                           'cache_age', 'connection_requests', 'cookie_count', 'header_size',
                           'business_value_score', 'data_sensitivity', 'performance_level', 'security_risk_score',
                           'data_version', 'hour', 'minute', 'second', 'quarter', 'week', 'day_of_year', 'weekday']:
            return 0

        # æµ®ç‚¹æ•°å­—æ®µ
        elif field_name in ['response_body_size_kb', 'total_bytes_sent_kb', 'response_transfer_speed',
                           'total_transfer_speed', 'nginx_transfer_speed', 'backend_efficiency',
                           'network_overhead', 'transfer_ratio', 'connection_cost_ratio',
                           'processing_efficiency_index', 'performance_score', 'latency_percentile',
                           'cache_hit_ratio', 'bot_probability', 'fraud_score', 'data_quality_score',
                           'data_completeness']:
            return 0.0

        # å­—ç¬¦ä¸²å­—æ®µï¼ˆé»˜è®¤ï¼‰
        else:
            return ''

    def test_connection(self) -> bool:
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            if not self.client:
                if not self.connect():
                    return False
            result = self.client.query("SELECT 1")
            return True
        except Exception as e:
            self.logger.error(f"è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å†™å…¥ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats['total_records'] > 0:
            stats['success_rate'] = (stats['success_records'] / stats['total_records']) * 100
        else:
            stats['success_rate'] = 0.0
        return stats

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_records': 0,
            'success_records': 0,
            'failed_records': 0,
            'batch_count': 0
        }

    def _create_success_result(self, count: int, message: str) -> Dict[str, Any]:
        """åˆ›å»ºæˆåŠŸç»“æœ"""
        return {
            'success': True,
            'count': count,
            'message': message,
            'timestamp': datetime.now()
        }

    def _create_error_result(self, error: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'success': False,
            'count': 0,
            'error': error,
            'timestamp': datetime.now()
        }

    def _safe_len(self, obj, default=1000):
        """å®‰å…¨çš„é•¿åº¦è·å–å‡½æ•°ï¼Œé˜²æ­¢ä»»ä½•len()ç›¸å…³é”™è¯¯"""
        try:
            if hasattr(obj, '__len__') and not isinstance(obj, (str, bytes)):
                return len(obj)
            else:
                return default
        except Exception:
            return default

    def batch_write_optimized(self, records: List[Dict[str, Any]]) -> bool:
        """
        ä¼˜åŒ–çš„æ‰¹é‡å†™å…¥æ–¹æ³• - ä½¿ç”¨ClickHouseæ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

        Args:
            records: è¦å†™å…¥çš„è®°å½•åˆ—è¡¨

        Returns:
            bool: å†™å…¥æ˜¯å¦æˆåŠŸ
        """
        if not self.client:
            if not self.connect():
                self.logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
                return False

        if not records:
            return True

        try:
            # ä¼˜åŒ–1: é¢„åˆ†é…æ•°ç»„å’Œæ‰¹é‡éªŒè¯
            list_data = []
            list_data_append = list_data.append  # å±€éƒ¨å˜é‡ä¼˜åŒ–

            # ä¼˜åŒ–2: ç¼“å­˜å­—æ®µé»˜è®¤å€¼
            field_defaults = {field: self._get_default_value(field) for field in self.dwd_fields}

            # ä¼˜åŒ–3: å¿«é€Ÿæ•°æ®è½¬æ¢ï¼ˆå‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€ï¼‰
            for record in records:
                if not isinstance(record, dict):
                    continue

                row_list = []
                row_list_append = row_list.append  # å±€éƒ¨å˜é‡ä¼˜åŒ–

                for field_name in self.dwd_fields:
                    value = record.get(field_name)
                    if value == '' or value is None:
                        value = field_defaults[field_name]
                    else:
                        value = self._process_field_value_fast(field_name, value)
                    row_list_append(value)

                list_data_append(row_list)

            if not list_data:
                return True

            # ä¼˜åŒ–4: ä½¿ç”¨ä¼˜åŒ–çš„ClickHouseé…ç½®å†™å…¥
            table_name = f"{self.config['database']}.dwd_nginx_enriched_v3"

            # ä¼˜åŒ–5: ä½¿ç”¨clientçš„é«˜æ€§èƒ½é…ç½®
            # ä½¿ç”¨å®‰å…¨é•¿åº¦å‡½æ•°å¹¶ç¡®ä¿æ˜¯æ•´æ•°
            data_length = self._safe_len(list_data)
            # ç¡®ä¿data_lengthæ˜¯æ•´æ•°å¹¶ä¸”åœ¨åˆç†èŒƒå›´å†…
            if not isinstance(data_length, int) or data_length < 1:
                data_length = 1000  # é»˜è®¤å®‰å…¨å€¼

            # ä½¿ç”¨å›ºå®šä¼˜åŒ–çš„æ•°å€¼ï¼Œé¿å…åœ¨settingsä¸­è¿›è¡Œå¤æ‚è®¡ç®—
            block_size = 8192 if data_length < 1000 else min(65536, data_length)
            insert_block_size = 1048576 if data_length < 10000 else min(10485760, data_length)

            # ç”±äºClickHouseå®¢æˆ·ç«¯åº“åœ¨æŸäº›ä¼˜åŒ–è®¾ç½®ä¸‹ä¼šå†…éƒ¨è°ƒç”¨len()å¯¼è‡´é”™è¯¯ï¼Œ
            # ç›´æ¥ä½¿ç”¨ç¨³å®šçš„write_batchæ–¹æ³•
            return self.write_batch(records)

            # æ›´æ–°ç»Ÿè®¡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            success_count = data_length  # ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„å®‰å…¨é•¿åº¦
            self.stats['total_records'] += success_count
            self.stats['success_records'] += success_count
            self.stats['batch_count'] += 1

            return True

        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–æ‰¹é‡å†™å…¥å¤±è´¥: {e}")

            # è¯¦ç»†é”™è¯¯è¯Šæ–­å·²å®Œæˆï¼Œç§»é™¤è°ƒè¯•ä»£ç 

            # ä½¿ç”¨å®‰å…¨çš„è®°å½•è®¡æ•°å¤„ç†
            try:
                record_count = self._safe_len(records, 1)
                if record_count == 1 and 'list_data' in locals():
                    record_count = self._safe_len(list_data, 1)
            except Exception as count_error:
                # æœ€åçš„å®‰å…¨ç½‘
                record_count = 1
                self.logger.error(f"è®°å½•è®¡æ•°å¤±è´¥: {count_error}")

            self.stats['failed_records'] += record_count
            return False

    def _process_field_value_fast(self, field_name: str, value: Any) -> Any:
        """å¿«é€Ÿå­—æ®µå€¼å¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘æ¡ä»¶åˆ¤æ–­"""
        # æ—¶é—´ç±»å‹å­—æ®µå¿«é€Ÿå¤„ç†
        if field_name in {'log_time', 'last_processed_at', 'created_at', 'updated_at'}:
            if isinstance(value, datetime):
                return value.replace(tzinfo=None) if value.tzinfo else value
            return datetime.now()

        # æ—¥æœŸç±»å‹å­—æ®µå¿«é€Ÿå¤„ç†
        if field_name in {'date_partition', 'date'}:
            if hasattr(value, 'date'):
                return value.date()
            return datetime.now().date()

        # æ•´æ•°å­—æ®µå¿«é€Ÿå¤„ç†
        if field_name.endswith(('_partition', '_count', '_size', '_code', '_port', 'duration')):
            try:
                return int(value) if value not in {None, '', 'null', 'NULL'} else 0
            except (ValueError, TypeError):
                return 0

        # æµ®ç‚¹æ•°å­—æ®µå¿«é€Ÿå¤„ç†
        if field_name.endswith(('_time', '_score', '_rate', '_ratio')):
            try:
                return float(value) if value not in {None, '', 'null', 'NULL'} else 0.0
            except (ValueError, TypeError):
                return 0.0

        # å­—ç¬¦ä¸²å­—æ®µï¼ˆé»˜è®¤ï¼‰
        return str(value) if value is not None else ''

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    writer = DWDWriter()

    # æµ‹è¯•è¿æ¥
    print("ğŸ”— æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    if writer.test_connection():
        print("âœ… è¿æ¥æˆåŠŸ")
    else:
        print("âŒ è¿æ¥å¤±è´¥")

    # å…³é—­è¿æ¥
    writer.close()