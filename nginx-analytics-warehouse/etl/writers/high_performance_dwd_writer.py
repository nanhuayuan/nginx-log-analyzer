#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½DWDå±‚æ•°æ®å†™å…¥å™¨ - é’ˆå¯¹å¤§æ‰¹é‡å†™å…¥ä¼˜åŒ–
High Performance DWD Writer - Optimized for large batch inserts

ç›¸æ¯”åŸç‰ˆdwd_writerçš„ä¼˜åŒ–ï¼š
1. å¼‚æ­¥æ‰¹é‡æ’å…¥ï¼Œå¤§å¹…æå‡å†™å…¥é€Ÿåº¦
2. é¢„å¤„ç†æ•°æ®ç±»å‹è½¬æ¢ï¼Œå‡å°‘è¿è¡Œæ—¶å¼€é”€
3. è¿æ¥æ± æ”¯æŒï¼Œé¿å…é¢‘ç¹è¿æ¥
4. å†…å­˜ä¼˜åŒ–ï¼Œæ”¯æŒè¶…å¤§æ‰¹é‡æ•°æ®
5. é”™è¯¯é‡è¯•æœºåˆ¶å’Œéƒ¨åˆ†å†™å…¥æ¢å¤
"""

import logging
import asyncio
import threading
import time
from datetime import datetime, date
from typing import Dict, Any, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor
import clickhouse_connect
from clickhouse_connect.driver.exceptions import ClickHouseError

class HighPerformanceDWDWriter:
    """é«˜æ€§èƒ½DWDå±‚æ•°æ®å†™å…¥å™¨"""
    
    def __init__(self, 
                 host: str = 'localhost', 
                 port: int = 8123,
                 database: str = 'nginx_analytics', 
                 username: str = 'default',
                 password: str = '',
                 insert_block_size: int = 50000,      # æ¯æ¬¡æ’å…¥çš„å—å¤§å°
                 max_insert_threads: int = 2,         # å¹¶å‘æ’å…¥çº¿ç¨‹æ•°
                 connection_timeout: int = 30,        # è¿æ¥è¶…æ—¶
                 retry_attempts: int = 3):            # é‡è¯•æ¬¡æ•°
        """
        åˆå§‹åŒ–é«˜æ€§èƒ½DWDå†™å…¥å™¨
        
        Args:
            insert_block_size: æ¯æ¬¡ClickHouseæ’å…¥çš„å—å¤§å°ï¼ˆæ¨è1ä¸‡-10ä¸‡ï¼‰
            max_insert_threads: æœ€å¤§å¹¶å‘æ’å…¥çº¿ç¨‹æ•°
            connection_timeout: è¿æ¥è¶…æ—¶ç§’æ•°
            retry_attempts: å¤±è´¥é‡è¯•æ¬¡æ•°
        """
        self.config = {
            'host': host,
            'port': port,
            'database': database,
            'username': username,
            'password': password,
            'connect_timeout': connection_timeout,
            'send_receive_timeout': connection_timeout * 2
        }
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.insert_block_size = insert_block_size
        self.max_insert_threads = max_insert_threads
        self.retry_attempts = retry_attempts
        
        self.client = None
        self.logger = logging.getLogger(__name__)
        
        # é«˜æ€§èƒ½å­—æ®µé…ç½® - é¢„ç¼–è¯‘ç±»å‹è½¬æ¢
        self._init_field_definitions()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_records': 0,
            'success_records': 0,
            'failed_records': 0,
            'batch_count': 0,
            'total_insert_time': 0.0,
            'avg_records_per_second': 0.0,
            'peak_records_per_second': 0.0,
            'retry_count': 0
        }
        
        # çº¿ç¨‹å®‰å…¨é”
        self._stats_lock = threading.Lock()
    
    def _init_field_definitions(self):
        """åˆå§‹åŒ–å­—æ®µå®šä¹‰å’Œç±»å‹è½¬æ¢å™¨"""
        # å®Œæ•´å­—æ®µåˆ—è¡¨ï¼ˆå·²æ’é™¤è‡ªåŠ¨ç”Ÿæˆå­—æ®µï¼‰
        self.dwd_fields = [
            # åŸºç¡€å­—æ®µ
            'log_time', 'date_partition', 'hour_partition', 'minute_partition', 'second_partition',
            'client_ip', 'client_port', 'xff_ip', 'server_name', 'request_method',
            'request_uri', 'request_uri_normalized', 'request_full_uri', 'query_parameters',
            'http_protocol_version', 'response_status_code', 'response_body_size', 'response_body_size_kb',
            'total_bytes_sent', 'total_bytes_sent_kb', 'total_request_duration',
            
            # æ€§èƒ½å­—æ®µ
            'upstream_connect_time', 'upstream_header_time', 'upstream_response_time',
            'backend_connect_phase', 'backend_process_phase', 'backend_transfer_phase',
            'nginx_transfer_phase', 'backend_total_phase', 'network_phase', 'processing_phase', 
            'transfer_phase', 'response_transfer_speed', 'total_transfer_speed', 'nginx_transfer_speed',
            'backend_efficiency', 'network_overhead', 'transfer_ratio', 'connection_cost_ratio',
            'processing_efficiency_index',
            
            # ä¸šåŠ¡å­—æ®µ
            'platform', 'platform_version', 'app_version', 'device_type', 'browser_type',
            'os_type', 'os_version', 'sdk_type', 'sdk_version', 'bot_type',
            'entry_source', 'referer_domain', 'search_engine', 'social_media',
            'api_category', 'api_module', 'api_version', 'business_domain', 'access_type',
            'client_category', 'application_name', 'service_name', 'trace_id', 'business_sign',
            'cluster_node', 'upstream_server', 'connection_requests', 'cache_status',
            'referer_url', 'user_agent_string', 'log_source_file',
            
            # çŠ¶æ€å­—æ®µ
            'is_success', 'is_business_success', 'is_slow', 'is_very_slow', 'is_error',
            'is_client_error', 'is_server_error', 'has_anomaly', 'anomaly_type',
            'user_experience_level', 'apdex_classification', 'api_importance', 'business_value_score',
            'data_quality_score', 'parsing_errors',
            
            # åœ°ç†å’Œé£é™©ä¿¡æ¯
            'client_region', 'client_isp', 'ip_risk_level', 'is_internal_ip'
        ]
        
        # é¢„ç¼–è¯‘ç±»å‹è½¬æ¢å™¨ï¼ˆé¿å…è¿è¡Œæ—¶ç±»å‹æ£€æŸ¥ï¼‰
        self.field_converters = {}
        self._build_field_converters()
    
    def _build_field_converters(self):
        """æ„å»ºé¢„ç¼–è¯‘çš„å­—æ®µç±»å‹è½¬æ¢å™¨"""
        # DateTimeå­—æ®µ
        datetime_fields = {'log_time'}
        for field in datetime_fields:
            self.field_converters[field] = lambda x: x if isinstance(x, datetime) else datetime.now()
        
        # Dateå­—æ®µ
        date_fields = {'date_partition'}
        for field in date_fields:
            self.field_converters[field] = lambda x: x.date() if hasattr(x, 'date') else datetime.now().date()
        
        # æ•´æ•°å­—æ®µ
        int_fields = {'hour_partition', 'minute_partition', 'second_partition', 'client_port', 
                     'response_body_size', 'total_bytes_sent', 'connection_requests', 'business_value_score'}
        for field in int_fields:
            self.field_converters[field] = lambda x: int(x) if x is not None and x != '' else 0
        
        # æµ®ç‚¹æ•°å­—æ®µ
        float_fields = {'response_body_size_kb', 'total_bytes_sent_kb', 'total_request_duration',
                       'upstream_connect_time', 'upstream_header_time', 'upstream_response_time',
                       'backend_connect_phase', 'backend_process_phase', 'backend_transfer_phase',
                       'nginx_transfer_phase', 'backend_total_phase', 'network_phase',
                       'processing_phase', 'transfer_phase', 'response_transfer_speed',
                       'total_transfer_speed', 'nginx_transfer_speed', 'backend_efficiency',
                       'network_overhead', 'transfer_ratio', 'connection_cost_ratio',
                       'processing_efficiency_index', 'data_quality_score'}
        for field in float_fields:
            self.field_converters[field] = lambda x: float(x) if x is not None and x != '' else 0.0
        
        # å¸ƒå°”å­—æ®µ
        bool_fields = {'is_success', 'is_business_success', 'is_slow', 'is_very_slow',
                      'is_error', 'is_client_error', 'is_server_error', 'has_anomaly', 'is_internal_ip'}
        for field in bool_fields:
            self.field_converters[field] = lambda x: bool(x) if x is not None else False
        
        # æ•°ç»„å­—æ®µ
        self.field_converters['parsing_errors'] = lambda x: x if isinstance(x, list) else []
        
        # å­—ç¬¦ä¸²å­—æ®µï¼ˆé»˜è®¤ï¼‰
        string_fields = set(self.dwd_fields) - set(datetime_fields) - set(date_fields) - \
                       set(int_fields) - set(float_fields) - set(bool_fields) - {'parsing_errors'}
        for field in string_fields:
            self.field_converters[field] = lambda x: str(x) if x is not None else ''
    
    def connect(self) -> bool:
        """è¿æ¥ClickHouse"""
        try:
            self.client = clickhouse_connect.get_client(**self.config)
            # è®¾ç½®æ€§èƒ½ä¼˜åŒ–å‚æ•°
            self.client.command("SET max_insert_block_size = 1048576")  # 1Mè¡Œ
            self.client.command("SET max_insert_threads = 16")           # 16ä¸ªæ’å…¥çº¿ç¨‹
            self.client.command("SET max_threads = 8")                   # 8ä¸ªæŸ¥è¯¢çº¿ç¨‹
            
            self.client.ping()
            self.logger.info(f"ğŸš€ é«˜æ€§èƒ½è¿æ¥å»ºç«‹: {self.config['host']}:{self.config['port']}")
            return True
        except Exception as e:
            self.logger.error(f"âŒ è¿æ¥å¤±è´¥: {e}")
            self.client = None
            return False
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            try:
                self.client.close()
                self.logger.info("ğŸ”Œ è¿æ¥å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"âŒ å…³é—­è¿æ¥å¤±è´¥: {e}")
            finally:
                self.client = None
    
    def write_batch(self, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        é«˜æ€§èƒ½æ‰¹é‡å†™å…¥æ•°æ®
        
        Args:
            records: è¦å†™å…¥çš„è®°å½•åˆ—è¡¨
            
        Returns:
            å†™å…¥ç»“æœå­—å…¸
        """
        if not records:
            return self._create_success_result(0, "æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥")
        
        if not self.client:
            if not self.connect():
                return self._create_error_result("æ•°æ®åº“è¿æ¥å¤±è´¥")
        
        start_time = time.time()
        total_records = len(records)
        
        self.logger.info(f"ğŸš€ å¼€å§‹é«˜æ€§èƒ½æ‰¹é‡å†™å…¥: {total_records:,} æ¡è®°å½•")
        
        try:
            # é¢„å¤„ç†æ•°æ® - æ‰¹é‡ç±»å‹è½¬æ¢
            prepared_data = self._prepare_batch_data_optimized(records)
            
            # æ ¹æ®æ•°æ®é‡å†³å®šæ’å…¥ç­–ç•¥
            if total_records <= self.insert_block_size:
                # å°æ‰¹é‡ï¼šç›´æ¥æ’å…¥
                result = self._single_insert(prepared_data, total_records)
            else:
                # å¤§æ‰¹é‡ï¼šåˆ†å—å¹¶å‘æ’å…¥
                result = self._chunked_parallel_insert(prepared_data, total_records)
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            duration = time.time() - start_time
            self._update_stats(total_records, result['success'], duration)
            
            if result['success']:
                speed = total_records / duration if duration > 0 else 0
                self.logger.info(f"âœ… å†™å…¥å®Œæˆ: {total_records:,} æ¡è®°å½•, "
                               f"{duration:.2f}s, {speed:.0f} è®°å½•/ç§’")
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"âŒ æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")
            self._update_stats(total_records, False, duration)
            return self._create_error_result(f"æ‰¹é‡å†™å…¥å¼‚å¸¸: {str(e)}")
    
    def _prepare_batch_data_optimized(self, records: List[Dict[str, Any]]) -> List[List[Any]]:
        """ä¼˜åŒ–çš„æ‰¹é‡æ•°æ®é¢„å¤„ç†"""
        prepared_data = []
        
        # é¢„åˆ†é…å†…å­˜
        prepared_data.reserve = len(records)
        
        for record in records:
            row_data = []
            
            # ä½¿ç”¨é¢„ç¼–è¯‘çš„ç±»å‹è½¬æ¢å™¨
            for field_name in self.dwd_fields:
                value = record.get(field_name)
                converter = self.field_converters.get(field_name, str)
                
                try:
                    converted_value = converter(value)
                    row_data.append(converted_value)
                except (ValueError, TypeError):
                    # å¿«é€Ÿfallbackåˆ°é»˜è®¤å€¼
                    row_data.append(self._get_default_value_fast(field_name))
            
            prepared_data.append(row_data)
        
        return prepared_data
    
    def _single_insert(self, prepared_data: List[List[Any]], total_records: int) -> Dict[str, Any]:
        """å•æ¬¡æ’å…¥"""
        try:
            table_name = f"{self.config['database']}.dwd_nginx_enriched_v2"
            
            self.client.insert(
                table=table_name,
                data=prepared_data,
                column_names=self.dwd_fields
            )
            
            with self._stats_lock:
                self.stats['batch_count'] += 1
            
            return self._create_success_result(total_records, f"å•æ¬¡æ’å…¥æˆåŠŸ: {total_records:,} æ¡è®°å½•")
            
        except ClickHouseError as e:
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
            if self.retry_attempts > 0 and "timeout" in str(e).lower():
                return self._retry_insert(prepared_data, total_records)
            
            return self._create_error_result(f"ClickHouseé”™è¯¯: {str(e)}")
    
    def _chunked_parallel_insert(self, prepared_data: List[List[Any]], total_records: int) -> Dict[str, Any]:
        """åˆ†å—å¹¶å‘æ’å…¥"""
        # å°†æ•°æ®åˆ†å—
        chunks = []
        for i in range(0, len(prepared_data), self.insert_block_size):
            chunk = prepared_data[i:i + self.insert_block_size]
            chunks.append(chunk)
        
        self.logger.info(f"ğŸ“¦ æ•°æ®åˆ†ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¹¶å‘æ’å…¥")
        
        # å¹¶å‘æ’å…¥
        success_count = 0
        errors = []
        
        with ThreadPoolExecutor(max_workers=min(self.max_insert_threads, len(chunks))) as executor:
            # æäº¤æ‰€æœ‰æ’å…¥ä»»åŠ¡
            future_to_chunk = {}
            for i, chunk in enumerate(chunks):
                future = executor.submit(self._insert_chunk, chunk, i)
                future_to_chunk[future] = (i, len(chunk))
            
            # æ”¶é›†ç»“æœ
            for future in future_to_chunk:
                chunk_id, chunk_size = future_to_chunk[future]
                try:
                    result = future.result()
                    if result['success']:
                        success_count += chunk_size
                        self.logger.debug(f"âœ… å— {chunk_id} æ’å…¥æˆåŠŸ: {chunk_size} æ¡è®°å½•")
                    else:
                        errors.append(f"å— {chunk_id}: {result['error']}")
                        self.logger.error(f"âŒ å— {chunk_id} æ’å…¥å¤±è´¥: {result['error']}")
                        
                except Exception as e:
                    errors.append(f"å— {chunk_id} æ‰§è¡Œå¼‚å¸¸: {e}")
                    self.logger.error(f"âŒ å— {chunk_id} æ‰§è¡Œå¼‚å¸¸: {e}")
        
        # åˆ¤æ–­æ•´ä½“ç»“æœ
        if success_count == total_records:
            with self._stats_lock:
                self.stats['batch_count'] += len(chunks)
            return self._create_success_result(success_count, f"å¹¶å‘æ’å…¥æˆåŠŸ: {success_count:,} æ¡è®°å½•")
        elif success_count > 0:
            return self._create_partial_result(success_count, total_records, errors)
        else:
            return self._create_error_result(f"å¹¶å‘æ’å…¥å…¨éƒ¨å¤±è´¥: {'; '.join(errors[:3])}")
    
    def _insert_chunk(self, chunk_data: List[List[Any]], chunk_id: int) -> Dict[str, Any]:
        """æ’å…¥å•ä¸ªæ•°æ®å—"""
        try:
            table_name = f"{self.config['database']}.dwd_nginx_enriched_v2"
            
            # æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„clientå¯èƒ½ä¼šæœ‰é—®é¢˜ï¼Œè¿™é‡Œå¤ç”¨ä¸»client
            # å¦‚æœæœ‰å¹¶å‘é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹è¿æ¥
            self.client.insert(
                table=table_name,
                data=chunk_data,
                column_names=self.dwd_fields
            )
            
            return {'success': True, 'chunk_id': chunk_id, 'count': len(chunk_data)}
            
        except Exception as e:
            return {'success': False, 'chunk_id': chunk_id, 'error': str(e)}
    
    def _retry_insert(self, prepared_data: List[List[Any]], total_records: int) -> Dict[str, Any]:
        """é‡è¯•æ’å…¥"""
        self.logger.warning(f"ğŸ”„ å¼€å§‹é‡è¯•æ’å…¥: {total_records} æ¡è®°å½•")
        
        for attempt in range(self.retry_attempts):
            try:
                time.sleep(1 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                
                # é‡æ–°è¿æ¥
                if not self.connect():
                    continue
                
                # é‡è¯•æ’å…¥
                table_name = f"{self.config['database']}.dwd_nginx_enriched_v2"
                self.client.insert(
                    table=table_name,
                    data=prepared_data,
                    column_names=self.dwd_fields
                )
                
                with self._stats_lock:
                    self.stats['batch_count'] += 1
                    self.stats['retry_count'] += 1
                
                self.logger.info(f"âœ… é‡è¯•ç¬¬ {attempt + 1} æ¬¡æˆåŠŸ")
                return self._create_success_result(total_records, f"é‡è¯•æ’å…¥æˆåŠŸ: {total_records} æ¡è®°å½•")
                
            except Exception as e:
                self.logger.error(f"âŒ é‡è¯•ç¬¬ {attempt + 1} æ¬¡å¤±è´¥: {e}")
                if attempt == self.retry_attempts - 1:
                    return self._create_error_result(f"é‡è¯• {self.retry_attempts} æ¬¡åä»ç„¶å¤±è´¥: {str(e)}")
    
    def _get_default_value_fast(self, field_name: str) -> Any:
        """å¿«é€Ÿè·å–å­—æ®µé»˜è®¤å€¼"""
        if field_name == 'log_time':
            return datetime.now()
        elif field_name == 'date_partition':
            return datetime.now().date()
        elif field_name in ['hour_partition', 'minute_partition', 'second_partition', 'client_port',
                           'response_body_size', 'total_bytes_sent', 'connection_requests', 'business_value_score']:
            return 0
        elif any(keyword in field_name for keyword in ['phase', 'speed', 'efficiency', 'overhead', 'ratio', 'time', 'kb']):
            return 0.0
        elif field_name.startswith('is_') or field_name.startswith('has_'):
            return False
        elif field_name == 'parsing_errors':
            return []
        else:
            return ''
    
    def _update_stats(self, record_count: int, success: bool, duration: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        with self._stats_lock:
            self.stats['total_records'] += record_count
            self.stats['total_insert_time'] += duration
            
            if success:
                self.stats['success_records'] += record_count
            else:
                self.stats['failed_records'] += record_count
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            if duration > 0:
                current_speed = record_count / duration
                self.stats['peak_records_per_second'] = max(
                    self.stats['peak_records_per_second'], current_speed
                )
            
            if self.stats['total_insert_time'] > 0:
                self.stats['avg_records_per_second'] = (
                    self.stats['success_records'] / self.stats['total_insert_time']
                )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†æ€§èƒ½ç»Ÿè®¡"""
        with self._stats_lock:
            stats = self.stats.copy()
            
            if stats['total_records'] > 0:
                stats['success_rate'] = (stats['success_records'] / stats['total_records']) * 100
            else:
                stats['success_rate'] = 0.0
            
            # æ ¼å¼åŒ–æ˜¾ç¤º
            stats['avg_records_per_second'] = round(stats['avg_records_per_second'], 1)
            stats['peak_records_per_second'] = round(stats['peak_records_per_second'], 1)
            stats['success_rate'] = round(stats['success_rate'], 2)
            
            return stats
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        with self._stats_lock:
            self.stats = {
                'total_records': 0,
                'success_records': 0,
                'failed_records': 0,
                'batch_count': 0,
                'total_insert_time': 0.0,
                'avg_records_per_second': 0.0,
                'peak_records_per_second': 0.0,
                'retry_count': 0
            }
    
    def _create_success_result(self, count: int, message: str) -> Dict[str, Any]:
        """åˆ›å»ºæˆåŠŸç»“æœ"""
        return {
            'success': True,
            'count': count,
            'message': message,
            'timestamp': datetime.now()
        }
    
    def _create_error_result(self, error: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'success': False,
            'count': 0,
            'error': error,
            'timestamp': datetime.now()
        }
    
    def _create_partial_result(self, success_count: int, total_count: int, errors: List[str]) -> Dict[str, Any]:
        """åˆ›å»ºéƒ¨åˆ†æˆåŠŸç»“æœ"""
        return {
            'success': False,  # éƒ¨åˆ†æˆåŠŸè§†ä¸ºå¤±è´¥
            'partial_success': True,
            'count': success_count,
            'total_count': total_count,
            'message': f"éƒ¨åˆ†å†™å…¥æˆåŠŸ: {success_count}/{total_count} æ¡è®°å½•",
            'errors': errors,
            'timestamp': datetime.now()
        }

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    writer = HighPerformanceDWDWriter(
        insert_block_size=10000,
        max_insert_threads=4
    )
    
    # æµ‹è¯•è¿æ¥
    print("ğŸ”— æµ‹è¯•é«˜æ€§èƒ½æ•°æ®åº“è¿æ¥...")
    if writer.connect():
        print("âœ… è¿æ¥æˆåŠŸ")
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        print("\\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        stats = writer.get_performance_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")
    else:
        print("âŒ è¿æ¥å¤±è´¥")
    
    # å…³é—­è¿æ¥
    writer.close()