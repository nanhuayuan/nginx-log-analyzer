#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆè¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å®Œå…¨é›†æˆåŸæœ‰åŠŸèƒ½ + è‡ªåŠ¨æ–‡ä»¶å‘ç°
Integrated Ultra Performance ETL Controller with Auto Discovery

æ ¸å¿ƒæ”¹è¿›ï¼š
1. å®Œå…¨é›†æˆåŸæœ‰äº¤äº’å¼æµç¨‹å’ŒçŠ¶æ€ç®¡ç†
2. è§£å†³æ€§èƒ½è¡°å‡é—®é¢˜ (300->140 RPS)
3. ä¼˜åŒ–è¿›åº¦åˆ·æ–°é¢‘ç‡ (1-5åˆ†é’Ÿå¯é…ç½®)
4. ä¿ç•™æ‰€æœ‰åŸæœ‰åŠŸèƒ½ç‰¹æ€§
5. å¢å¼ºç¼“å­˜ç®¡ç†é˜²æ­¢å†…å­˜è†¨èƒ€
6. æ–°å¢è‡ªåŠ¨æ–‡ä»¶å‘ç°å’Œç›‘æ§åŠŸèƒ½
"""

import sys
import os
import json
import time
import argparse
import threading
import queue
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, deque
import gc
import traceback
import logging

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
current_dir = Path(__file__).parent
etl_root = current_dir.parent
sys.path.append(str(etl_root))

from parsers.base_log_parser import BaseLogParser
from processors.field_mapper import FieldMapper
from writers.dwd_writer import DWDWriter

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

class AutoFileDiscovery:
    """è‡ªåŠ¨æ–‡ä»¶å‘ç°å™¨"""

    def __init__(self, log_dir: Path, scan_interval: int = 180):
        self.log_dir = log_dir
        self.scan_interval = scan_interval  # é»˜è®¤3åˆ†é’Ÿ
        self.last_scan_time = 0
        self.known_files = set()
        self.logger = logging.getLogger(__name__)

    def discover_new_files(self) -> List[Path]:
        """å‘ç°æ–°å¢çš„æ—¥å¿—æ–‡ä»¶"""
        current_time = time.time()
        if current_time - self.last_scan_time < self.scan_interval:
            return []

        new_files = []
        try:
            if not self.log_dir.exists():
                return []

            # æ‰«ææ‰€æœ‰æ—¥æœŸç›®å½•ä¸‹çš„.logæ–‡ä»¶
            for date_dir in self.log_dir.iterdir():
                if date_dir.is_dir() and date_dir.name.isdigit() and len(date_dir.name) == 8:
                    try:
                        datetime.strptime(date_dir.name, '%Y%m%d')
                        for log_file in date_dir.glob("*.log"):
                            if log_file not in self.known_files:
                                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ç¨³å®šï¼ˆæœ€åä¿®æ”¹æ—¶é—´è¶…è¿‡30ç§’ï¼‰
                                if time.time() - log_file.stat().st_mtime > 30:
                                    new_files.append(log_file)
                                    self.known_files.add(log_file)
                    except (ValueError, OSError):
                        continue

            self.last_scan_time = current_time
            if new_files:
                self.logger.info(f"å‘ç° {len(new_files)} ä¸ªæ–°æ—¥å¿—æ–‡ä»¶")

        except Exception as e:
            self.logger.error(f"æ–‡ä»¶å‘ç°è¿‡ç¨‹å‡ºé”™: {e}")

        return new_files

    def initialize_known_files(self, existing_files: set = None):
        """åˆå§‹åŒ–å·²çŸ¥æ–‡ä»¶åˆ—è¡¨"""
        try:
            if existing_files:
                self.known_files.update(existing_files)

            if not self.log_dir.exists():
                return

            for date_dir in self.log_dir.iterdir():
                if date_dir.is_dir() and date_dir.name.isdigit() and len(date_dir.name) == 8:
                    try:
                        datetime.strptime(date_dir.name, '%Y%m%d')
                        for log_file in date_dir.glob("*.log"):
                            self.known_files.add(log_file)
                    except (ValueError, OSError):
                        continue

            self.logger.info(f"åˆå§‹åŒ–å·²çŸ¥æ–‡ä»¶åˆ—è¡¨ï¼Œå…± {len(self.known_files)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–å·²çŸ¥æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨ - è§£å†³æ€§èƒ½è¡°å‡é—®é¢˜"""

    def __init__(self):
        self.performance_history = deque(maxlen=50)
        self.cache_optimization_interval = 300  # 5åˆ†é’Ÿ
        self.gc_optimization_interval = 180     # 3åˆ†é’Ÿ
        self.last_cache_cleanup = time.time()
        self.last_gc_cleanup = time.time()

    def monitor_performance(self, current_speed: float):
        """ç›‘æ§æ€§èƒ½å¹¶è®°å½•å†å²"""
        self.performance_history.append({
            'timestamp': time.time(),
            'speed': current_speed
        })

    def should_optimize_cache(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç¼“å­˜ä¼˜åŒ–"""
        return time.time() - self.last_cache_cleanup > self.cache_optimization_interval

    def should_optimize_gc(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦GCä¼˜åŒ–"""
        return time.time() - self.last_gc_cleanup > self.gc_optimization_interval

    def optimize_cache(self, cache_dict: dict, max_size: int = 5000):
        """ä¼˜åŒ–ç¼“å­˜å¤§å°"""
        if len(cache_dict) > max_size:
            # ä¿ç•™æœ€è¿‘ä½¿ç”¨çš„ä¸€åŠ
            items = list(cache_dict.items())
            keep_count = max_size // 2
            cache_dict.clear()
            cache_dict.update(items[-keep_count:])

        self.last_cache_cleanup = time.time()
        return len(cache_dict)

    def optimize_gc(self) -> int:
        """æ‰§è¡Œåƒåœ¾å›æ”¶ä¼˜åŒ–"""
        collected = gc.collect()
        self.last_gc_cleanup = time.time()
        return collected

    def get_performance_trend(self) -> str:
        """è·å–æ€§èƒ½è¶‹åŠ¿"""
        if len(self.performance_history) < 10:
            return "insufficient_data"

        recent_speeds = [h['speed'] for h in list(self.performance_history)[-10:]]
        early_speeds = [h['speed'] for h in list(self.performance_history)[:10]]

        recent_avg = sum(recent_speeds) / len(recent_speeds)
        early_avg = sum(early_speeds) / len(early_speeds)

        if recent_avg < early_avg * 0.8:
            return "declining"
        elif recent_avg > early_avg * 1.1:
            return "improving"
        else:
            return "stable"

class IntegratedProgressTracker:
    """é›†æˆè¿›åº¦è¿½è¸ªå™¨ - å®Œå…¨å…¼å®¹åŸæœ‰çŠ¶æ€ç®¡ç†"""

    def __init__(self, logger, refresh_interval_minutes: int = 3):
        self.logger = logger
        self.refresh_interval = refresh_interval_minutes * 60  # è½¬æ¢ä¸ºç§’
        self.last_display_time = 0

        # å…¨å±€ç»Ÿè®¡ (å…¼å®¹åŸæœ‰æ ¼å¼)
        self.session_stats = {
            'start_time': None,
            'end_time': None,
            'total_files_processed': 0,
            'total_lines_processed': 0,
            'total_records_written': 0,
            'total_errors': 0,
            'cache_hit_rate': 0.0,
            'avg_processing_speed': 0.0,
            'processing_errors': [],
            'error_stats': {
                'parsing_errors': 0,
                'mapping_errors': 0,
                'database_errors': 0,
                'critical_errors': 0
            },
            'detailed_error_log': []
        }

        # å®æ—¶ç»Ÿè®¡ (æ–°å¢)
        self.real_time_stats = {
            'total_written_records': 0,
            'total_written_batches': 0,
            'last_batch_time': time.time()
        }

        # å½“å‰å¤„ç†çŠ¶æ€
        self.current_files = {}
        self.completed_files = []
        self.performance_samples = deque(maxlen=100)

        # æ–‡ä»¶çº§åˆ«è¿›åº¦ (å…¼å®¹åŸæœ‰æ ¼å¼)
        self.file_progress = {}

    def start_session(self, total_files: int = 0):
        """å¼€å§‹å¤„ç†ä¼šè¯"""
        self.session_stats['start_time'] = time.time()
        self.session_stats['total_files_to_process'] = total_files
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†ä¼šè¯ï¼Œé¢„è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶")

    def start_file_processing(self, thread_id: int, file_path: Path, estimated_lines: int = None):
        """å¼€å§‹æ–‡ä»¶å¤„ç†"""
        self.current_files[thread_id] = {
            'file_path': file_path,
            'start_time': time.time(),
            'processed_lines': 0,
            'processed_records': 0,
            'estimated_lines': estimated_lines or 10000,
            'errors': 0
        }

        self.file_progress[str(file_path)] = {
            'status': 'processing',
            'start_time': time.time(),
            'thread_id': thread_id,
            'processed_lines': 0,
            'estimated_lines': estimated_lines or 10000
        }

    def update_file_progress(self, thread_id: int, lines_processed: int, records_processed: int, errors: int = 0):
        """æ›´æ–°æ–‡ä»¶å¤„ç†è¿›åº¦"""
        if thread_id in self.current_files:
            file_info = self.current_files[thread_id]
            file_info['processed_lines'] = lines_processed
            file_info['processed_records'] = records_processed
            file_info['errors'] = errors

            # è®¡ç®—å½“å‰é€Ÿåº¦
            elapsed = time.time() - file_info['start_time']
            if elapsed > 0:
                current_speed = records_processed / elapsed
                self.performance_samples.append(current_speed)

            # æ›´æ–°æ–‡ä»¶çº§åˆ«è¿›åº¦
            file_path = str(file_info['file_path'])
            if file_path in self.file_progress:
                self.file_progress[file_path]['processed_lines'] = lines_processed

    def update_batch_written(self, records_count: int):
        """æ›´æ–°æ‰¹æ¬¡å†™å…¥ç»Ÿè®¡"""
        self.real_time_stats['total_written_records'] += records_count
        self.real_time_stats['total_written_batches'] += 1
        self.real_time_stats['last_batch_time'] = time.time()

    def complete_file_processing(self, thread_id: int, final_records: int, final_errors: int):
        """å®Œæˆæ–‡ä»¶å¤„ç†"""
        if thread_id in self.current_files:
            file_info = self.current_files[thread_id]
            processing_time = time.time() - file_info['start_time']

            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            self.session_stats['total_files_processed'] += 1
            self.session_stats['total_lines_processed'] += file_info['processed_lines']
            self.session_stats['total_records_written'] += final_records
            self.session_stats['total_errors'] += final_errors

            # è®°å½•å®Œæˆçš„æ–‡ä»¶
            completed_file = {
                'file_path': str(file_info['file_path']),
                'processing_time': processing_time,
                'records_processed': final_records,
                'errors': final_errors,
                'speed': final_records / processing_time if processing_time > 0 else 0
            }
            self.completed_files.append(completed_file)

            # æ›´æ–°æ–‡ä»¶è¿›åº¦çŠ¶æ€
            file_path = str(file_info['file_path'])
            if file_path in self.file_progress:
                self.file_progress[file_path]['status'] = 'completed'
                self.file_progress[file_path]['end_time'] = time.time()
                self.file_progress[file_path]['records_processed'] = final_records

            # æ¸…ç†å½“å‰å¤„ç†çŠ¶æ€
            del self.current_files[thread_id]

    def should_display_progress(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ˜¾ç¤ºè¿›åº¦"""
        current_time = time.time()
        return current_time - self.last_display_time >= self.refresh_interval

    def display_progress(self, force: bool = False):
        """æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯"""
        current_time = time.time()

        if not force and not self.should_display_progress():
            return

        self.last_display_time = current_time

        if not self.session_stats['start_time']:
            return

        elapsed = current_time - self.session_stats['start_time']

        # è®¡ç®—å¹³å‡é€Ÿåº¦ - ä½¿ç”¨å®æ—¶ç»Ÿè®¡
        total_records = self.real_time_stats['total_written_records']
        if elapsed > 0:
            self.session_stats['avg_processing_speed'] = total_records / elapsed

        # è®¡ç®—å½“å‰é€Ÿåº¦ - åŸºäºæœ€è¿‘çš„æ‰¹æ¬¡å†™å…¥
        if self.performance_samples:
            # ä½¿ç”¨æœ€è¿‘å‡ ä¸ªæ ·æœ¬çš„å¹³å‡å€¼
            recent_samples = list(self.performance_samples)[-5:]  # æœ€è¿‘5ä¸ªæ ·æœ¬
            current_speed = sum(recent_samples) / len(recent_samples)
        else:
            # å¦‚æœæ²¡æœ‰æ€§èƒ½æ ·æœ¬ï¼ŒåŸºäºå½“å‰æ­£åœ¨å¤„ç†çš„è®°å½•è®¡ç®—
            current_processing_records = sum(f['processed_records'] for f in self.current_files.values())
            if elapsed > 0 and current_processing_records > 0:
                current_speed = current_processing_records / elapsed
            else:
                current_speed = 0

        print("\\n" + "=" * 80)
        print("ğŸš€ ETLå¤„ç†è¿›åº¦æŠ¥å‘Š")
        print("=" * 80)
        print(f"ğŸ“ å·²å®Œæˆæ–‡ä»¶: {self.session_stats['total_files_processed']} ä¸ª")
        print(f"ğŸ“Š å¤„ç†è®°å½•æ•°: {total_records:,} æ¡")
        print(f"âŒ é”™è¯¯è®°å½•æ•°: {self.session_stats['total_errors']:,} æ¡")
        print(f"â±ï¸ è¿è¡Œæ—¶é—´: {str(timedelta(seconds=int(elapsed)))}")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {self.session_stats['avg_processing_speed']:.0f} RPS")
        print(f"ğŸ“ˆ å½“å‰é€Ÿåº¦: {current_speed:.0f} RPS")

        # æ˜¾ç¤ºå½“å‰å¤„ç†æ–‡ä»¶
        if self.current_files:
            print("\\nğŸ“‚ å½“å‰å¤„ç†æ–‡ä»¶:")
            for tid, file_info in self.current_files.items():
                progress = file_info['processed_lines'] / file_info['estimated_lines']
                progress_bar = "â–ˆ" * int(progress * 20) + "â–‘" * (20 - int(progress * 20))
                print(f"  çº¿ç¨‹{tid}: {file_info['file_path'].name}")
                print(f"           [{progress_bar}] {progress*100:.1f}% ({file_info['processed_records']} æ¡)")

        print("=" * 80)

    def get_session_summary(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯æ€»ç»“"""
        if self.session_stats['start_time']:
            elapsed = time.time() - self.session_stats['start_time']
            self.session_stats['avg_processing_speed'] = self.session_stats['total_records_written'] / elapsed if elapsed > 0 else 0

        return self.session_stats.copy()

class IntegratedUltraETLController:
    """é›†æˆè¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å®Œå…¨å…¼å®¹åŸæœ‰åŠŸèƒ½"""

    def __init__(self,
                 base_log_dir: str = None,
                 state_file: str = None,
                 batch_size: int = 2000,
                 max_workers: int = 4,
                 connection_pool_size: int = None,
                 memory_limit_mb: int = 512,
                 enable_detailed_logging: bool = True,
                 progress_refresh_minutes: int = 3):
        """
        åˆå§‹åŒ–é›†æˆè¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨

        Args:
            progress_refresh_minutes: è¿›åº¦åˆ·æ–°é—´éš”(åˆ†é’Ÿ) 1-5åˆ†é’Ÿ
        """
        # åŸºç¡€é…ç½® (å®Œå…¨å…¼å®¹åŸæœ‰)
        self.base_log_dir = Path(base_log_dir) if base_log_dir else \
            Path("D:/project/nginx-log-analyzer/nginx-analytics-warehouse/nginx_logs")
        self.state_file = Path(state_file) if state_file else \
            Path(etl_root / "processed_logs_state.json")

        # æ€§èƒ½é…ç½®
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.connection_pool_size = connection_pool_size if connection_pool_size is not None else max_workers
        self.memory_limit_mb = memory_limit_mb
        self.enable_detailed_logging = enable_detailed_logging

        # æ—¥å¿—é…ç½®
        self.logger = self._setup_logger()

        # é›†æˆç»„ä»¶
        self.progress_tracker = IntegratedProgressTracker(self.logger, progress_refresh_minutes)
        self.performance_optimizer = PerformanceOptimizer()

        # çº¿ç¨‹å®‰å…¨çš„ç»„ä»¶æ±  (åŸæœ‰è®¾è®¡)
        self.parser_pool = [BaseLogParser() for _ in range(max_workers)]
        self.mapper_pool = [FieldMapper() for _ in range(max_workers)]
        self.writer_pool = []

        # ä¼˜åŒ–çš„ç¼“å­˜æœºåˆ¶ (è§£å†³æ€§èƒ½è¡°å‡)
        self.ua_cache = {}
        self.uri_cache = {}
        self.cache_stats = {'ua_hits': 0, 'ua_misses': 0, 'uri_hits': 0, 'uri_misses': 0}

        # å¤„ç†çŠ¶æ€ (å…¼å®¹åŸæœ‰æ ¼å¼)
        self.processed_state = self.load_state()

        # å¼‚æ­¥å†™å…¥ä¼˜åŒ– - å¢åŠ ç¼“å†²åŒºå¤§å°
        self.write_buffer = []
        self.write_buffer_lock = threading.Lock()
        self.async_write_queue = queue.Queue(maxsize=500)  # å¢åŠ é˜Ÿåˆ—å¤§å°
        self.buffer_flush_threshold = batch_size * 2  # åŠ¨æ€åˆ·æ–°é˜ˆå€¼

        # æ€§èƒ½ç»Ÿè®¡ (å…¼å®¹åŸæœ‰)
        self.write_stats = {
            'total_writes': 0,
            'total_records': 0,
            'total_write_time': 0,
            'buffer_flushes': 0
        }

        # åˆå§‹åŒ–ç³»ç»Ÿ
        self._init_connection_pool()

        # è‡ªåŠ¨æ–‡ä»¶å‘ç°åŠŸèƒ½ (æ–°å¢)
        self.auto_discovery = AutoFileDiscovery(self.base_log_dir)
        self.monitoring_enabled = False
        self.monitoring_thread = None
        self.stop_monitoring = threading.Event()
        self.is_processing = False
        self.processing_lock = threading.Lock()

        # åˆå§‹åŒ–å·²çŸ¥æ–‡ä»¶åˆ—è¡¨
        existing_files = set(Path(f) for f in self.processed_state.get('processed_files', {}))
        self.auto_discovery.initialize_known_files(existing_files)

        self.logger.info("ğŸš€ é›†æˆè¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.base_log_dir}")
        self.logger.info(f"âš™ï¸ æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        self.logger.info(f"ğŸ§µ å·¥ä½œçº¿ç¨‹æ•°: {self.max_workers}")
        self.logger.info(f"ğŸ”— è¿æ¥æ± å¤§å°: {self.connection_pool_size}")
        self.logger.info(f"ğŸ“Š è¿›åº¦åˆ·æ–°é—´éš”: {progress_refresh_minutes} åˆ†é’Ÿ")
        self.logger.info(f"ğŸ” è‡ªåŠ¨æ‰«æé—´éš”: {self.auto_discovery.scan_interval} ç§’")

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)

        return logger

    def _init_connection_pool(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        self.logger.info("ğŸ”— åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        success_count = 0

        for i in range(self.connection_pool_size):
            try:
                writer = DWDWriter()
                if writer.connect():
                    self.writer_pool.append(writer)
                    success_count += 1
                else:
                    self.logger.error(f"âŒ æ•°æ®åº“è¿æ¥ {i+1} å¤±è´¥")
            except Exception as e:
                self.logger.error(f"âŒ åˆ›å»ºæ•°æ®åº“è¿æ¥ {i+1} å¼‚å¸¸: {e}")

        if success_count == 0:
            raise RuntimeError("æ— æ³•åˆ›å»ºä»»ä½•æ•°æ®åº“è¿æ¥")

        self.logger.info(f"âœ… æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å®Œæˆ: {success_count}/{self.connection_pool_size}")

    def get_writer(self) -> Optional[DWDWriter]:
        """è·å–å¯ç”¨çš„å†™å…¥å™¨"""
        if self.writer_pool:
            return self.writer_pool.pop(0)
        return None

    def return_writer(self, writer: DWDWriter):
        """å½’è¿˜å†™å…¥å™¨"""
        if writer:
            self.writer_pool.append(writer)

    def cached_ua_parse(self, user_agent: str, mapper: FieldMapper) -> Dict:
        """ä¼˜åŒ–çš„ç¼“å­˜ç”¨æˆ·ä»£ç†è§£æ"""
        if user_agent in self.ua_cache:
            self.cache_stats['ua_hits'] += 1
            return self.ua_cache[user_agent]

        self.cache_stats['ua_misses'] += 1

        # æ‰§è¡Œè§£æ
        try:
            parsed = mapper._parse_user_agent_enhanced(user_agent)
        except:
            parsed = {'browser': 'Unknown', 'os': 'Unknown'}

        # ç¼“å­˜ç®¡ç† (é˜²æ­¢å†…å­˜è†¨èƒ€) - æ›´ç§¯æçš„æ¸…ç†
        if len(self.ua_cache) > 3000:  # é™ä½é˜ˆå€¼ï¼Œæ›´é¢‘ç¹æ¸…ç†
            cache_size = self.performance_optimizer.optimize_cache(self.ua_cache, 3000)
            if self.enable_detailed_logging:
                self.logger.info(f"ğŸ§¹ UAç¼“å­˜ä¼˜åŒ–å®Œæˆï¼Œå½“å‰å¤§å°: {cache_size}")

        self.ua_cache[user_agent] = parsed
        return parsed

    def cached_uri_parse(self, uri: str, mapper: FieldMapper) -> Dict:
        """ä¼˜åŒ–çš„ç¼“å­˜URIè§£æ"""
        if uri in self.uri_cache:
            self.cache_stats['uri_hits'] += 1
            return self.uri_cache[uri]

        self.cache_stats['uri_misses'] += 1

        # æ‰§è¡Œè§£æ
        try:
            parsed = mapper._parse_uri_components(uri)
        except:
            parsed = {'path': uri, 'query_count': 0}

        # ç¼“å­˜ç®¡ç† - ä¼˜åŒ–æ¸…ç†ç­–ç•¥
        if len(self.uri_cache) > 3000:  # é™ä½é˜ˆå€¼
            # ä¿ç•™æœ€å¸¸ç”¨çš„ç¼“å­˜
            items = list(self.uri_cache.items())
            self.uri_cache.clear()
            self.uri_cache.update(items[-1500:])  # ä¿ç•™æ›´å°‘é¡¹ç›®

        self.uri_cache[uri] = parsed
        return parsed

    def process_file_batch(self, file_paths: List[Path], thread_id: int,
                          test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¤„ç†æ–‡ä»¶æ‰¹æ¬¡ - é›†æˆæ€§èƒ½ä¼˜åŒ–"""
        results = []

        for file_path in file_paths:
            try:
                result = self.process_single_file(file_path, thread_id, test_mode, limit)
                results.append(result)

                # æ€§èƒ½ç›‘æ§
                if result.get('success') and 'speed_rps' in result:
                    self.performance_optimizer.monitor_performance(result['speed_rps'])

                # å®šæœŸGCä¼˜åŒ–
                if self.performance_optimizer.should_optimize_gc():
                    collected = self.performance_optimizer.optimize_gc()
                    if collected > 0:
                        self.logger.info(f"ğŸ§¹ GCä¼˜åŒ–å®Œæˆï¼Œæ¸…ç†å¯¹è±¡: {collected}")

            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                results.append({
                    'success': False,
                    'file_path': str(file_path),
                    'error': str(e)
                })

        return {
            'success': all(r.get('success', False) for r in results),
            'thread_id': thread_id,
            'file_results': results
        }

    def process_single_file(self, file_path: Path, thread_id: int,
                           test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ - å®Œå…¨å…¼å®¹åŸæœ‰é€»è¾‘"""
        start_time = time.time()

        # ä¼°ç®—æ–‡ä»¶è¡Œæ•°
        estimated_lines = self._estimate_file_lines(file_path)

        # å¼€å§‹æ–‡ä»¶å¤„ç†è¿½è¸ª
        self.progress_tracker.start_file_processing(thread_id, file_path, estimated_lines)

        try:
            # è·å–ç»„ä»¶
            parser = self.parser_pool[thread_id % len(self.parser_pool)]
            mapper = self.mapper_pool[thread_id % len(self.mapper_pool)]
            writer = self.get_writer() if not test_mode else None

            if not test_mode and not writer:
                error_msg = f"çº¿ç¨‹ {thread_id} æ— æ³•è·å–æ•°æ®åº“è¿æ¥"
                self.logger.error(error_msg)
                return {'success': False, 'error': error_msg}

            batch = []
            file_records = 0
            file_lines = 0
            file_errors = 0

            try:
                # æµå¼å¤„ç†æ–‡ä»¶
                for parsed_data in parser.parse_file(file_path):
                    file_lines += 1

                    # æ›´æ–°è¿›åº¦ (å‡å°‘é¢‘ç‡ä»¥æé«˜æ€§èƒ½)
                    if file_lines % 5000 == 0:  # ä»1000æ”¹ä¸º5000ï¼Œå‡å°‘è¿›åº¦æ›´æ–°å¼€é”€
                        self.progress_tracker.update_file_progress(thread_id, file_lines, file_records, file_errors)

                        # æ˜¾ç¤ºè¿›åº¦ (æ ¹æ®é…ç½®çš„åˆ·æ–°é—´éš”)
                        self.progress_tracker.display_progress()

                        # æ‰§è¡ŒGCä¼˜åŒ–
                        if self.performance_optimizer.should_optimize_gc():
                            collected = self.performance_optimizer.optimize_gc()
                            if self.enable_detailed_logging:
                                self.logger.info(f"ğŸ—ï¸ GCä¼˜åŒ–å®Œæˆï¼Œå›æ”¶å¯¹è±¡: {collected}")

                    if parsed_data:
                        try:
                            # ç¼“å­˜è§£æ
                            user_agent = parsed_data.get('user_agent', '')
                            if user_agent:
                                parsed_data['_cached_ua'] = self.cached_ua_parse(user_agent, mapper)

                            # å®‰å…¨çš„URIè§£æ
                            request = parsed_data.get('request', '')
                            uri = ''
                            if request:
                                request_parts = request.split(' ')
                                if len(request_parts) >= 2:
                                    uri = request_parts[1]
                                else:
                                    uri = request  # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¯·æ±‚
                            if uri:
                                parsed_data['_cached_uri'] = self.cached_uri_parse(uri, mapper)

                            # å­—æ®µæ˜ å°„
                            mapped_data = mapper.map_to_dwd(parsed_data, file_path.name)
                            batch.append(mapped_data)
                            file_records += 1

                            # æ‰¹é‡å†™å…¥ - å¢åŠ å¼‚æ­¥å¤„ç†
                            if len(batch) >= self.batch_size:
                                if not test_mode and writer:
                                    try:
                                        write_start = time.time()

                                        # ä½¿ç”¨æ›´é«˜æ•ˆçš„å†™å…¥æ–¹å¼
                                        result = writer.write_batch_optimized(batch)  # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
                                        write_time = time.time() - write_start

                                        # æ£€æŸ¥å†™å…¥ç»“æœ
                                        if result and result.get('success', False):
                                            # æ›´æ–°å†™å…¥ç»Ÿè®¡
                                            self.write_stats['total_writes'] += 1
                                            self.write_stats['total_records'] += len(batch)
                                            self.write_stats['total_write_time'] += write_time

                                            # æ›´æ–°å®æ—¶ç»Ÿè®¡ (æ–°å¢)
                                            self.progress_tracker.update_batch_written(len(batch))

                                            # è®°å½•æ€§èƒ½æŒ‡æ ‡
                                            current_speed = len(batch) / write_time if write_time > 0 else 0
                                            self.performance_optimizer.monitor_performance(current_speed)

                                            if self.enable_detailed_logging:
                                                self.logger.info(f"âœ… [çº¿ç¨‹{thread_id}] æ‰¹æ¬¡å†™å…¥æˆåŠŸ: {len(batch)} æ¡è®°å½•, {write_time:.2f}ç§’")
                                        else:
                                            self.logger.error(f"å†™å…¥è¿”å›å¤±è´¥ç»“æœ: {result}")
                                            raise Exception("å†™å…¥å¤±è´¥")

                                    except Exception as e:
                                        # å¦‚æœä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ–¹æ³•
                                        try:
                                            self.logger.warning(f"ä¼˜åŒ–å†™å…¥å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {e}")
                                            result = writer.write_batch(batch)
                                            if result and result.get('success', False):
                                                self.write_stats['total_writes'] += 1
                                                self.write_stats['total_records'] += len(batch)
                                                # æ›´æ–°å®æ—¶ç»Ÿè®¡
                                                self.progress_tracker.update_batch_written(len(batch))
                                            else:
                                                raise Exception("æ ‡å‡†å†™å…¥ä¹Ÿå¤±è´¥")
                                        except Exception as e2:
                                            self.logger.error(f"æ‰¹é‡å†™å…¥å®Œå…¨å¤±è´¥: {e2}")
                                            file_errors += len(batch)  # è®°å½•å¤±è´¥çš„è®°å½•æ•°

                                batch.clear()

                            # æ£€æŸ¥é™åˆ¶
                            if limit and file_records >= limit:
                                break

                        except Exception as e:
                            file_errors += 1
                            if file_errors <= 5:
                                self.logger.error(f"è®°å½•å¤„ç†é”™è¯¯: {e}")

                # å¤„ç†å‰©ä½™æ‰¹æ¬¡
                if batch and not test_mode and writer:
                    try:
                        result = writer.write_batch(batch)
                        if result and result.get('success', False):
                            self.write_stats['total_writes'] += 1
                            self.write_stats['total_records'] += len(batch)
                            # æ›´æ–°å®æ—¶ç»Ÿè®¡
                            self.progress_tracker.update_batch_written(len(batch))
                    except Exception as e:
                        self.logger.error(f"æœ€ç»ˆæ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                        file_errors += 1

            finally:
                # å½’è¿˜å†™å…¥å™¨
                if writer:
                    self.return_writer(writer)

            # å®Œæˆæ–‡ä»¶å¤„ç†
            processing_time = time.time() - start_time
            self.progress_tracker.complete_file_processing(thread_id, file_records, file_errors)

            # æ ‡è®°æ–‡ä»¶å·²å¤„ç†
            if not test_mode:
                self.mark_file_processed(file_path, file_records, processing_time)

            result = {
                'success': True,
                'file_path': str(file_path),
                'lines_processed': file_lines,
                'records_processed': file_records,
                'errors': file_errors,
                'processing_time': processing_time,
                'speed_rps': file_records / processing_time if processing_time > 0 else 0
            }

            self.logger.info(
                f"âœ… {file_path.name}: {file_records:,} æ¡è®°å½•, "
                f"{processing_time:.1f}ç§’, {result['speed_rps']:.0f} RPS"
            )

            return result

        except Exception as e:
            error_msg = f"å¤„ç†æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}"
            self.logger.error(error_msg)
            self.progress_tracker.complete_file_processing(thread_id, 0, 1)
            return {
                'success': False,
                'file_path': str(file_path),
                'error': error_msg
            }

    def _estimate_file_lines(self, file_path: Path) -> int:
        """ä¼°ç®—æ–‡ä»¶è¡Œæ•°"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                sample_lines = []
                for i, line in enumerate(f):
                    sample_lines.append(len(line))
                    if i >= 999:
                        break

            if sample_lines:
                avg_line_length = sum(sample_lines) / len(sample_lines)
                file_size = file_path.stat().st_size
                estimated_lines = int(file_size / avg_line_length)
                return max(1000, estimated_lines)
            else:
                return 10000
        except Exception:
            return 10000

    # === å®Œå…¨å…¼å®¹åŸæœ‰çš„çŠ¶æ€ç®¡ç† ===

    def load_state(self) -> Dict[str, Any]:
        """åŠ è½½å¤„ç†çŠ¶æ€ - å®Œå…¨å…¼å®¹åŸæœ‰æ ¼å¼"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

        return {
            'processed_files': {},
            'last_update': None,
            'total_processed_records': 0,
            'processing_history': []
        }

    def save_state(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€ - å®Œå…¨å…¼å®¹åŸæœ‰æ ¼å¼"""
        try:
            self.processed_state['last_update'] = datetime.now().isoformat()
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_state, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            self.logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def scan_log_directories(self) -> Dict[str, List[Path]]:
        """æ‰«ææ—¥å¿—ç›®å½• - å®Œå…¨å…¼å®¹åŸæœ‰é€»è¾‘"""
        if not self.base_log_dir.exists():
            return {}

        log_files_by_date = {}
        for date_dir in self.base_log_dir.iterdir():
            if date_dir.is_dir() and date_dir.name.isdigit() and len(date_dir.name) == 8:
                try:
                    datetime.strptime(date_dir.name, '%Y%m%d')
                    log_files = list(date_dir.glob("*.log"))
                    if log_files:
                        log_files_by_date[date_dir.name] = sorted(log_files)
                except ValueError:
                    continue

        return log_files_by_date

    def is_file_processed(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        file_key = str(file_path)
        return file_key in self.processed_state.get('processed_files', {})

    def mark_file_processed(self, file_path: Path, record_count: int, processing_time: float):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        file_key = str(file_path)
        try:
            self.processed_state['processed_files'][file_key] = {
                'processed_at': datetime.now().isoformat(),
                'record_count': record_count,
                'processing_time': processing_time,
                'mtime': file_path.stat().st_mtime,
                'file_size': file_path.stat().st_size
            }
            # å®šæœŸä¿å­˜çŠ¶æ€
            if len(self.processed_state['processed_files']) % 10 == 0:
                self.save_state()
        except Exception as e:
            self.logger.error(f"æ ‡è®°æ–‡ä»¶çŠ¶æ€å¤±è´¥ {file_path}: {e}")

    # === å®Œå…¨å…¼å®¹åŸæœ‰çš„å¤„ç†æ–¹æ³• ===

    def process_date_parallel(self, date_str: str, force_reprocess: bool = False,
                             test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ—¥å¿— - å®Œå…¨å…¼å®¹åŸæœ‰é€»è¾‘"""
        start_time = time.time()
        date_dir = self.base_log_dir / date_str

        if not date_dir.exists():
            return {
                'success': False,
                'error': f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date_dir}',
                'date': date_str
            }

        # è·å–æ—¥å¿—æ–‡ä»¶
        log_files = list(date_dir.glob("*.log"))
        if not log_files:
            return {
                'success': True,
                'message': f'æ—¥æœŸ {date_str} æ²¡æœ‰æ—¥å¿—æ–‡ä»¶',
                'date': date_str,
                'processed_files': 0,
                'total_records': 0
            }

        # è¿‡æ»¤å·²å¤„ç†æ–‡ä»¶
        if not force_reprocess:
            unprocessed_files = [f for f in log_files if not self.is_file_processed(f)]
            if not unprocessed_files:
                return {
                    'success': True,
                    'message': f'æ—¥æœŸ {date_str} çš„æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†',
                    'date': date_str,
                    'processed_files': 0,
                    'total_records': 0
                }
            log_files = unprocessed_files

        self.logger.info(f"ğŸ“… å¤„ç†æ—¥æœŸ {date_str}: {len(log_files)} ä¸ªæ–‡ä»¶")

        # å¯åŠ¨è¿›åº¦è¿½è¸ª
        self.progress_tracker.start_session(len(log_files))

        # æ–‡ä»¶åˆ†ç»„å¤„ç†
        files_per_thread = max(1, len(log_files) // self.max_workers)
        file_groups = [log_files[i:i + files_per_thread]
                      for i in range(0, len(log_files), files_per_thread)]

        # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§çº¿ç¨‹æ•°
        while len(file_groups) > self.max_workers:
            last_group = file_groups.pop()
            file_groups[-1].extend(last_group)

        # å¹¶è¡Œå¤„ç†
        total_records = 0
        total_errors = 0
        processed_files = 0

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_group = {
                executor.submit(self.process_file_batch, group, thread_id, test_mode, limit): thread_id
                for thread_id, group in enumerate(file_groups)
            }

            for future in as_completed(future_to_group):
                thread_id = future_to_group[future]
                try:
                    result = future.result()
                    if result['success']:
                        for file_result in result['file_results']:
                            if file_result.get('success'):
                                total_records += file_result.get('records_processed', 0)
                                processed_files += 1
                            total_errors += file_result.get('errors', 0)
                except Exception as e:
                    self.logger.error(f"çº¿ç¨‹ {thread_id} å¤„ç†å¼‚å¸¸: {e}")
                    total_errors += 1

        # æœ€ç»ˆè¿›åº¦æ˜¾ç¤º
        self.progress_tracker.display_progress(force=True)

        # ä¿å­˜çŠ¶æ€
        self.save_state()

        processing_time = time.time() - start_time
        processing_speed = total_records / processing_time if processing_time > 0 else 0

        return {
            'success': total_errors == 0,
            'date': date_str,
            'processed_files': processed_files,
            'total_records': total_records,
            'total_errors': total_errors,
            'duration': processing_time,
            'processing_speed': processing_speed
        }

    def process_all_parallel(self, test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¤„ç†æ‰€æœ‰æ—¥å¿— - å®Œå…¨å…¼å®¹åŸæœ‰é€»è¾‘"""
        start_time = time.time()
        log_files_by_date = self.scan_log_directories()

        if not log_files_by_date:
            return {
                'success': False,
                'error': 'æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶'
            }

        self.logger.info(f"ğŸ“ å‘ç° {len(log_files_by_date)} ä¸ªæ—¥æœŸçš„æ—¥å¿—æ–‡ä»¶")

        total_records = 0
        processed_dates = 0
        all_errors = []

        for date_str in sorted(log_files_by_date.keys()):
            result = self.process_date_parallel(date_str, force_reprocess=False,
                                              test_mode=test_mode, limit=limit)
            if result['success'] and result.get('processed_files', 0) > 0:
                processed_dates += 1
                total_records += result['total_records']
            else:
                if result.get('error'):
                    all_errors.append(result['error'])

        duration = time.time() - start_time
        overall_speed = total_records / duration if duration > 0 else 0

        return {
            'success': len(all_errors) == 0,
            'processed_dates': processed_dates,
            'total_records': total_records,
            'duration': duration,
            'processing_speed': overall_speed,
            'errors': all_errors
        }

    # === å®Œå…¨å…¼å®¹åŸæœ‰çš„æ€§èƒ½ç»Ÿè®¡å’Œé”™è¯¯æ—¥å¿— ===

    def show_performance_stats(self):
        """æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ - å®Œå…¨å…¼å®¹åŸæœ‰æ ¼å¼"""
        print("\\n" + "=" * 80)
        print("ğŸš€ é›†æˆè¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 80)

        print(f"âš™ï¸  é…ç½®ä¿¡æ¯:")
        print(f"   æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        print(f"   è¿æ¥æ± å¤§å°: {self.connection_pool_size}")
        print(f"   è¯¦ç»†æ—¥å¿—: {'å¯ç”¨' if self.enable_detailed_logging else 'ç¦ç”¨'}")

        # ç¼“å­˜ç»Ÿè®¡
        total_ua = self.cache_stats['ua_hits'] + self.cache_stats['ua_misses']
        total_uri = self.cache_stats['uri_hits'] + self.cache_stats['uri_misses']
        ua_hit_rate = self.cache_stats['ua_hits'] / total_ua * 100 if total_ua > 0 else 0
        uri_hit_rate = self.cache_stats['uri_hits'] / total_uri * 100 if total_uri > 0 else 0

        print(f"\\nğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:")
        print(f"   User-Agentç¼“å­˜: {len(self.ua_cache)} é¡¹ (å‘½ä¸­ç‡: {ua_hit_rate:.1f}%)")
        print(f"   URIç¼“å­˜: {len(self.uri_cache)} é¡¹ (å‘½ä¸­ç‡: {uri_hit_rate:.1f}%)")

        # æ€§èƒ½è¶‹åŠ¿
        trend = self.performance_optimizer.get_performance_trend()
        trend_icon = {"declining": "ğŸ“‰", "improving": "ğŸ“ˆ", "stable": "ğŸ“Š"}.get(trend, "â“")
        print(f"   æ€§èƒ½è¶‹åŠ¿: {trend_icon} {trend}")

        # ä¼šè¯ç»Ÿè®¡
        session_stats = self.progress_tracker.get_session_summary()
        if session_stats['avg_processing_speed'] > 0:
            print(f"\\nğŸƒ æ€§èƒ½æŒ‡æ ‡:")
            print(f"   å¹³å‡å¤„ç†é€Ÿåº¦: {session_stats['avg_processing_speed']:.1f} è®°å½•/ç§’")
            print(f"   æ€»å¤„ç†è®°å½•æ•°: {session_stats.get('total_records_written', 0):,}")
            print(f"   æ€»å¤„ç†æ–‡ä»¶æ•°: {session_stats.get('total_files_processed', 0)}")

        # å†™å…¥ç»Ÿè®¡
        if self.write_stats['total_writes'] > 0:
            avg_write_time = self.write_stats['total_write_time'] / self.write_stats['total_writes']
            print(f"\\nğŸ’¾ å†™å…¥ç»Ÿè®¡:")
            print(f"   æ€»å†™å…¥æ¬¡æ•°: {self.write_stats['total_writes']}")
            print(f"   æ€»å†™å…¥è®°å½•: {self.write_stats['total_records']:,}")
            print(f"   å¹³å‡å†™å…¥æ—¶é—´: {avg_write_time:.3f}ç§’")

        print("=" * 80)

    def print_detailed_error_log(self):
        """æ‰“å°è¯¦ç»†é”™è¯¯æ—¥å¿— - å®Œå…¨å…¼å®¹åŸæœ‰æ ¼å¼"""
        if not self.enable_detailed_logging:
            print("è¯¦ç»†é”™è¯¯æ—¥å¿—æœªå¯ç”¨ï¼Œè¯·åœ¨åˆå§‹åŒ–æ—¶è®¾ç½® enable_detailed_logging=True")
            return

        session_stats = self.progress_tracker.get_session_summary()
        error_log = session_stats.get('detailed_error_log', [])

        if not error_log:
            print("âœ… æ²¡æœ‰è¯¦ç»†é”™è¯¯è®°å½•")
            return

        print("\\n" + "="*80)
        print("ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿— (æœ€è¿‘10ä¸ª)")
        print("="*80)

        for error in error_log[-10:]:
            print(f"\\nğŸ“… æ—¶é—´: {error['timestamp']}")
            print(f"ğŸ·ï¸  ç±»å‹: {error['type']}")
            print(f"ğŸ“ æ¶ˆæ¯: {error['message']}")
            if error.get('context'):
                print(f"ğŸ” ä¸Šä¸‹æ–‡: {error['context']}")
            print("-" * 40)

        print("="*80)

    # === è‡ªåŠ¨æ–‡ä»¶å‘ç°å’Œç›‘æ§åŠŸèƒ½ (æ–°å¢) ===

    def auto_process_new_files(self) -> Dict[str, Any]:
        """è‡ªåŠ¨å¤„ç†æ–°å‘ç°çš„æ–‡ä»¶"""
        with self.processing_lock:
            if self.is_processing:
                return {'success': False, 'error': 'Already processing', 'skipped': True}
            self.is_processing = True

        try:
            start_time = time.time()
            new_files = self.auto_discovery.discover_new_files()

            if not new_files:
                return {'success': True, 'new_files': 0, 'message': 'No new files found'}

            self.logger.info(f"å¼€å§‹è‡ªåŠ¨å¤„ç† {len(new_files)} ä¸ªæ–°æ–‡ä»¶")

            # ä½¿ç”¨åŸæœ‰çš„é«˜æ€§èƒ½å¤„ç†é€»è¾‘
            total_records = 0
            processed_files = 0
            errors = []

            for file_path in new_files:
                try:
                    # ä½¿ç”¨åŸæœ‰çš„å¤„ç†é€»è¾‘
                    result = self._process_single_file(file_path, test_mode=False)
                    if result['success'] and not result.get('skipped', False):
                        total_records += result['records']
                        processed_files += 1
                    elif not result['success']:
                        errors.append(f"{file_path}: {result.get('error', 'Unknown error')}")
                except Exception as e:
                    errors.append(f"{file_path}: {str(e)}")

            processing_time = time.time() - start_time

            result = {
                'success': True,
                'new_files': len(new_files),
                'processed_files': processed_files,
                'total_records': total_records,
                'processing_time': processing_time,
                'processing_speed': total_records / processing_time if processing_time > 0 else 0,
                'errors': errors
            }

            if processed_files > 0:
                self.logger.info(
                    f"è‡ªåŠ¨å¤„ç†å®Œæˆ: {processed_files} æ–‡ä»¶, {total_records:,} è®°å½•, "
                    f"é€Ÿåº¦ {result['processing_speed']:.1f} rec/s"
                )

            return result

        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")
            return {'success': False, 'error': str(e)}
        finally:
            with self.processing_lock:
                self.is_processing = False

    def monitoring_loop(self):
        """ç›‘æ§å¾ªç¯çº¿ç¨‹"""
        self.logger.info(f"è‡ªåŠ¨ç›‘æ§å·²å¯åŠ¨ï¼Œæ‰«æé—´éš” {self.auto_discovery.scan_interval} ç§’")

        while not self.stop_monitoring.is_set():
            try:
                # ç­‰å¾…æ‰«æé—´éš”
                if self.stop_monitoring.wait(self.auto_discovery.scan_interval):
                    break  # æ”¶åˆ°åœæ­¢ä¿¡å·

                # æ‰§è¡Œè‡ªåŠ¨å¤„ç†
                result = self.auto_process_new_files()

                if result.get('new_files', 0) > 0:
                    print(f"\nğŸ” è‡ªåŠ¨å‘ç°å¹¶å¤„ç†äº† {result['processed_files']} ä¸ªæ–°æ–‡ä»¶ï¼Œ"
                          f"å…± {result['total_records']:,} æ¡è®°å½•")
                    print("ğŸ‘† æŒ‰å›è½¦é”®è¿”å›èœå•ï¼Œæˆ–ç­‰å¾…ç»§ç»­ç›‘æ§...")

            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†ç»§ç»­

    def start_auto_monitoring(self):
        """å¯åŠ¨è‡ªåŠ¨ç›‘æ§"""
        if self.monitoring_enabled:
            print("âš ï¸ è‡ªåŠ¨ç›‘æ§å·²ç»åœ¨è¿è¡Œä¸­")
            return False

        self.monitoring_enabled = True
        self.stop_monitoring.clear()
        self.monitoring_thread = threading.Thread(target=self.monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        print(f"âœ… è‡ªåŠ¨ç›‘æ§å·²å¯åŠ¨ï¼Œæ‰«æé—´éš” {self.auto_discovery.scan_interval} ç§’")
        print("ğŸ’¡ æç¤º: æŒ‰ Ctrl+C æˆ–è¾“å…¥ä»»æ„é”®åœæ­¢ç›‘æ§")
        return True

    def stop_auto_monitoring(self):
        """åœæ­¢è‡ªåŠ¨ç›‘æ§"""
        if not self.monitoring_enabled:
            return False

        self.monitoring_enabled = False
        self.stop_monitoring.set()

        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=5)

        print("âœ… è‡ªåŠ¨ç›‘æ§å·²åœæ­¢")
        return True

    # === å®Œå…¨å…¼å®¹åŸæœ‰çš„äº¤äº’å¼èœå• ===

    def interactive_menu(self):
        """äº¤äº’å¼èœå• - å®Œå…¨å…¼å®¹åŸæœ‰åŠŸèƒ½"""
        while True:
            print("\\n" + "=" * 80)
            print("ğŸš€ é›†æˆè¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - äº¤äº’å¼èœå•")
            print("=" * 80)
            print("1. ğŸ”¥ å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿— (æ¨è)")
            print("2. ğŸ“… å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ—¥å¿—")
            print("3. ğŸ“Š æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡")
            print("4. ğŸ§ª æµ‹è¯•æ¨¡å¼å¤„ç†")
            print("5. âš™ï¸ æ€§èƒ½å‚æ•°è°ƒä¼˜")
            print("6. ğŸ” æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—")
            print("7. ğŸ—‚ï¸ æŸ¥çœ‹å¤„ç†çŠ¶æ€")
            print("0. ğŸ‘‹ é€€å‡º")
            print("-" * 80)
            print(f"ğŸ“Š å½“å‰é…ç½®: æ‰¹é‡{self.batch_size} | çº¿ç¨‹{self.max_workers} | è¿æ¥æ± {self.connection_pool_size}")

            try:
                choice = input("è¯·é€‰æ‹©æ“ä½œ [0-7]: ").strip()

                if choice == '0':
                    print("ğŸ‘‹ å†è§ï¼")
                    break

                elif choice == '1':
                    print("\\nğŸ”¥ é«˜æ€§èƒ½å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—...")
                    limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None

                    result = self.process_all_parallel(test_mode=False, limit=limit)
                    if result['success']:
                        print(f"\\nâœ… å¤„ç†å®Œæˆ: {result['total_records']:,} è®°å½•, é€Ÿåº¦ {result['processing_speed']:.1f} rec/s")

                        # å¤„ç†å®Œæˆåè¯¢é—®æ˜¯å¦è¿›å…¥è‡ªåŠ¨ç›‘æ§æ¨¡å¼
                        print("\\nğŸ¤– å¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥è¿›å…¥è‡ªåŠ¨ç›‘æ§æ¨¡å¼")
                        print(f"ğŸ“Š å°†æ¯ {self.auto_discovery.scan_interval} ç§’è‡ªåŠ¨æ£€æŸ¥æ–°æ–‡ä»¶å¹¶å¤„ç†")
                        auto_monitor = input("æ˜¯å¦å¯åŠ¨è‡ªåŠ¨ç›‘æ§ï¼Ÿ(Y/n): ").strip().lower()

                        if auto_monitor != 'n':
                            self.start_auto_monitoring()
                            try:
                                print("\\nğŸ” è‡ªåŠ¨ç›‘æ§ä¸­...")
                                print("ğŸ’¡ æç¤º: æŒ‰å›è½¦é”®åœæ­¢ç›‘æ§å¹¶è¿”å›èœå•")
                                input()  # ç­‰å¾…ç”¨æˆ·è¾“å…¥
                            except KeyboardInterrupt:
                                pass
                            finally:
                                self.stop_auto_monitoring()
                    else:
                        print(f"\\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                elif choice == '2':
                    date_str = input("\\nè¯·è¾“å…¥æ—¥æœŸ (YYYYMMDDæ ¼å¼): ").strip()
                    if len(date_str) != 8 or not date_str.isdigit():
                        print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯")
                        continue

                    force = input("å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Ÿ(y/N): ").strip().lower() == 'y'
                    limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None

                    result = self.process_date_parallel(date_str, force, test_mode=False, limit=limit)
                    if result['success']:
                        print(f"\\nâœ… å¤„ç†å®Œæˆ: {result['total_records']:,} è®°å½•, é€Ÿåº¦ {result['processing_speed']:.1f} rec/s")
                    else:
                        print(f"\\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                elif choice == '3':
                    self.show_performance_stats()

                elif choice == '4':
                    date_str = input("\\nè¯·è¾“å…¥æµ‹è¯•æ—¥æœŸ (YYYYMMDDæ ¼å¼): ").strip()
                    limit_input = input("é™åˆ¶å¤„ç†è¡Œæ•° (å»ºè®®100-1000): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else 100

                    result = self.process_date_parallel(date_str, False, test_mode=True, limit=limit)
                    if result['success']:
                        print(f"\\nâœ… æµ‹è¯•å®Œæˆ: {result['total_records']:,} è®°å½•, é€Ÿåº¦ {result['processing_speed']:.1f} rec/s")

                elif choice == '5':
                    self._interactive_performance_tuning()

                elif choice == '6':
                    self.print_detailed_error_log()

                elif choice == '7':
                    self._show_processing_status()

                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")

                input("\\næŒ‰å›è½¦é”®ç»§ç»­...")

            except KeyboardInterrupt:
                print("\\n\\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\\nâŒ æ“ä½œå¼‚å¸¸: {e}")

    def _interactive_performance_tuning(self):
        """äº¤äº’å¼æ€§èƒ½è°ƒä¼˜ - æ‰©å±•åŸæœ‰åŠŸèƒ½"""
        print("\\nâš™ï¸ æ€§èƒ½å‚æ•°è°ƒä¼˜")
        print(f"å½“å‰é…ç½®:")
        print(f"  æ‰¹é‡å¤§å°: {self.batch_size}")
        print(f"  çº¿ç¨‹æ•°: {self.max_workers}")
        print(f"  è¿æ¥æ± : {self.connection_pool_size}")
        print(f"  è¿›åº¦åˆ·æ–°é—´éš”: {self.progress_tracker.refresh_interval // 60} åˆ†é’Ÿ")
        print(f"  è‡ªåŠ¨æ‰«æé—´éš”: {self.auto_discovery.scan_interval} ç§’")

        # è·å–ç³»ç»Ÿä¿¡æ¯æ¨è
        if PSUTIL_AVAILABLE:
            try:
                cpu_count = psutil.cpu_count()
                memory_gb = psutil.virtual_memory().total / (1024**3)
                print(f"\\nğŸ’» ç³»ç»Ÿä¿¡æ¯:")
                print(f"  CPUæ ¸å¿ƒæ•°: {cpu_count}")
                print(f"  æ€»å†…å­˜: {memory_gb:.1f}GB")
                print(f"\\nğŸ“Š æ¨èé…ç½®:")
                print(f"  æ¨èçº¿ç¨‹æ•°: {min(cpu_count, 8)}")
                print(f"  æ¨èæ‰¹é‡å¤§å°: {min(4000, int(memory_gb * 500))}")
            except:
                pass

        # äº¤äº’å¼é…ç½®
        new_batch = input(f"\\næ–°çš„æ‰¹é‡å¤§å° (å½“å‰{self.batch_size}, æ¨è1000-5000): ").strip()
        new_workers = input(f"æ–°çš„çº¿ç¨‹æ•° (å½“å‰{self.max_workers}, æ¨è2-8): ").strip()
        new_pool = input(f"æ–°çš„è¿æ¥æ± å¤§å° (å½“å‰{self.connection_pool_size}, æ¨è=çº¿ç¨‹æ•°): ").strip()
        new_refresh = input(f"æ–°çš„è¿›åº¦åˆ·æ–°é—´éš”(åˆ†é’Ÿ) (å½“å‰{self.progress_tracker.refresh_interval // 60}, æ¨è1-5): ").strip()
        new_scan_interval = input(f"æ–°çš„è‡ªåŠ¨æ‰«æé—´éš”(ç§’) (å½“å‰{self.auto_discovery.scan_interval}, æ¨è180-600): ").strip()

        # åº”ç”¨é…ç½®
        if new_batch.isdigit():
            self.batch_size = max(500, min(10000, int(new_batch)))
            print(f"âœ… æ‰¹é‡å¤§å°è°ƒæ•´ä¸º: {self.batch_size}")

        if new_workers.isdigit():
            old_workers = self.max_workers
            self.max_workers = max(1, min(16, int(new_workers)))
            print(f"âœ… çº¿ç¨‹æ•°è°ƒæ•´ä¸º: {self.max_workers}")

            if self.max_workers != old_workers:
                print("ğŸ”„ é‡æ–°åˆå§‹åŒ–ç»„ä»¶æ± ...")
                self.parser_pool = [BaseLogParser() for _ in range(self.max_workers)]
                self.mapper_pool = [FieldMapper() for _ in range(self.max_workers)]

        if new_pool.isdigit():
            self.connection_pool_size = max(1, min(16, int(new_pool)))
            print(f"âœ… è¿æ¥æ± å¤§å°è°ƒæ•´ä¸º: {self.connection_pool_size}")
            print("âš ï¸  è¿æ¥æ± è°ƒæ•´éœ€è¦é‡å¯ç¨‹åºç”Ÿæ•ˆ")

        if new_refresh.isdigit():
            refresh_minutes = max(1, min(30, int(new_refresh)))
            self.progress_tracker.refresh_interval = refresh_minutes * 60
            print(f"âœ… è¿›åº¦åˆ·æ–°é—´éš”è°ƒæ•´ä¸º: {refresh_minutes} åˆ†é’Ÿ")

        if new_scan_interval.isdigit():
            scan_interval = max(60, min(3600, int(new_scan_interval)))  # 1åˆ†é’Ÿåˆ°1å°æ—¶
            self.auto_discovery.scan_interval = scan_interval
            print(f"âœ… è‡ªåŠ¨æ‰«æé—´éš”è°ƒæ•´ä¸º: {scan_interval} ç§’")
            if self.monitoring_enabled:
                print("âš ï¸  è‡ªåŠ¨ç›‘æ§æ­£åœ¨è¿è¡Œï¼Œæ–°è®¾ç½®å°†åœ¨ä¸‹æ¬¡æ‰«ææ—¶ç”Ÿæ•ˆ")

    def _show_processing_status(self):
        """æ˜¾ç¤ºå¤„ç†çŠ¶æ€"""
        print("\\nğŸ“‹ æ–‡ä»¶å¤„ç†çŠ¶æ€")
        print("-" * 50)

        processed_files = self.processed_state.get('processed_files', {})
        if not processed_files:
            print("æš‚æ— å·²å¤„ç†æ–‡ä»¶è®°å½•")
            return

        print(f"å·²å¤„ç†æ–‡ä»¶æ€»æ•°: {len(processed_files)}")
        print(f"æœ€åæ›´æ–°æ—¶é—´: {self.processed_state.get('last_update', 'Unknown')}")

        # æ˜¾ç¤ºæœ€è¿‘å¤„ç†çš„10ä¸ªæ–‡ä»¶
        recent_files = sorted(processed_files.items(),
                            key=lambda x: x[1].get('processed_at', ''),
                            reverse=True)[:10]

        print("\\nğŸ“„ æœ€è¿‘å¤„ç†çš„æ–‡ä»¶:")
        for file_path, info in recent_files:
            file_name = Path(file_path).name
            record_count = info.get('record_count', 0)
            processing_time = info.get('processing_time', 0)
            speed = record_count / processing_time if processing_time > 0 else 0
            processed_at = info.get('processed_at', '')[:19]  # åªæ˜¾ç¤ºåˆ°ç§’

            print(f"  {file_name}: {record_count:,} æ¡è®°å½•, {speed:.0f} RPS, {processed_at}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")

        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        self.save_state()

        # å…³é—­æ•°æ®åº“è¿æ¥
        for writer in self.writer_pool:
            try:
                writer.close()
            except Exception as e:
                self.logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

        # æ¸…ç†ç¼“å­˜
        self.ua_cache.clear()
        self.uri_cache.clear()

        self.logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # åœæ­¢è‡ªåŠ¨ç›‘æ§çº¿ç¨‹
        if hasattr(self, 'monitoring_enabled') and self.monitoring_enabled:
            self.stop_auto_monitoring()

        self.cleanup()

def main():
    """ä¸»å‡½æ•° - å®Œå…¨å…¼å®¹åŸæœ‰å‘½ä»¤è¡Œå‚æ•°"""
    import logging

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(
        description='é›†æˆè¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - è§£å†³æ€§èƒ½è¡°å‡é—®é¢˜'
    )

    parser.add_argument('--date', help='å¤„ç†æŒ‡å®šæ—¥æœŸ (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰æœªå¤„ç†æ—¥å¿—')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--limit', type=int, help='æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°é™åˆ¶')
    parser.add_argument('--batch-size', type=int, default=2000, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--workers', type=int, default=4, help='å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--pool-size', type=int, help='è¿æ¥æ± å¤§å°')
    parser.add_argument('--detailed-logging', action='store_true', default=True, help='å¯ç”¨è¯¦ç»†é”™è¯¯æ—¥å¿—')
    parser.add_argument('--refresh-minutes', type=int, default=3, help='è¿›åº¦åˆ·æ–°é—´éš”(åˆ†é’Ÿ)')

    args = parser.parse_args()

    try:
        with IntegratedUltraETLController(
            batch_size=args.batch_size,
            max_workers=args.workers,
            connection_pool_size=args.pool_size,
            enable_detailed_logging=args.detailed_logging,
            progress_refresh_minutes=args.refresh_minutes
        ) as controller:

            # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºäº¤äº’å¼èœå•
            if not any([args.date, args.all]):
                controller.interactive_menu()
                return

            if args.date:
                result = controller.process_date_parallel(
                    args.date,
                    force_reprocess=args.force,
                    test_mode=args.test,
                    limit=args.limit
                )

                print(f"\\nğŸ¯ å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸ: {result['date']}")
                print(f"æ–‡ä»¶: {result.get('processed_files', 0)}")
                print(f"è®°å½•: {result.get('total_records', 0):,}")
                print(f"è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")

            elif args.all:
                result = controller.process_all_parallel(
                    test_mode=args.test,
                    limit=args.limit
                )

                print(f"\\nğŸ¯ æ‰¹é‡å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸæ•°: {result.get('processed_dates', 0)}")
                print(f"è®°å½•æ•°: {result.get('total_records', 0):,}")
                print(f"æ€»è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"å¹³å‡é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")

            controller.show_performance_stats()

    except KeyboardInterrupt:
        print("\\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\\nâŒ æ‰§è¡Œé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()