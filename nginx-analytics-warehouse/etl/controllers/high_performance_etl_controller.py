#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å›å½’é«˜æ€§èƒ½ç‰ˆæœ¬ (æ–¹æ¡ˆA)
åŸºäºbackup-20250913-014803ï¼Œä¼˜åŒ–æ”¹è¿›ï¼š
1. ä¿ç•™è¯¦ç»†é”™è¯¯æ—¥å¿—è¾“å‡º - è§£å†³é”™è¯¯å®šä½é—®é¢˜
2. å¯é…ç½®çš„æ‰¹å¤„ç†ã€æ‰¹å¤§å°å’Œçº¿ç¨‹æ± å¤§å° - çµæ´»è°ƒä¼˜
3. ç§»é™¤è¿‡åº¦ç›‘æ§å¼€é”€ - æ¢å¤é«˜æ€§èƒ½ (ç›®æ ‡1200+ rps)
4. ç®€åŒ–è€Œä¸å¤±å®ç”¨çš„é”™è¯¯å¤„ç†
"""

import sys
import os
import json
import time
import argparse
import threading
import queue
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import gc
import traceback

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
current_dir = Path(__file__).parent
etl_root = current_dir.parent
sys.path.append(str(etl_root))

# å¯¼å…¥åŠ¨æ€æ‰¹å¤§å°ä¼˜åŒ–å™¨
from utils.dynamic_batch_optimizer import DynamicBatchOptimizer, BatchSizeRecommendation

from parsers.base_log_parser import BaseLogParser
from processors.field_mapper import FieldMapper
from writers.dwd_writer import DWDWriter

class HighPerformanceETLController:
    """é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å›å½’é«˜æ€§èƒ½ç‰ˆæœ¬"""

    def __init__(self,
                 base_log_dir: str = None,
                 state_file: str = None,
                 batch_size: int = 25000,       # å¯é…ç½®æ‰¹å¤„ç†å¤§å°
                 max_workers: int = 6,          # å¯é…ç½®çº¿ç¨‹æ•°
                 connection_pool_size: int = None,  # å¯é…ç½®è¿æ¥æ± å¤§å°
                 memory_limit_mb: int = 512,    # å†…å­˜é™åˆ¶
                 enable_detailed_logging: bool = True):  # è¯¦ç»†æ—¥å¿—å¼€å…³
        """
        åˆå§‹åŒ–é«˜æ€§èƒ½ETLæ§åˆ¶å™¨

        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆæ¨è1000-5000ï¼Œå¯æ ¹æ®å†…å­˜è°ƒæ•´ï¼‰
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆæ¨è2-8ï¼Œæ ¹æ®CPUæ ¸å¿ƒæ•°ï¼‰
            connection_pool_size: æ•°æ®åº“è¿æ¥æ± å¤§å°ï¼ˆé»˜è®¤ä¸max_workersç›¸åŒï¼‰
            memory_limit_mb: å†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆMBï¼‰
            enable_detailed_logging: æ˜¯å¦å¯ç”¨è¯¦ç»†é”™è¯¯æ—¥å¿—
        """
        # åŸºç¡€é…ç½®
        self.base_log_dir = Path(base_log_dir) if base_log_dir else \
            Path("D:/project/nginx-log-analyzer/nginx-analytics-warehouse/nginx_logs")
        self.state_file = Path(state_file) if state_file else \
            Path(etl_root / "processed_logs_state.json")

        # æ€§èƒ½é…ç½® - å¯è°ƒä¼˜å‚æ•°
        self.initial_batch_size = batch_size
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.connection_pool_size = connection_pool_size if connection_pool_size is not None else max_workers
        self.memory_limit_mb = memory_limit_mb
        self.enable_detailed_logging = enable_detailed_logging

        # åˆå§‹åŒ–åŠ¨æ€æ‰¹å¤§å°ä¼˜åŒ–å™¨
        self.batch_optimizer = DynamicBatchOptimizer(
            initial_batch_size=batch_size,
            min_batch_size=max(1000, batch_size // 10),
            max_batch_size=min(100000, batch_size * 4),
            memory_threshold=0.8,
            cpu_threshold=0.9
        )

        # æ—¥å¿—é…ç½®
        import logging
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # åˆ›å»ºè¯¦ç»†çš„console handlerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_detailed_logging and not self.logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

        # çº¿ç¨‹å®‰å…¨çš„ç»„ä»¶æ± 
        self.parser_pool = [BaseLogParser() for _ in range(max_workers)]
        self.mapper_pool = [FieldMapper() for _ in range(max_workers)]
        self.writer_pool = []

        # æ•°æ®åº“å†™å…¥ä¼˜åŒ–å‚æ•°
        self.enable_async_writes = True  # å¼‚æ­¥å†™å…¥
        self.write_buffer_size = batch_size * 2  # å†™å…¥ç¼“å†²åŒºå¤§å°
        self.delayed_commit_seconds = 0.5  # å»¶è¿Ÿæäº¤æ—¶é—´
        self.enable_batch_aggregation = True  # æ‰¹é‡èšåˆ
        self.max_concurrent_writes = max(2, max_workers // 2)  # æœ€å¤§å¹¶å‘å†™å…¥æ•°

        # å†™å…¥ç¼“å†²åŒºå’Œé˜Ÿåˆ—
        self.write_buffer = []
        self.write_buffer_lock = threading.Lock()
        self.async_write_queue = queue.Queue(maxsize=100)
        self.write_thread_pool = None

        # æ€§èƒ½ç»Ÿè®¡
        self.write_stats = {
            'total_writes': 0,
            'total_records': 0,
            'total_write_time': 0,
            'buffer_flushes': 0,
            'async_writes': 0
        }

        # åˆå§‹åŒ–è¿æ¥æ± 
        self._init_connection_pool()

        # å¯åŠ¨å¼‚æ­¥å†™å…¥çº¿ç¨‹
        self._start_async_write_threads()

        # ç®€åŒ–çš„ç¼“å­˜ç­–ç•¥ - é«˜æ•ˆä½†ä¸è¿‡åº¦
        self.ua_cache = {}  # User-Agentè§£æç¼“å­˜
        self.uri_cache = {}  # URIè§£æç¼“å­˜
        self.cache_hit_stats = {'ua_hits': 0, 'uri_hits': 0, 'total_requests': 0}

        # çº¿ç¨‹åŒæ­¥
        self.result_queue = queue.Queue()
        self.error_queue = queue.Queue()

        # å†™å…¥ä¼˜åŒ–æ§åˆ¶
        self.last_buffer_flush = time.time()
        self.shutdown_event = threading.Event()
        self.stats_lock = threading.Lock()

        # å¤„ç†çŠ¶æ€
        self.processed_state = self.load_state()

        # ç®€åŒ–çš„ç»Ÿè®¡ä¿¡æ¯ - ä¿ç•™æ ¸å¿ƒæŒ‡æ ‡ï¼Œç§»é™¤è¿‡åº¦ç›‘æ§
        self.session_stats = {
            'start_time': None,
            'end_time': None,
            'total_files_processed': 0,
            'total_lines_processed': 0,
            'total_records_written': 0,
            'total_errors': 0,
            'cache_hit_rate': 0.0,
            'avg_processing_speed': 0.0,
            'processing_errors': [],
            # ç®€åŒ–çš„é”™è¯¯ç»Ÿè®¡ - åªä¿ç•™å…³é”®é”™è¯¯ç±»å‹
            'error_stats': {
                'parsing_errors': 0,
                'mapping_errors': 0,
                'database_errors': 0,
                'critical_errors': 0
            },
            'detailed_error_log': []  # è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼Œä½†ä¸è¿‡åº¦ç»Ÿè®¡
        }

        self.logger.info("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ - å›å½’é«˜æ€§èƒ½ç‰ˆæœ¬")
        self.logger.info(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.base_log_dir}")
        self.logger.info(f"âš™ï¸ æ‰¹å¤„ç†å¤§å°: {self.batch_size:,} (å¯è°ƒä¼˜)")
        self.logger.info(f"ğŸ§µ æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers} (å¯è°ƒä¼˜)")
        self.logger.info(f"ğŸ”— è¿æ¥æ± å¤§å°: {self.connection_pool_size} (å¯è°ƒä¼˜)")
        self.logger.info(f"ğŸ“‹ è¯¦ç»†æ—¥å¿—: {'å¯ç”¨' if self.enable_detailed_logging else 'ç¦ç”¨'}")

        # æ€§èƒ½å»ºè®®
        if self.max_workers > 8:
            self.logger.warning(f"âš ï¸  çº¿ç¨‹æ•° {self.max_workers} è¾ƒé«˜ï¼Œå»ºè®®ç›‘æ§CPUä½¿ç”¨ç‡")
        if self.batch_size > 5000:
            self.logger.info(f"ğŸ’¡ å¤§æ‰¹é‡æ¨¡å¼ ({self.batch_size:,})ï¼Œç¡®ä¿å†…å­˜å……è¶³")

    def _start_async_write_threads(self):
        """å¯åŠ¨å¼‚æ­¥å†™å…¥çº¿ç¨‹æ± """
        if self.enable_async_writes:
            self.write_thread_pool = ThreadPoolExecutor(
                max_workers=self.max_concurrent_writes,
                thread_name_prefix="AsyncWriter"
            )

            # ä¸ºå¼‚æ­¥å†™å…¥çº¿ç¨‹åˆ›å»ºä¸“ç”¨å†™å…¥å™¨æ± 
            self.async_writer_pool = []
            for i in range(self.max_concurrent_writes):
                try:
                    async_writer = DWDWriter()
                    if async_writer.connect():
                        self.async_writer_pool.append(async_writer)
                        self.logger.info(f"âœ… å¼‚æ­¥å†™å…¥å™¨ {i+1} è¿æ¥æˆåŠŸ")
                    else:
                        self.logger.error(f"âŒ å¼‚æ­¥å†™å…¥å™¨ {i+1} è¿æ¥å¤±è´¥")
                except Exception as e:
                    self.logger.error(f"âŒ åˆ›å»ºå¼‚æ­¥å†™å…¥å™¨ {i+1} å¤±è´¥: {e}")

            self.logger.info(f"ğŸš€ å¯åŠ¨å¼‚æ­¥å†™å…¥çº¿ç¨‹æ± : {self.max_concurrent_writes}ä¸ªçº¿ç¨‹, {len(self.async_writer_pool)}ä¸ªä¸“ç”¨å†™å…¥å™¨")

    def _init_connection_pool(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ±  - ç®€åŒ–ç‰ˆæœ¬"""
        self.logger.info("ğŸ”— åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        success_count = 0

        for i in range(self.connection_pool_size):
            try:
                writer = DWDWriter()
                if writer.connect():
                    self.writer_pool.append(writer)
                    success_count += 1
                    if self.enable_detailed_logging:
                        self.logger.info(f"âœ… è¿æ¥ {i+1} å»ºç«‹æˆåŠŸ")
                else:
                    self._log_error('critical_errors', f"è¿æ¥ {i+1} å»ºç«‹å¤±è´¥", {
                        'connection_id': i+1,
                        'type': 'connection_failure'
                    })
            except Exception as e:
                self._log_error('critical_errors', f"è¿æ¥ {i+1} åˆå§‹åŒ–å¼‚å¸¸: {str(e)}", {
                    'connection_id': i+1,
                    'exception_type': type(e).__name__,
                    'traceback': traceback.format_exc() if self.enable_detailed_logging else None
                })

        if not self.writer_pool:
            error_msg = "æ— æ³•å»ºç«‹ä»»ä½•æ•°æ®åº“è¿æ¥"
            self._log_error('critical_errors', error_msg)
            raise RuntimeError(f"âŒ {error_msg}")

        self.logger.info(f"ğŸ”— è¿æ¥æ± åˆå§‹åŒ–å®Œæˆï¼š{success_count}/{self.connection_pool_size} ä¸ªè¿æ¥æˆåŠŸ")

    def get_writer(self) -> Optional[DWDWriter]:
        """ä»è¿æ¥æ± è·å–Writerï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.stats_lock:
            if self.writer_pool:
                return self.writer_pool.pop()
        return None

    def return_writer(self, writer: DWDWriter):
        """å½’è¿˜Writeråˆ°è¿æ¥æ± ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if writer is None:
            return
        with self.stats_lock:
            if len(self.writer_pool) < self.connection_pool_size:
                self.writer_pool.append(writer)

    def cached_ua_parse(self, user_agent: str, mapper: FieldMapper) -> Dict[str, str]:
        """é«˜æ•ˆçš„User-Agentè§£æç¼“å­˜"""
        if not user_agent:
            return {}

        # å‡å°‘é”ç«äº‰ - å…ˆæ£€æŸ¥ç¼“å­˜
        with self.stats_lock:
            self.cache_hit_stats['total_requests'] += 1
            if user_agent in self.ua_cache:
                self.cache_hit_stats['ua_hits'] += 1
                return self.ua_cache[user_agent]

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè§£æ
        try:
            result = mapper._parse_user_agent(user_agent)
        except Exception as e:
            if self.enable_detailed_logging:
                self._log_error('mapping_errors', f"User-Agentè§£æå¤±è´¥: {str(e)}", {
                    'user_agent': user_agent[:100],
                    'exception_type': type(e).__name__
                })
            result = {}

        # æ›´æ–°ç¼“å­˜
        with self.stats_lock:
            if len(self.ua_cache) < 5000:  # é™åˆ¶ç¼“å­˜å¤§å°
                self.ua_cache[user_agent] = result

        return result

    def cached_uri_parse(self, uri: str, mapper: FieldMapper) -> Dict[str, str]:
        """é«˜æ•ˆçš„URIç»“æ„è§£æç¼“å­˜"""
        if not uri:
            return {}

        with self.stats_lock:
            if uri in self.uri_cache:
                self.cache_hit_stats['uri_hits'] += 1
                return self.uri_cache[uri]

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè§£æ
        try:
            result = mapper._parse_uri_structure(uri)
        except Exception as e:
            if self.enable_detailed_logging:
                self._log_error('mapping_errors', f"URIè§£æå¤±è´¥: {str(e)}", {
                    'uri': uri[:200],
                    'exception_type': type(e).__name__
                })
            result = {}

        with self.stats_lock:
            if len(self.uri_cache) < 3000:  # é™åˆ¶ç¼“å­˜å¤§å°
                self.uri_cache[uri] = result

        return result

    def _log_error(self, error_type: str, error_msg: str, context: Dict[str, Any] = None):
        """ç»Ÿä¸€çš„é”™è¯¯æ—¥å¿—æ–¹æ³• - è¯¦ç»†ä½†é«˜æ•ˆ"""
        # æ›´æ–°ç®€åŒ–çš„é”™è¯¯ç»Ÿè®¡
        with self.stats_lock:
            if error_type in self.session_stats['error_stats']:
                self.session_stats['error_stats'][error_type] += 1
            self.session_stats['total_errors'] += 1

            # è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_detailed_logging:
                error_detail = {
                    'timestamp': datetime.now().isoformat(),
                    'type': error_type,
                    'message': error_msg,
                    'context': context or {}
                }
                self.session_stats['detailed_error_log'].append(error_detail)

                # é™åˆ¶è¯¦ç»†æ—¥å¿—å¤§å°
                if len(self.session_stats['detailed_error_log']) > 500:
                    self.session_stats['detailed_error_log'] = self.session_stats['detailed_error_log'][-250:]

        # è®°å½•åˆ°æ ‡å‡†æ—¥å¿—
        if error_type == 'critical_errors':
            self.logger.error(f"âŒ [{error_type}] {error_msg}")
        elif self.enable_detailed_logging:
            self.logger.warning(f"âš ï¸ [{error_type}] {error_msg}")

    def _optimized_batch_write(self, data_batch: List[Dict[str, Any]], writer: DWDWriter) -> bool:
        """ä¼˜åŒ–çš„æ‰¹é‡å†™å…¥æ–¹æ³• - ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æ•°æ®åº“å†™å…¥ä¼˜åŒ–æŠ€æœ¯"""
        if not data_batch:
            return True

        start_time = time.time()

        try:
            # ä¼˜åŒ–1: é¢„å¤„ç†å’ŒéªŒè¯æ•°æ®
            validated_batch = []
            for record in data_batch:
                if self._validate_record(record):
                    validated_batch.append(record)

            if not validated_batch:
                return True

            # ä¼˜åŒ–2: ä½¿ç”¨ClickHouseä¼˜åŒ–çš„æ‰¹é‡æ’å…¥
            success = writer.batch_write_optimized(validated_batch)

            if success:
                write_time = time.time() - start_time
                with self.write_buffer_lock:
                    self.write_stats['total_writes'] += 1
                    self.write_stats['total_records'] += len(validated_batch)
                    self.write_stats['total_write_time'] += write_time

                return True
            else:
                self._log_error('database_errors', f"æ‰¹é‡å†™å…¥å¤±è´¥: {len(validated_batch)} è®°å½•")
                return False

        except Exception as e:
            self._log_error('database_errors', f"æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")
            return False

    def _validate_record(self, record: Dict[str, Any]) -> bool:
        """å¿«é€ŸéªŒè¯è®°å½•çš„æœ‰æ•ˆæ€§"""
        return (
            record and
            isinstance(record, dict) and
            'log_time' in record and
            'client_ip' in record
        )

    def _async_write_worker(self, data_batch: List[Dict[str, Any]]):
        """å¼‚æ­¥å†™å…¥å·¥ä½œçº¿ç¨‹"""
        max_retries = 3
        retry_delay = 0.1

        for attempt in range(max_retries):
            try:
                # ä¼˜å…ˆä½¿ç”¨ä¸“ç”¨å¼‚æ­¥å†™å…¥å™¨ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„åˆ†é…
                if hasattr(self, 'async_writer_pool') and self.async_writer_pool:
                    import time
                    # ä½¿ç”¨æ—¶é—´æˆ³å’Œçº¿ç¨‹IDç¡®ä¿æ›´å¥½çš„åˆ†å¸ƒ
                    thread_id = threading.current_thread().ident or 0
                    time_factor = int(time.time() * 1000) % 1000  # æ¯«ç§’çº§æ—¶é—´å› å­
                    writer_index = (thread_id + time_factor) % len(self.async_writer_pool)
                    async_writer = self.async_writer_pool[writer_index]

                    # æ£€æŸ¥å†™å…¥å™¨è¿æ¥çŠ¶æ€
                    if not async_writer or not async_writer.client:
                        self.logger.warning(f"å¼‚æ­¥å†™å…¥å™¨ {writer_index} è¿æ¥ä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¸»å†™å…¥å™¨æ± ")
                    else:
                        return self._optimized_batch_write(data_batch, async_writer)

                # å›é€€åˆ°ä¸»å†™å…¥å™¨æ± 
                if not self._ensure_writer_pool_available():
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        continue
                    self._log_error('database_errors', "æ— å¯ç”¨çš„å†™å…¥è¿æ¥")
                    return False

                # çº¿ç¨‹å®‰å…¨åœ°è·å–å†™å…¥å™¨
                with self.stats_lock:
                    if not self.writer_pool:
                        if attempt < max_retries - 1:
                            time.sleep(retry_delay)
                            continue
                        self._log_error('database_errors', "æ— å¯ç”¨çš„å†™å…¥è¿æ¥")
                        return False

                    # è·å–å†™å…¥å™¨ï¼ˆè½®è¯¢ç­–ç•¥ï¼‰
                    thread_id = threading.current_thread().ident or 0
                    writer_index = thread_id % len(self.writer_pool)
                    writer = self.writer_pool[writer_index]

                # åœ¨é”å¤–æ‰§è¡Œå†™å…¥æ“ä½œ
                return self._optimized_batch_write(data_batch, writer)

            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    continue
                self._log_error('database_errors', f"å¼‚æ­¥å†™å…¥å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
                return False

        return False

    def _flush_write_buffer(self, force: bool = False):
        """åˆ·æ–°å†™å…¥ç¼“å†²åŒº"""
        with self.write_buffer_lock:
            current_time = time.time()
            should_flush = (
                force or
                len(self.write_buffer) >= self.write_buffer_size or
                (self.write_buffer and
                 current_time - self.last_buffer_flush > self.delayed_commit_seconds)
            )

            if should_flush and self.write_buffer:
                buffer_to_flush = self.write_buffer.copy()
                self.write_buffer.clear()
                self.last_buffer_flush = current_time
                self.write_stats['buffer_flushes'] += 1

                # æ£€æŸ¥å†™å…¥å™¨æ± çŠ¶æ€ - æ£€æŸ¥ä¸»æ± æˆ–å¼‚æ­¥æ± 
                has_main_writers = bool(self.writer_pool)
                has_async_writers = hasattr(self, 'async_writer_pool') and bool(self.async_writer_pool)

                if not has_main_writers and not has_async_writers:
                    self._log_error('database_errors', "åˆ·æ–°ç¼“å†²åŒºæ—¶æ— å¯ç”¨çš„å†™å…¥è¿æ¥")
                    return False

                # å¼‚æ­¥æäº¤å†™å…¥
                if (self.enable_async_writes and
                    self.write_thread_pool and
                    not self.shutdown_event.is_set() and
                    has_async_writers):
                    try:
                        future = self.write_thread_pool.submit(self._async_write_worker, buffer_to_flush)
                        self.write_stats['async_writes'] += 1
                        return future
                    except Exception as e:
                        self._log_error('database_errors', f"å¼‚æ­¥å†™å…¥æäº¤å¤±è´¥: {e}")
                        # é™çº§åˆ°åŒæ­¥å†™å…¥
                        if has_main_writers:
                            writer = self.writer_pool[0]
                            return self._optimized_batch_write(buffer_to_flush, writer)
                        else:
                            return False
                else:
                    # åŒæ­¥å†™å…¥
                    if has_main_writers:
                        writer = self.writer_pool[0]
                        return self._optimized_batch_write(buffer_to_flush, writer)
                    elif has_async_writers:
                        # ä½¿ç”¨å¼‚æ­¥å†™å…¥å™¨è¿›è¡ŒåŒæ­¥å†™å…¥
                        async_writer = self.async_writer_pool[0]
                        return self._optimized_batch_write(buffer_to_flush, async_writer)
                    else:
                        self._log_error('database_errors', "åŒæ­¥å†™å…¥æ—¶æ— å¯ç”¨çš„å†™å…¥è¿æ¥")
                        return False

        return True

    def _add_to_write_buffer(self, data_batch: List[Dict[str, Any]]):
        """æ·»åŠ æ•°æ®åˆ°å†™å…¥ç¼“å†²åŒº"""
        if not data_batch:
            return

        with self.write_buffer_lock:
            self.write_buffer.extend(data_batch)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
        self._flush_write_buffer()

    def _ensure_writer_pool_available(self) -> bool:
        """ç¡®ä¿å†™å…¥å™¨æ± å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å°è¯•é‡æ–°åˆå§‹åŒ–"""
        if self.writer_pool:
            return True

        self.logger.warning("å†™å…¥å™¨æ± ä¸ºç©ºï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–...")
        try:
            # é‡æ–°åˆå§‹åŒ–è¿æ¥æ± 
            success_count = 0
            for i in range(min(2, self.connection_pool_size)):  # è‡³å°‘åˆ›å»º2ä¸ªè¿æ¥
                try:
                    writer = DWDWriter()
                    if writer.connect():
                        self.writer_pool.append(writer)
                        success_count += 1
                except Exception as e:
                    self.logger.error(f"é‡æ–°åˆ›å»ºè¿æ¥ {i+1} å¤±è´¥: {e}")

            if success_count > 0:
                self.logger.info(f"æˆåŠŸé‡æ–°åˆ›å»º {success_count} ä¸ªæ•°æ®åº“è¿æ¥")
                return True
            else:
                self.logger.error("æ— æ³•é‡æ–°åˆ›å»ºä»»ä½•æ•°æ®åº“è¿æ¥")
                return False

        except Exception as e:
            self.logger.error(f"é‡æ–°åˆå§‹åŒ–å†™å…¥å™¨æ± å¤±è´¥: {e}")
            return False

    def process_file_batch(self, file_paths: List[Path], thread_id: int,
                          test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """
        é«˜æ€§èƒ½æ–‡ä»¶æ‰¹é‡å¤„ç† - ç§»é™¤è¿‡åº¦ç›‘æ§ï¼Œä¿ç•™å…³é”®é”™è¯¯æ—¥å¿—
        """
        start_time = time.time()

        # è·å–çº¿ç¨‹ä¸“ç”¨ç»„ä»¶
        parser = self.parser_pool[thread_id % len(self.parser_pool)]
        mapper = self.mapper_pool[thread_id % len(self.mapper_pool)]
        writer = None

        if not test_mode:
            writer = self.get_writer()
            if not writer:
                error_msg = f"çº¿ç¨‹ {thread_id} æ— æ³•è·å–æ•°æ®åº“è¿æ¥"
                self._log_error('critical_errors', error_msg, {'thread_id': thread_id})
                return {
                    'success': False,
                    'error': error_msg,
                    'thread_id': thread_id
                }

        try:
            batch_stats = {
                'thread_id': thread_id,
                'total_files': len(file_paths),
                'processed_files': 0,
                'total_records': 0,
                'total_lines': 0,
                'errors': []
            }

            # é«˜æ€§èƒ½æ‰¹é‡ç¼“å†²åŒº
            mega_batch = []

            for file_path in file_paths:
                if self.enable_detailed_logging:
                    self.logger.info(f"ğŸ§µ{thread_id} å¤„ç†æ–‡ä»¶: {file_path.name}")

                file_start = time.time()
                file_records = 0
                file_lines = 0

                try:
                    # æµå¼å¤„ç†æ–‡ä»¶ - æ ¸å¿ƒé«˜æ€§èƒ½å¾ªç¯
                    for parsed_data in parser.parse_file(file_path):
                        file_lines += 1

                        if parsed_data:
                            try:
                                # é«˜æ•ˆç¼“å­˜æ˜ å°„
                                user_agent = parsed_data.get('user_agent', '')
                                uri = parsed_data.get('request', '').split(' ')[1] if 'request' in parsed_data else ''

                                # ä½¿ç”¨ç¼“å­˜
                                if user_agent:
                                    parsed_data['_cached_ua'] = self.cached_ua_parse(user_agent, mapper)
                                if uri:
                                    parsed_data['_cached_uri'] = self.cached_uri_parse(uri, mapper)

                                # å­—æ®µæ˜ å°„
                                mapped_data = mapper.map_to_dwd(parsed_data, file_path.name)
                                mega_batch.append(mapped_data)
                                file_records += 1

                                # åŠ¨æ€æ‰¹é‡å†™å…¥æ£€æŸ¥ - ä½¿ç”¨åŠ¨æ€ä¼˜åŒ–çš„æ‰¹å¤§å°
                                current_batch_size = self.batch_optimizer.get_current_batch_size()
                                if len(mega_batch) >= current_batch_size:
                                    if not test_mode:
                                        # è®°å½•æ‰¹å¤„ç†æ€§èƒ½å¼€å§‹æ—¶é—´
                                        batch_start_time = time.time()

                                        # ä½¿ç”¨ä¼˜åŒ–çš„ç¼“å†²å†™å…¥ç³»ç»Ÿ
                                        self._add_to_write_buffer(mega_batch.copy())

                                        # è®°å½•æ‰¹å¤„ç†æ€§èƒ½
                                        batch_duration = time.time() - batch_start_time
                                        self.batch_optimizer.record_batch_performance(
                                            current_batch_size,
                                            len(mega_batch),
                                            batch_duration
                                        )

                                        # å°è¯•ä¼˜åŒ–æ‰¹å¤§å°
                                        new_batch_size, reason = self.batch_optimizer.optimize_batch_size()
                                        if new_batch_size != current_batch_size:
                                            self.logger.info(f"ğŸ”§ æ‰¹å¤§å°ä¼˜åŒ–: {current_batch_size} -> {new_batch_size}, åŸå› : {reason}")

                                    mega_batch.clear()
                                    gc.collect()  # å†…å­˜ä¼˜åŒ–

                                # æ£€æŸ¥é™åˆ¶
                                if limit and file_records >= limit:
                                    break

                            except Exception as e:
                                # è¯¦ç»†é”™è¯¯å®šä½ - å…³é”®æ”¹è¿›
                                error_msg = f"è®°å½•å¤„ç†é”™è¯¯ (è¡Œ {file_lines}): {str(e)}"
                                error_context = {
                                    'file_name': file_path.name,
                                    'line_number': file_lines,
                                    'thread_id': thread_id,
                                    'exception_type': type(e).__name__
                                }
                                if self.enable_detailed_logging:
                                    error_context['traceback'] = traceback.format_exc()
                                    error_context['parsed_data_keys'] = list(parsed_data.keys()) if hasattr(parsed_data, 'keys') else str(type(parsed_data))

                                self._log_error('mapping_errors', error_msg, error_context)

                except Exception as e:
                    error_msg = f"æ–‡ä»¶å¤„ç†å¼‚å¸¸ {file_path.name}: {str(e)}"
                    self._log_error('parsing_errors', error_msg, {
                        'file_name': file_path.name,
                        'thread_id': thread_id,
                        'exception_type': type(e).__name__,
                        'traceback': traceback.format_exc() if self.enable_detailed_logging else None
                    })
                    batch_stats['errors'].append(error_msg)

                file_duration = time.time() - file_start
                batch_stats['processed_files'] += 1
                batch_stats['total_records'] += file_records
                batch_stats['total_lines'] += file_lines

                # æ€§èƒ½åé¦ˆ
                if self.enable_detailed_logging and file_records > 0:
                    speed = file_records / file_duration if file_duration > 0 else 0
                    self.logger.info(f"ğŸ§µ{thread_id} å®Œæˆ {file_path.name}: {file_records} è®°å½•, {speed:.1f} rec/s")

            # å¤„ç†å‰©ä½™æ‰¹æ¬¡ - ä½¿ç”¨ä¼˜åŒ–çš„ç¼“å†²å†™å…¥
            if mega_batch and not test_mode:
                # ä½¿ç”¨ä¼˜åŒ–çš„ç¼“å†²å†™å…¥ç³»ç»Ÿå¤„ç†å‰©ä½™æ•°æ®
                self._add_to_write_buffer(mega_batch)
                # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºä»¥ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è¢«å†™å…¥
                self._flush_write_buffer(force=True)

            batch_stats['duration'] = time.time() - start_time
            batch_stats['success'] = len(batch_stats['errors']) == 0

            return batch_stats

        finally:
            # å½’è¿˜è¿æ¥åˆ°æ± 
            if writer:
                self.return_writer(writer)

    def process_date_parallel(self, date_str: str, force_reprocess: bool = False,
                             test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """
        å¹¶è¡Œå¤„ç†æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰æ—¥å¿— - é«˜æ€§èƒ½ç‰ˆæœ¬
        """
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {date_str} çš„æ—¥å¿—")
        self.session_stats['start_time'] = datetime.now()
        start_time = time.time()

        # æ£€æŸ¥æ—¥æœŸç›®å½•
        date_dir = self.base_log_dir / date_str
        if not date_dir.exists():
            error_msg = f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date_dir}'
            self._log_error('critical_errors', error_msg, {'date': date_str})
            return {'success': False, 'error': error_msg, 'date': date_str}

        # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        try:
            all_log_files = list(date_dir.glob("*.log"))
        except Exception as e:
            error_msg = f'æ‰«ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}'
            self._log_error('critical_errors', error_msg, {'date': date_str, 'directory': str(date_dir)})
            return {'success': False, 'error': error_msg, 'date': date_str}

        if not all_log_files:
            return {'success': True, 'processed_files': 0, 'message': 'æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶', 'date': date_str}

        # è¿‡æ»¤éœ€è¦å¤„ç†çš„æ–‡ä»¶
        if not force_reprocess:
            pending_files = [f for f in all_log_files if not self.is_file_processed(f)]
        else:
            pending_files = all_log_files

        if not pending_files:
            self.logger.info(f"ğŸ“‹ æ—¥æœŸ {date_str} çš„æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†")
            return {'success': True, 'processed_files': 0, 'message': 'æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†', 'date': date_str}

        self.logger.info(f"ğŸ“ æ‰¾åˆ° {len(pending_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")

        # é«˜æ•ˆçš„æ–‡ä»¶åˆ†æ‰¹ç­–ç•¥
        files_per_thread = max(1, len(pending_files) // self.max_workers)
        file_batches = []

        for i in range(0, len(pending_files), files_per_thread):
            batch = pending_files[i:i + files_per_thread]
            if batch:
                file_batches.append(batch)

        actual_workers = min(self.max_workers, len(file_batches))
        self.logger.info(f"ğŸ§µ åˆ†é… {len(file_batches)} ä¸ªæ‰¹æ¬¡ç»™ {actual_workers} ä¸ªçº¿ç¨‹")

        # é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†
        all_results = []
        total_records = 0
        total_errors = []

        try:
            with ThreadPoolExecutor(max_workers=actual_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_batch = {}
                for i, batch in enumerate(file_batches):
                    future = executor.submit(self.process_file_batch, batch, i, test_mode, limit)
                    future_to_batch[future] = i

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_batch):
                    batch_id = future_to_batch[future]
                    try:
                        result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                        all_results.append(result)

                        if result['success']:
                            total_records += result['total_records']
                            if self.enable_detailed_logging:
                                self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ: {result['processed_files']} æ–‡ä»¶, "
                                               f"{result['total_records']} è®°å½•")
                        else:
                            total_errors.extend(result.get('errors', []))
                            self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_id} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    except Exception as e:
                        error_msg = f"æ‰¹æ¬¡ {batch_id} æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                        self._log_error('critical_errors', error_msg, {
                            'batch_id': batch_id,
                            'exception_type': type(e).__name__
                        })
                        total_errors.append(error_msg)

        except Exception as e:
            error_msg = f"çº¿ç¨‹æ± æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            self._log_error('critical_errors', error_msg, {'date': date_str})
            return {'success': False, 'error': error_msg, 'date': date_str}

        # æ›´æ–°å¤„ç†çŠ¶æ€
        if not test_mode:
            for file_path in pending_files:
                self.mark_file_processed(file_path, 0, 0)
            self.save_state()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        cache_hit_rate = 0.0
        if self.cache_hit_stats['total_requests'] > 0:
            total_hits = self.cache_hit_stats['ua_hits'] + self.cache_hit_stats['uri_hits']
            cache_hit_rate = (total_hits / (self.cache_hit_stats['total_requests'] * 2)) * 100

        duration = time.time() - start_time
        speed = total_records / duration if duration > 0 else 0

        # æ›´æ–°ä¼šè¯ç»Ÿè®¡
        self.session_stats['end_time'] = datetime.now()
        self.session_stats['avg_processing_speed'] = speed
        self.session_stats['cache_hit_rate'] = cache_hit_rate
        self.session_stats['total_records_written'] = total_records

        success = len(total_errors) == 0

        result = {
            'success': success,
            'date': date_str,
            'processed_files': sum(r.get('processed_files', 0) for r in all_results),
            'total_records': total_records,
            'duration': duration,
            'processing_speed': speed,
            'cache_hit_rate': cache_hit_rate,
            'errors': total_errors
        }

        if success:
            self.logger.info(f"ğŸ‰ æ—¥æœŸ {date_str} é«˜æ€§èƒ½å¤„ç†å®Œæˆ!")
            self.logger.info(f"ğŸ“Š {result['processed_files']} æ–‡ä»¶, {total_records:,} è®°å½•")
            self.logger.info(f"â±ï¸  è€—æ—¶ {duration:.2f}s, é€Ÿåº¦ {speed:.1f} è®°å½•/ç§’")
            self.logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")

        return result

    # === ç»§æ‰¿çš„æ ¸å¿ƒæ–¹æ³• ===

    def process_all_parallel(self, test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—"""
        self.logger.info("ğŸš€ å¼€å§‹é«˜æ€§èƒ½å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—")
        start_time = time.time()

        log_files_by_date = self.scan_log_directories()
        if not log_files_by_date:
            return {'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶'}

        total_records = 0
        processed_dates = 0
        all_errors = []

        for date_str in sorted(log_files_by_date.keys()):
            result = self.process_date_parallel(date_str, force_reprocess=False,
                                              test_mode=test_mode, limit=limit)
            if result['success'] and result.get('processed_files', 0) > 0:
                processed_dates += 1
                total_records += result['total_records']
            else:
                if result.get('errors'):
                    all_errors.extend(result['errors'])

        duration = time.time() - start_time
        overall_speed = total_records / duration if duration > 0 else 0

        return {
            'success': len(all_errors) == 0,
            'processed_dates': processed_dates,
            'total_records': total_records,
            'duration': duration,
            'processing_speed': overall_speed,
            'errors': all_errors
        }

    def load_state(self) -> Dict[str, Any]:
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self._log_error('critical_errors', f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

        return {
            'processed_files': {},
            'last_update': None,
            'total_processed_records': 0,
            'processing_history': []
        }

    def save_state(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        try:
            self.processed_state['last_update'] = datetime.now().isoformat()
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_state, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            self._log_error('critical_errors', f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def scan_log_directories(self) -> Dict[str, List[Path]]:
        """æ‰«ææ—¥å¿—ç›®å½•"""
        if not self.base_log_dir.exists():
            return {}

        log_files_by_date = {}
        for date_dir in self.base_log_dir.iterdir():
            if date_dir.is_dir() and date_dir.name.isdigit() and len(date_dir.name) == 8:
                try:
                    datetime.strptime(date_dir.name, '%Y%m%d')
                    log_files = list(date_dir.glob("*.log"))
                    if log_files:
                        log_files_by_date[date_dir.name] = sorted(log_files)
                except ValueError:
                    continue

        return log_files_by_date

    def is_file_processed(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        file_key = str(file_path)
        return file_key in self.processed_state.get('processed_files', {})

    def mark_file_processed(self, file_path: Path, record_count: int, processing_time: float):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        file_key = str(file_path)
        try:
            self.processed_state['processed_files'][file_key] = {
                'processed_at': datetime.now().isoformat(),
                'record_count': record_count,
                'processing_time': processing_time,
                'mtime': file_path.stat().st_mtime,
                'file_size': file_path.stat().st_size
            }
        except Exception as e:
            self._log_error('critical_errors', f"æ ‡è®°æ–‡ä»¶çŠ¶æ€å¤±è´¥ {file_path}: {e}")

    def show_performance_stats(self):
        """æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š (å›å½’é«˜æ€§èƒ½ç‰ˆæœ¬)")
        print("=" * 80)

        print(f"âš™ï¸  é…ç½®ä¿¡æ¯:")
        print(f"   åˆå§‹æ‰¹å¤„ç†å¤§å°: {self.initial_batch_size:,}")
        print(f"   å½“å‰æ‰¹å¤„ç†å¤§å°: {self.batch_optimizer.get_current_batch_size():,} (åŠ¨æ€ä¼˜åŒ–)")
        print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers} (å¯è°ƒä¼˜)")
        print(f"   è¿æ¥æ± å¤§å°: {self.connection_pool_size} (å¯è°ƒä¼˜)")
        print(f"   è¯¦ç»†æ—¥å¿—: {'å¯ç”¨' if self.enable_detailed_logging else 'ç¦ç”¨'}")

        # æ˜¾ç¤ºåŠ¨æ€æ‰¹å¤§å°ä¼˜åŒ–å™¨ç»Ÿè®¡
        optimizer_stats = self.batch_optimizer.get_performance_stats()
        if optimizer_stats:
            print(f"\nğŸ”§  åŠ¨æ€æ‰¹å¤§å°ä¼˜åŒ–ç»Ÿè®¡:")
            print(f"   æµ‹é‡æ¬¡æ•°: {optimizer_stats.get('total_measurements', 0)}")
            print(f"   å¹³å‡ååé‡: {optimizer_stats.get('avg_throughput', 0):.1f} è®°å½•/ç§’")
            print(f"   å³°å€¼ååé‡: {optimizer_stats.get('max_throughput', 0):.1f} è®°å½•/ç§’")
            print(f"   è¿ç»­è‰¯å¥½æ€§èƒ½: {optimizer_stats.get('consecutive_good_performance', 0)}")
            print(f"   è¿ç»­ä¸ä½³æ€§èƒ½: {optimizer_stats.get('consecutive_bad_performance', 0)}")

            if 'avg_memory_usage' in optimizer_stats:
                print(f"   å¹³å‡å†…å­˜ä½¿ç”¨: {optimizer_stats['avg_memory_usage']*100:.1f}%")
            if 'avg_cpu_usage' in optimizer_stats:
                print(f"   å¹³å‡CPUä½¿ç”¨: {optimizer_stats['avg_cpu_usage']*100:.1f}%")

        print(f"\nğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:")
        print(f"   User-Agentç¼“å­˜: {len(self.ua_cache)} é¡¹")
        print(f"   URIç¼“å­˜: {len(self.uri_cache)} é¡¹")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {self.session_stats.get('cache_hit_rate', 0):.1f}%")

        if self.session_stats.get('avg_processing_speed', 0) > 0:
            print(f"\nğŸƒ æ€§èƒ½æŒ‡æ ‡:")
            print(f"   å¹³å‡å¤„ç†é€Ÿåº¦: {self.session_stats['avg_processing_speed']:.1f} è®°å½•/ç§’")
            print(f"   æ€»å¤„ç†è®°å½•æ•°: {self.session_stats.get('total_records_written', 0):,}")

        # ç®€åŒ–çš„é”™è¯¯æ±‡æ€»
        if self.session_stats['total_errors'] > 0:
            print(f"\nâŒ é”™è¯¯æ±‡æ€»:")
            print(f"   æ€»é”™è¯¯æ•°: {self.session_stats['total_errors']}")
            for error_type, count in self.session_stats['error_stats'].items():
                if count > 0:
                    print(f"   {error_type}: {count}")

        print("=" * 80)

    def print_detailed_error_log(self):
        """æ‰“å°è¯¦ç»†é”™è¯¯æ—¥å¿— - å¸®åŠ©å®šä½é—®é¢˜"""
        if not self.enable_detailed_logging:
            print("è¯¦ç»†é”™è¯¯æ—¥å¿—æœªå¯ç”¨ï¼Œè¯·åœ¨åˆå§‹åŒ–æ—¶è®¾ç½® enable_detailed_logging=True")
            return

        error_log = self.session_stats.get('detailed_error_log', [])
        if not error_log:
            print("âœ… æ²¡æœ‰è¯¦ç»†é”™è¯¯è®°å½•")
            return

        print("\n" + "="*80)
        print("ğŸ” è¯¦ç»†é”™è¯¯æ—¥å¿— (æœ€è¿‘10ä¸ª)")
        print("="*80)

        for error in error_log[-10:]:
            print(f"\nğŸ“… æ—¶é—´: {error['timestamp']}")
            print(f"ğŸ·ï¸  ç±»å‹: {error['type']}")
            print(f"ğŸ“ æ¶ˆæ¯: {error['message']}")
            if error.get('context'):
                print(f"ğŸ” ä¸Šä¸‹æ–‡: {error['context']}")
            print("-" * 40)

        print("="*80)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")

        # è®¾ç½®å…³é—­äº‹ä»¶
        self.shutdown_event.set()

        # åˆ·æ–°ä»»ä½•å‰©ä½™çš„å†™å…¥ç¼“å†²åŒº
        try:
            self._flush_write_buffer(force=True)
            self.logger.info("âœ… å†™å…¥ç¼“å†²åŒºå·²åˆ·æ–°")
        except Exception as e:
            self.logger.error(f"âŒ åˆ·æ–°å†™å…¥ç¼“å†²åŒºå¤±è´¥: {e}")

        # å…³é—­å¼‚æ­¥å†™å…¥çº¿ç¨‹æ± 
        if self.write_thread_pool:
            try:
                self.write_thread_pool.shutdown(wait=True)
                self.logger.info("âœ… å¼‚æ­¥å†™å…¥çº¿ç¨‹æ± å·²å…³é—­")
            except Exception as e:
                self.logger.error(f"âŒ å…³é—­å¼‚æ­¥å†™å…¥çº¿ç¨‹æ± å¤±è´¥: {e}")

        # å…³é—­ä¸“ç”¨å¼‚æ­¥å†™å…¥å™¨
        if hasattr(self, 'async_writer_pool'):
            for async_writer in self.async_writer_pool:
                try:
                    async_writer.close()
                except:
                    pass
            self.async_writer_pool.clear()

        # å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥
        for writer in self.writer_pool:
            try:
                writer.close()
            except:
                pass
        self.writer_pool.clear()

        # æ¸…ç†ç¼“å­˜
        self.ua_cache.clear()
        self.uri_cache.clear()
        self.write_buffer.clear()

        # æ˜¾ç¤ºå†™å…¥ç»Ÿè®¡
        if self.write_stats['total_writes'] > 0:
            avg_write_time = self.write_stats['total_write_time'] / self.write_stats['total_writes']
            self.logger.info(f"ğŸ“Š å†™å…¥ç»Ÿè®¡: {self.write_stats['total_writes']} æ¬¡å†™å…¥, "
                           f"{self.write_stats['total_records']} è®°å½•, "
                           f"å¹³å‡å†™å…¥æ—¶é—´: {avg_write_time:.3f}s")
            self.logger.info(f"ğŸ“Š ç¼“å†²ç»Ÿè®¡: {self.write_stats['buffer_flushes']} æ¬¡åˆ·æ–°, "
                           f"{self.write_stats['async_writes']} æ¬¡å¼‚æ­¥å†™å…¥")

        gc.collect()
        self.logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

    def _dynamic_batch_optimization_menu(self):
        """åŠ¨æ€æ‰¹å¤§å°ä¼˜åŒ–ç®¡ç†èœå•"""
        while True:
            print("\n" + "=" * 80)
            print("ğŸ”§ åŠ¨æ€æ‰¹å¤§å°ä¼˜åŒ–ç®¡ç†")
            print("=" * 80)

            # æ˜¾ç¤ºå½“å‰çŠ¶æ€
            current_batch_size = self.batch_optimizer.get_current_batch_size()
            optimizer_stats = self.batch_optimizer.get_performance_stats()

            print(f"ğŸ“Š å½“å‰çŠ¶æ€:")
            print(f"   åˆå§‹æ‰¹å¤§å°: {self.initial_batch_size:,}")
            print(f"   å½“å‰æ‰¹å¤§å°: {current_batch_size:,}")
            print(f"   æµ‹é‡æ¬¡æ•°: {optimizer_stats.get('total_measurements', 0)}")

            if optimizer_stats.get('avg_throughput', 0) > 0:
                print(f"   å¹³å‡ååé‡: {optimizer_stats['avg_throughput']:.1f} è®°å½•/ç§’")
                print(f"   å³°å€¼ååé‡: {optimizer_stats['max_throughput']:.1f} è®°å½•/ç§’")

            print(f"\nğŸ“‹ ç®¡ç†é€‰é¡¹:")
            print("1. ğŸ“ˆ æŸ¥çœ‹è¯¦ç»†ä¼˜åŒ–ç»Ÿè®¡")
            print("2. ğŸ”„ æ‰‹åŠ¨è§¦å‘ä¼˜åŒ–")
            print("3. ğŸ¯ å¼ºåˆ¶è®¾ç½®æ‰¹å¤§å°")
            print("4. ğŸ”„ é‡ç½®ä¼˜åŒ–å™¨")
            print("5. ğŸ’¡ è·å–ç³»ç»Ÿæ¨èæ‰¹å¤§å°")
            print("0. ğŸ”™ è¿”å›ä¸»èœå•")

            try:
                choice = input("è¯·é€‰æ‹©æ“ä½œ [0-5]: ").strip()

                if choice == '0':
                    break

                elif choice == '1':
                    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                    print("\nğŸ“ˆ è¯¦ç»†ä¼˜åŒ–ç»Ÿè®¡:")
                    for key, value in optimizer_stats.items():
                        if isinstance(value, float):
                            if 'usage' in key:
                                print(f"   {key}: {value*100:.1f}%")
                            else:
                                print(f"   {key}: {value:.2f}")
                        else:
                            print(f"   {key}: {value}")

                elif choice == '2':
                    # æ‰‹åŠ¨è§¦å‘ä¼˜åŒ–
                    print("\nğŸ”„ æ‰‹åŠ¨è§¦å‘æ‰¹å¤§å°ä¼˜åŒ–...")
                    new_size, reason = self.batch_optimizer.optimize_batch_size()
                    print(f"ä¼˜åŒ–ç»“æœ: {current_batch_size} -> {new_size}")
                    print(f"ä¼˜åŒ–åŸå› : {reason}")

                elif choice == '3':
                    # å¼ºåˆ¶è®¾ç½®æ‰¹å¤§å°
                    new_size_input = input(f"è¯·è¾“å…¥æ–°çš„æ‰¹å¤§å° (å½“å‰ {current_batch_size}): ").strip()
                    if new_size_input.isdigit():
                        new_size = int(new_size_input)
                        if 1000 <= new_size <= 100000:
                            self.batch_optimizer.force_batch_size(new_size)
                            print(f"âœ… å·²å¼ºåˆ¶è®¾ç½®æ‰¹å¤§å°ä¸º: {new_size:,}")
                        else:
                            print("âŒ æ‰¹å¤§å°å¿…é¡»åœ¨ 1,000 - 100,000 èŒƒå›´å†…")
                    else:
                        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

                elif choice == '4':
                    # é‡ç½®ä¼˜åŒ–å™¨
                    confirm = input("ç¡®è®¤é‡ç½®ä¼˜åŒ–å™¨ï¼Ÿè¿™å°†æ¸…é™¤æ‰€æœ‰æ€§èƒ½å†å² (y/N): ").strip().lower()
                    if confirm == 'y':
                        self.batch_optimizer.reset_optimizer()
                        print("âœ… ä¼˜åŒ–å™¨å·²é‡ç½®")
                    else:
                        print("âŒ å·²å–æ¶ˆé‡ç½®")

                elif choice == '5':
                    # ç³»ç»Ÿæ¨è
                    print("\nğŸ’¡ ç³»ç»Ÿæ¨èæ‰¹å¤§å°:")
                    system_info = BatchSizeRecommendation.get_system_info()

                    if 'error' not in system_info:
                        recommended_size = BatchSizeRecommendation.recommend_initial_batch_size(
                            system_info['available_memory_gb']
                        )
                        print(f"   å¯ç”¨å†…å­˜: {system_info['available_memory_gb']:.1f} GB")
                        print(f"   CPUæ ¸å¿ƒæ•°: {system_info['cpu_count']}")
                        print(f"   æ¨èæ‰¹å¤§å°: {recommended_size:,}")

                        apply = input("æ˜¯å¦åº”ç”¨æ¨èçš„æ‰¹å¤§å°ï¼Ÿ (y/N): ").strip().lower()
                        if apply == 'y':
                            self.batch_optimizer.force_batch_size(recommended_size)
                            print(f"âœ… å·²åº”ç”¨æ¨èæ‰¹å¤§å°: {recommended_size:,}")
                    else:
                        print(f"âŒ æ— æ³•è·å–ç³»ç»Ÿä¿¡æ¯: {system_info['error']}")

                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")

                input("\næŒ‰å›è½¦é”®ç»§ç»­...")

            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ æ“ä½œå¤±è´¥: {e}")

    # === äº¤äº’å¼é…ç½®èœå• ===

    def interactive_menu(self):
        """äº¤äº’å¼èœå• - å¢åŠ é…ç½®è°ƒä¼˜é€‰é¡¹"""
        while True:
            print("\n" + "=" * 80)
            print("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å›å½’é«˜æ€§èƒ½ç‰ˆæœ¬")
            print("=" * 80)
            print("1. ğŸ”¥ å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—")
            print("2. ğŸ“… å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ—¥å¿—")
            print("3. ğŸ“Š æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡")
            print("4. ğŸ§ª æµ‹è¯•æ¨¡å¼å¤„ç†")
            print("5. âš™ï¸ æ€§èƒ½å‚æ•°è°ƒä¼˜")
            print("6. ğŸ” æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—")
            print("7. ğŸ”§ åŠ¨æ€æ‰¹å¤§å°ä¼˜åŒ–ç®¡ç†")
            print("0. ğŸ‘‹ é€€å‡º")
            print("-" * 80)
            current_batch_size = self.batch_optimizer.get_current_batch_size()
            print(f"ğŸ“Š å½“å‰é…ç½®: æ‰¹é‡{current_batch_size} (åŠ¨æ€) | çº¿ç¨‹{self.max_workers} | è¿æ¥æ± {self.connection_pool_size}")

            try:
                choice = input("è¯·é€‰æ‹©æ“ä½œ [0-7]: ").strip()

                if choice == '0':
                    print("ğŸ‘‹ å†è§ï¼")
                    break

                elif choice == '1':
                    print("\nğŸ”¥ é«˜æ€§èƒ½å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—...")
                    limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None

                    result = self.process_all_parallel(test_mode=False, limit=limit)
                    if result['success']:
                        print(f"\nâœ… å¤„ç†å®Œæˆ: {result['total_records']:,} è®°å½•, é€Ÿåº¦ {result['processing_speed']:.1f} rec/s")
                    else:
                        print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                elif choice == '2':
                    date_str = input("\nè¯·è¾“å…¥æ—¥æœŸ (YYYYMMDDæ ¼å¼): ").strip()
                    if len(date_str) != 8 or not date_str.isdigit():
                        print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯")
                        continue

                    force = input("å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Ÿ(y/N): ").strip().lower() == 'y'
                    limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None

                    result = self.process_date_parallel(date_str, force, test_mode=False, limit=limit)
                    if result['success']:
                        print(f"\nâœ… å¤„ç†å®Œæˆ: {result['total_records']:,} è®°å½•, é€Ÿåº¦ {result['processing_speed']:.1f} rec/s")
                    else:
                        print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                elif choice == '3':
                    self.show_performance_stats()

                elif choice == '4':
                    date_str = input("\nè¯·è¾“å…¥æµ‹è¯•æ—¥æœŸ (YYYYMMDDæ ¼å¼): ").strip()
                    limit_input = input("é™åˆ¶å¤„ç†è¡Œæ•° (å»ºè®®100-1000): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else 100

                    result = self.process_date_parallel(date_str, False, test_mode=True, limit=limit)
                    if result['success']:
                        print(f"\nâœ… æµ‹è¯•å®Œæˆ: {result['total_records']:,} è®°å½•, é€Ÿåº¦ {result['processing_speed']:.1f} rec/s")

                elif choice == '5':
                    print("\nâš™ï¸ æ€§èƒ½å‚æ•°è°ƒä¼˜")
                    print(f"å½“å‰é…ç½®:")
                    print(f"  æ‰¹é‡å¤§å°: {self.batch_size}")
                    print(f"  çº¿ç¨‹æ•°: {self.max_workers}")
                    print(f"  è¿æ¥æ± : {self.connection_pool_size}")

                    new_batch = input(f"\næ–°çš„æ‰¹é‡å¤§å° (å½“å‰{self.batch_size}, æ¨è1000-5000): ").strip()
                    new_workers = input(f"æ–°çš„çº¿ç¨‹æ•° (å½“å‰{self.max_workers}, æ¨è2-8): ").strip()
                    new_pool = input(f"æ–°çš„è¿æ¥æ± å¤§å° (å½“å‰{self.connection_pool_size}, æ¨è=çº¿ç¨‹æ•°): ").strip()

                    if new_batch.isdigit():
                        self.batch_size = int(new_batch)
                        print(f"âœ… æ‰¹é‡å¤§å°è°ƒæ•´ä¸º: {self.batch_size}")

                    if new_workers.isdigit():
                        old_workers = self.max_workers
                        self.max_workers = int(new_workers)
                        print(f"âœ… çº¿ç¨‹æ•°è°ƒæ•´ä¸º: {self.max_workers}")

                        # é‡æ–°åˆå§‹åŒ–ç»„ä»¶æ± 
                        if self.max_workers != old_workers:
                            print("ğŸ”„ é‡æ–°åˆå§‹åŒ–ç»„ä»¶æ± ...")
                            self.parser_pool = [BaseLogParser() for _ in range(self.max_workers)]
                            self.mapper_pool = [FieldMapper() for _ in range(self.max_workers)]

                    if new_pool.isdigit():
                        self.connection_pool_size = int(new_pool)
                        print(f"âœ… è¿æ¥æ± å¤§å°è°ƒæ•´ä¸º: {self.connection_pool_size}")
                        print("âš ï¸  è¿æ¥æ± è°ƒæ•´éœ€è¦é‡å¯ç¨‹åºç”Ÿæ•ˆ")

                elif choice == '6':
                    self.print_detailed_error_log()

                elif choice == '7':
                    self._dynamic_batch_optimization_menu()

                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")

                input("\næŒ‰å›è½¦é”®ç»§ç»­...")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ æ“ä½œå¼‚å¸¸: {e}")
                if self.enable_detailed_logging:
                    self._log_error('critical_errors', f"èœå•æ“ä½œå¼‚å¸¸: {e}", {
                        'exception_type': type(e).__name__,
                        'traceback': traceback.format_exc()
                    })

def main():
    """ä¸»å‡½æ•°"""
    import logging

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(
        description='é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å›å½’é«˜æ€§èƒ½ç‰ˆæœ¬ (ç›®æ ‡1200+ rps)'
    )

    parser.add_argument('--date', help='å¤„ç†æŒ‡å®šæ—¥æœŸ (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰æœªå¤„ç†æ—¥å¿—')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--limit', type=int, help='æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°é™åˆ¶')
    parser.add_argument('--batch-size', type=int, default=2000, help='æ‰¹å¤„ç†å¤§å° (å¯è°ƒä¼˜)')
    parser.add_argument('--workers', type=int, default=6, help='å·¥ä½œçº¿ç¨‹æ•° (å¯è°ƒä¼˜)')
    parser.add_argument('--pool-size', type=int, help='è¿æ¥æ± å¤§å° (å¯è°ƒä¼˜ï¼Œé»˜è®¤=çº¿ç¨‹æ•°)')
    parser.add_argument('--detailed-logging', action='store_true', default=True, help='å¯ç”¨è¯¦ç»†é”™è¯¯æ—¥å¿—')

    args = parser.parse_args()

    try:
        with HighPerformanceETLController(
            batch_size=args.batch_size,
            max_workers=args.workers,
            connection_pool_size=args.pool_size,
            enable_detailed_logging=args.detailed_logging
        ) as controller:

            # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºäº¤äº’å¼èœå•
            if not any([args.date, args.all]):
                controller.interactive_menu()
                return

            if args.date:
                result = controller.process_date_parallel(
                    args.date,
                    force_reprocess=args.force,
                    test_mode=args.test,
                    limit=args.limit
                )

                print(f"\nğŸ¯ å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸ: {result['date']}")
                print(f"æ–‡ä»¶: {result.get('processed_files', 0)}")
                print(f"è®°å½•: {result.get('total_records', 0):,}")
                print(f"è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")

            elif args.all:
                result = controller.process_all_parallel(
                    test_mode=args.test,
                    limit=args.limit
                )

                print(f"\nğŸ¯ æ‰¹é‡å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸæ•°: {result.get('processed_dates', 0)}")
                print(f"è®°å½•æ•°: {result.get('total_records', 0):,}")
                print(f"æ€»è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"å¹³å‡é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")

            controller.show_performance_stats()
            controller.print_detailed_error_log()

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()