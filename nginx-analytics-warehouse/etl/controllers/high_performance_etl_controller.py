#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ¶æ„ä¼˜åŒ–ç‰ˆæœ¬
åŸºäºbackup-20250913-105701ç‰ˆæœ¬ï¼Œä¿ç•™æœ‰æ•ˆä¼˜åŒ–ï¼Œç§»é™¤è¿‡åº¦å¤æ‚åŒ–çš„éƒ¨åˆ†

ä¼˜åŒ–ç­–ç•¥ (B. æ¶æ„ä¼˜åŒ–):
1. ä¿ç•™åŸºç¡€å¤šçº¿ç¨‹å’Œæ‰¹å¤„ç†
2. ç®€åŒ–ç¼“å­˜ç­–ç•¥ï¼Œç§»é™¤è¿‡åº¦ä¼˜åŒ–
3. å¢å¼ºé”™è¯¯ä¿¡æ¯è¾“å‡ºå’Œè¯Šæ–­èƒ½åŠ›
4. ç§»é™¤å¤æ‚çš„é¢„ç¼–è¯‘æ­£åˆ™å’Œè¿‡åº¦ä¼˜åŒ–çš„ç»„ä»¶
5. é™ä½æ‰¹å¤„ç†å¤§å°å’Œçº¿ç¨‹æ•°åˆ°åˆç†èŒƒå›´
"""

import sys
import os
import json
import time
import argparse
import threading
import queue
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
import gc
import traceback

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
current_dir = Path(__file__).parent
etl_root = current_dir.parent
sys.path.append(str(etl_root))

from parsers.base_log_parser import BaseLogParser
from processors.field_mapper import FieldMapper
from writers.dwd_writer import DWDWriter

class HighPerformanceETLController:
    """é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ¶æ„ä¼˜åŒ–ç‰ˆæœ¬"""

    def __init__(self,
                 base_log_dir: str = None,
                 state_file: str = None,
                 batch_size: int = 2000,        # æ¢å¤åˆ°åˆç†çš„æ‰¹é‡å¤§å°
                 max_workers: int = 4,          # æ¢å¤åˆ°åˆç†çš„çº¿ç¨‹æ•°
                 connection_pool_size: int = None,
                 memory_limit_mb: int = 512):
        """
        åˆå§‹åŒ–é«˜æ€§èƒ½ETLæ§åˆ¶å™¨

        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆæ¢å¤åˆ°2000ï¼Œå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§ï¼‰
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆæ¢å¤åˆ°4ï¼Œé¿å…èµ„æºç«äº‰ï¼‰
            connection_pool_size: æ•°æ®åº“è¿æ¥æ± å¤§å°
            memory_limit_mb: å†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆMBï¼‰
        """
        # åŸºç¡€é…ç½®
        self.base_log_dir = Path(base_log_dir) if base_log_dir else \
            Path("D:/project/nginx-log-analyzer/nginx-analytics-warehouse/nginx_logs")
        self.state_file = Path(state_file) if state_file else \
            Path(etl_root / "processed_logs_state.json")

        # æ€§èƒ½ä¼˜åŒ–é…ç½® - å›å½’ä¿å®ˆä½†ç¨³å®šçš„è®¾ç½®
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.connection_pool_size = connection_pool_size if connection_pool_size is not None else max_workers
        self.memory_limit_mb = memory_limit_mb

        # æ—¥å¿—é…ç½®
        import logging
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # åˆ›å»ºè¯¦ç»†çš„console handler
        if not self.logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

        # === å¢å¼ºçš„é”™è¯¯ç»Ÿè®¡å’Œè¯Šæ–­ç³»ç»Ÿ ===
        # å¿…é¡»å…ˆåˆå§‹åŒ–è¿™ä¸ªï¼Œå› ä¸ºå…¶ä»–æ–¹æ³•ä¼šç”¨åˆ°
        self.session_stats = {
            'start_time': None,
            'end_time': None,
            'total_files_processed': 0,
            'total_lines_processed': 0,
            'total_records_written': 0,
            'total_errors': 0,
            'cache_hit_rate': 0.0,
            'avg_processing_speed': 0.0,
            'peak_memory_usage_mb': 0,
            'processing_errors': [],
            # === è¯¦ç»†é”™è¯¯ç»Ÿè®¡ ===
            'error_stats': {
                'parsing_errors': 0,        # è§£æé”™è¯¯
                'field_mapping_errors': 0,  # å­—æ®µæ˜ å°„é”™è¯¯
                'database_write_errors': 0, # æ•°æ®åº“å†™å…¥é”™è¯¯
                'connection_errors': 0,     # è¿æ¥é”™è¯¯
                'timeout_errors': 0,        # è¶…æ—¶é”™è¯¯
                'critical_errors': 0,       # è‡´å‘½é”™è¯¯
                'warning_errors': 0,        # è­¦å‘Šçº§é”™è¯¯
                'skipped_lines': 0,         # è·³è¿‡çš„è¡Œæ•°
                'invalid_records': 0,       # æ— æ•ˆè®°å½•æ•°
                'file_access_errors': 0,    # æ–‡ä»¶è®¿é—®é”™è¯¯
                'memory_errors': 0,         # å†…å­˜ç›¸å…³é”™è¯¯
                'thread_errors': 0          # çº¿ç¨‹ç›¸å…³é”™è¯¯
            },
            'error_details': [],            # è¯¦ç»†é”™è¯¯è®°å½•
            'file_error_stats': {},         # æŒ‰æ–‡ä»¶ç»Ÿè®¡é”™è¯¯
            'performance_warnings': [],      # æ€§èƒ½è­¦å‘Š
            'diagnostic_info': {             # è¯Šæ–­ä¿¡æ¯
                'thread_status': {},
                'connection_status': {},
                'memory_usage_history': [],
                'processing_speed_history': []
            }
        }

        # çº¿ç¨‹å®‰å…¨çš„ç»„ä»¶æ± 
        self.parser_pool = [BaseLogParser() for _ in range(max_workers)]
        self.mapper_pool = [FieldMapper() for _ in range(max_workers)]
        self.writer_pool = []

        # ç®€åŒ–çš„ç¼“å­˜ç­–ç•¥ - ç§»é™¤è¿‡åº¦ä¼˜åŒ–
        self.ua_cache = {}  # User-Agentè§£æç¼“å­˜
        self.uri_cache = {}  # URIè§£æç¼“å­˜
        self.cache_hit_stats = {'ua_hits': 0, 'uri_hits': 0, 'total_requests': 0}

        # çº¿ç¨‹åŒæ­¥
        self.result_queue = queue.Queue()
        self.error_queue = queue.Queue()
        self.stats_lock = threading.Lock()

        # å¤„ç†çŠ¶æ€
        self.processed_state = self.load_state()

        # åˆå§‹åŒ–è¿æ¥æ±  - æ”¾åœ¨æœ€åï¼Œå› ä¸ºéœ€è¦ä½¿ç”¨session_stats
        self._init_connection_pool()

        self.logger.info("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ - æ¶æ„ä¼˜åŒ–ç‰ˆæœ¬")
        self.logger.info(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.base_log_dir}")
        self.logger.info(f"âš™ï¸ æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        self.logger.info(f"ğŸ§µ æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        self.logger.info(f"ğŸ”— è¿æ¥æ± å¤§å°: {self.connection_pool_size}")

        # æ€§èƒ½å»ºè®®
        if self.batch_size > 5000:
            self.record_performance_warning(
                f"æ‰¹å¤„ç†å¤§å° {self.batch_size} å¯èƒ½è¿‡å¤§ï¼Œå»ºè®®ä½¿ç”¨ 2000-3000",
                {'batch_size': self.batch_size, 'recommendation': '2000-3000'}
            )

        if self.max_workers > 6:
            self.record_performance_warning(
                f"çº¿ç¨‹æ•° {self.max_workers} å¯èƒ½è¿‡é«˜ï¼Œå»ºè®®ä½¿ç”¨ 4-6",
                {'max_workers': self.max_workers, 'recommendation': '4-6'}
            )

    def _init_connection_pool(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        self.logger.info("ğŸ”— åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        success_count = 0
        failed_count = 0

        for i in range(self.connection_pool_size):
            try:
                writer = DWDWriter()
                if writer.connect():
                    self.writer_pool.append(writer)
                    success_count += 1
                    self.logger.info(f"âœ… è¿æ¥ {i+1} å»ºç«‹æˆåŠŸ")
                else:
                    failed_count += 1
                    error_msg = f"è¿æ¥ {i+1} å»ºç«‹å¤±è´¥ - connect() è¿”å› False"
                    self.logger.error(f"âŒ {error_msg}")
                    self.record_error('connection_errors', error_msg, {'connection_id': i+1})

            except Exception as e:
                failed_count += 1
                error_msg = f"è¿æ¥ {i+1} åˆå§‹åŒ–å¼‚å¸¸: {str(e)}"
                self.logger.error(f"âŒ {error_msg}")
                self.record_error('connection_errors', error_msg, {
                    'connection_id': i+1,
                    'exception_type': type(e).__name__,
                    'traceback': traceback.format_exc()
                })

        if not self.writer_pool:
            critical_error = "æ— æ³•å»ºç«‹ä»»ä½•æ•°æ®åº“è¿æ¥"
            self.record_error('critical_errors', critical_error)
            raise RuntimeError(f"âŒ {critical_error}")

        # è®°å½•è¿æ¥æ± çŠ¶æ€
        self.session_stats['diagnostic_info']['connection_status'] = {
            'total_requested': self.connection_pool_size,
            'successful': success_count,
            'failed': failed_count,
            'success_rate': (success_count / self.connection_pool_size) * 100
        }

        self.logger.info(f"ğŸ”— è¿æ¥æ± åˆå§‹åŒ–å®Œæˆï¼š{success_count}/{self.connection_pool_size} ä¸ªè¿æ¥æˆåŠŸ")

        if failed_count > 0:
            self.record_performance_warning(
                f"è¿æ¥æ± åˆå§‹åŒ–ä¸å®Œæ•´ï¼š{failed_count} ä¸ªè¿æ¥å¤±è´¥",
                {'failed_connections': failed_count, 'total_connections': self.connection_pool_size}
            )

    def get_writer(self) -> Optional[DWDWriter]:
        """ä»è¿æ¥æ± è·å–Writerï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.stats_lock:
            if self.writer_pool:
                return self.writer_pool.pop()
        return None

    def return_writer(self, writer: DWDWriter):
        """å½’è¿˜Writeråˆ°è¿æ¥æ± ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if writer is None:
            return

        with self.stats_lock:
            if len(self.writer_pool) < self.connection_pool_size:
                self.writer_pool.append(writer)

    def cached_ua_parse(self, user_agent: str, mapper: FieldMapper) -> Dict[str, str]:
        """ç®€åŒ–çš„User-Agentè§£æç¼“å­˜"""
        if not user_agent:
            return {}

        with self.stats_lock:
            self.cache_hit_stats['total_requests'] += 1

            if user_agent in self.ua_cache:
                self.cache_hit_stats['ua_hits'] += 1
                return self.ua_cache[user_agent]

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè§£æ
        try:
            result = mapper._parse_user_agent(user_agent)
        except Exception as e:
            error_msg = f"User-Agentè§£æå¤±è´¥: {str(e)}"
            self.record_error('field_mapping_errors', error_msg, {
                'user_agent': user_agent[:100],  # åªè®°å½•å‰100ä¸ªå­—ç¬¦
                'exception_type': type(e).__name__
            })
            result = {}

        with self.stats_lock:
            # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
            if len(self.ua_cache) < 5000:  # é™ä½ç¼“å­˜å¤§å°
                self.ua_cache[user_agent] = result

        return result

    def cached_uri_parse(self, uri: str, mapper: FieldMapper) -> Dict[str, str]:
        """ç®€åŒ–çš„URIç»“æ„è§£æç¼“å­˜"""
        if not uri:
            return {}

        with self.stats_lock:
            if uri in self.uri_cache:
                self.cache_hit_stats['uri_hits'] += 1
                return self.uri_cache[uri]

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè§£æ
        try:
            result = mapper._parse_uri_structure(uri)
        except Exception as e:
            error_msg = f"URIè§£æå¤±è´¥: {str(e)}"
            self.record_error('field_mapping_errors', error_msg, {
                'uri': uri[:200],  # åªè®°å½•å‰200ä¸ªå­—ç¬¦
                'exception_type': type(e).__name__
            })
            result = {}

        with self.stats_lock:
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.uri_cache) < 3000:  # é™ä½ç¼“å­˜å¤§å°
                self.uri_cache[uri] = result

        return result

    def process_file_batch(self, file_paths: List[Path], thread_id: int,
                          test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆå•çº¿ç¨‹æ‰§è¡Œï¼‰- å¢å¼ºé”™è¯¯å¤„ç†ç‰ˆæœ¬
        """
        start_time = time.time()
        thread_name = f"Thread-{thread_id}"

        self.logger.info(f"ğŸ§µ{thread_id} å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶")

        # æ›´æ–°çº¿ç¨‹çŠ¶æ€
        with self.stats_lock:
            self.session_stats['diagnostic_info']['thread_status'][thread_name] = {
                'status': 'running',
                'start_time': start_time,
                'files_assigned': len(file_paths),
                'current_file': None
            }

        # è·å–çº¿ç¨‹ä¸“ç”¨ç»„ä»¶
        parser = self.parser_pool[thread_id % len(self.parser_pool)]
        mapper = self.mapper_pool[thread_id % len(self.mapper_pool)]
        writer = None

        if not test_mode:
            writer = self.get_writer()
            if not writer:
                error_msg = f"çº¿ç¨‹ {thread_id} æ— æ³•è·å–æ•°æ®åº“è¿æ¥"
                self.record_error('connection_errors', error_msg, {'thread_id': thread_id})
                return {
                    'success': False,
                    'error': error_msg,
                    'thread_id': thread_id,
                    'error_details': {'error_type': 'connection_error'}
                }

        try:
            batch_stats = {
                'thread_id': thread_id,
                'total_files': len(file_paths),
                'processed_files': 0,
                'total_records': 0,
                'total_lines': 0,
                'errors': [],
                'file_results': [],
                'detailed_errors': []
            }

            # æ‰¹é‡ç¼“å†²åŒº
            mega_batch = []

            for file_index, file_path in enumerate(file_paths):
                # æ›´æ–°å½“å‰å¤„ç†æ–‡ä»¶çŠ¶æ€
                with self.stats_lock:
                    self.session_stats['diagnostic_info']['thread_status'][thread_name]['current_file'] = file_path.name

                self.logger.info(f"ğŸ§µ{thread_id} å¤„ç†æ–‡ä»¶ {file_index+1}/{len(file_paths)}: {file_path.name}")

                file_start = time.time()
                file_records = 0
                file_lines = 0
                file_errors = []

                try:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œå¯è¯»
                    if not file_path.exists():
                        error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                        self.record_error('file_access_errors', error_msg, {
                            'file_path': str(file_path),
                            'thread_id': thread_id
                        })
                        file_errors.append(error_msg)
                        continue

                    if not file_path.is_file():
                        error_msg = f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}"
                        self.record_error('file_access_errors', error_msg, {
                            'file_path': str(file_path),
                            'thread_id': thread_id
                        })
                        file_errors.append(error_msg)
                        continue

                    # æµå¼å¤„ç†æ–‡ä»¶
                    line_number = 0
                    for parsed_data in parser.parse_file(file_path):
                        line_number += 1
                        file_lines += 1

                        if parsed_data:
                            try:
                                # ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–çš„æ˜ å°„
                                user_agent = parsed_data.get('user_agent', '')
                                uri = parsed_data.get('request', '').split(' ')[1] if 'request' in parsed_data else ''

                                # ä½¿ç”¨ç¼“å­˜
                                if user_agent:
                                    parsed_data['_cached_ua'] = self.cached_ua_parse(user_agent, mapper)
                                if uri:
                                    parsed_data['_cached_uri'] = self.cached_uri_parse(uri, mapper)

                                # å­—æ®µæ˜ å°„
                                mapped_data = mapper.map_to_dwd(parsed_data, file_path.name)
                                mega_batch.append(mapped_data)
                                file_records += 1

                                # æ£€æŸ¥æ‰¹é‡å¤§å°
                                if len(mega_batch) >= self.batch_size:
                                    if not test_mode:
                                        write_result = writer.write_batch(mega_batch)
                                        if not write_result['success']:
                                            error_msg = f"æ‰¹é‡å†™å…¥å¤±è´¥: {write_result['error']}"
                                            self.record_error('database_write_errors', error_msg, {
                                                'file_name': file_path.name,
                                                'thread_id': thread_id,
                                                'batch_size': len(mega_batch)
                                            })
                                            file_errors.append(error_msg)

                                    mega_batch.clear()
                                    gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶

                                # æ£€æŸ¥é™åˆ¶
                                if limit and file_records >= limit:
                                    self.logger.info(f"ğŸ§µ{thread_id} è¾¾åˆ°è¡Œæ•°é™åˆ¶ {limit}ï¼Œåœæ­¢å¤„ç† {file_path.name}")
                                    break

                            except Exception as e:
                                error_msg = f"è®°å½•å¤„ç†é”™è¯¯ (è¡Œ {line_number}): {str(e)}"
                                self.record_error('field_mapping_errors', error_msg, {
                                    'file_name': file_path.name,
                                    'line_number': line_number,
                                    'thread_id': thread_id,
                                    'exception_type': type(e).__name__,
                                    'traceback': traceback.format_exc()
                                })
                                file_errors.append(error_msg)
                        else:
                            # è§£æå¤±è´¥çš„è¡Œ
                            self.record_error('parsing_errors', f"è§£æå¤±è´¥ (è¡Œ {line_number})", {
                                'file_name': file_path.name,
                                'line_number': line_number,
                                'thread_id': thread_id
                            })

                except Exception as e:
                    error_msg = f"æ–‡ä»¶å¤„ç†å¼‚å¸¸ {file_path.name}: {str(e)}"
                    self.logger.error(f"ğŸ§µ{thread_id} {error_msg}")
                    self.record_error('file_access_errors', error_msg, {
                        'file_name': file_path.name,
                        'thread_id': thread_id,
                        'exception_type': type(e).__name__,
                        'traceback': traceback.format_exc()
                    })
                    file_errors.append(error_msg)

                file_duration = time.time() - file_start
                file_result = {
                    'file': file_path.name,
                    'records': file_records,
                    'lines': file_lines,
                    'duration': file_duration,
                    'errors': file_errors,
                    'error_count': len(file_errors)
                }
                batch_stats['file_results'].append(file_result)

                batch_stats['processed_files'] += 1
                batch_stats['total_records'] += file_records
                batch_stats['total_lines'] += file_lines
                batch_stats['errors'].extend(file_errors)

                # è®°å½•å¤„ç†é€Ÿåº¦
                speed = file_records / file_duration if file_duration > 0 else 0
                self.logger.info(f"ğŸ§µ{thread_id} å®Œæˆ {file_path.name}: {file_records} è®°å½•, {file_duration:.2f}s, {speed:.1f} rec/s")

            # å¤„ç†å‰©ä½™æ‰¹æ¬¡
            if mega_batch:
                if not test_mode:
                    try:
                        write_result = writer.write_batch(mega_batch)
                        if not write_result['success']:
                            error_msg = f"æœ€ç»ˆæ‰¹æ¬¡å†™å…¥å¤±è´¥: {write_result['error']}"
                            self.record_error('database_write_errors', error_msg, {
                                'thread_id': thread_id,
                                'batch_size': len(mega_batch)
                            })
                            batch_stats['errors'].append(error_msg)
                    except Exception as e:
                        error_msg = f"æœ€ç»ˆæ‰¹æ¬¡å†™å…¥å¼‚å¸¸: {str(e)}"
                        self.record_error('database_write_errors', error_msg, {
                            'thread_id': thread_id,
                            'exception_type': type(e).__name__,
                            'traceback': traceback.format_exc()
                        })
                        batch_stats['errors'].append(error_msg)
                mega_batch.clear()

            batch_stats['duration'] = time.time() - start_time
            batch_stats['success'] = len(batch_stats['errors']) == 0

            # æ›´æ–°çº¿ç¨‹å®ŒæˆçŠ¶æ€
            with self.stats_lock:
                self.session_stats['diagnostic_info']['thread_status'][thread_name].update({
                    'status': 'completed',
                    'end_time': time.time(),
                    'duration': batch_stats['duration'],
                    'records_processed': batch_stats['total_records'],
                    'files_processed': batch_stats['processed_files'],
                    'error_count': len(batch_stats['errors'])
                })

            return batch_stats

        except Exception as e:
            critical_error = f"çº¿ç¨‹ {thread_id} å‘ç”Ÿè‡´å‘½é”™è¯¯: {str(e)}"
            self.logger.error(f"âŒ {critical_error}")
            self.record_error('critical_errors', critical_error, {
                'thread_id': thread_id,
                'exception_type': type(e).__name__,
                'traceback': traceback.format_exc()
            })

            # æ›´æ–°çº¿ç¨‹é”™è¯¯çŠ¶æ€
            with self.stats_lock:
                self.session_stats['diagnostic_info']['thread_status'][thread_name].update({
                    'status': 'error',
                    'error': str(e),
                    'end_time': time.time()
                })

            return {
                'success': False,
                'error': critical_error,
                'thread_id': thread_id,
                'error_details': {
                    'error_type': 'critical_error',
                    'exception_type': type(e).__name__,
                    'traceback': traceback.format_exc()
                }
            }

        finally:
            # å½’è¿˜è¿æ¥åˆ°æ± 
            if writer:
                self.return_writer(writer)

    def process_date_parallel(self, date_str: str, force_reprocess: bool = False,
                             test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """
        å¹¶è¡Œå¤„ç†æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰æ—¥å¿— - å¢å¼ºé”™è¯¯å¤„ç†ç‰ˆæœ¬
        """
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {date_str} çš„æ—¥å¿—")
        self.session_stats['start_time'] = datetime.now()
        start_time = time.time()

        # æ£€æŸ¥æ—¥æœŸç›®å½•
        date_dir = self.base_log_dir / date_str
        if not date_dir.exists():
            error_msg = f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date_dir}'
            self.record_error('file_access_errors', error_msg, {'date': date_str})
            return {
                'success': False,
                'error': error_msg,
                'date': date_str,
                'error_details': {'error_type': 'directory_not_found'}
            }

        # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        try:
            all_log_files = list(date_dir.glob("*.log"))
        except Exception as e:
            error_msg = f'æ‰«ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}'
            self.record_error('file_access_errors', error_msg, {
                'date': date_str,
                'directory': str(date_dir),
                'exception_type': type(e).__name__
            })
            return {
                'success': False,
                'error': error_msg,
                'date': date_str,
                'error_details': {'error_type': 'file_scan_error'}
            }

        if not all_log_files:
            error_msg = f'ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.logæ–‡ä»¶: {date_dir}'
            self.record_error('file_access_errors', error_msg, {'date': date_str})
            return {
                'success': False,
                'error': error_msg,
                'date': date_str,
                'error_details': {'error_type': 'no_log_files'}
            }

        # è¿‡æ»¤éœ€è¦å¤„ç†çš„æ–‡ä»¶
        if not force_reprocess:
            pending_files = [f for f in all_log_files if not self.is_file_processed(f)]
        else:
            pending_files = all_log_files

        if not pending_files:
            self.logger.info(f"ğŸ“‹ æ—¥æœŸ {date_str} çš„æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†")
            return {
                'success': True,
                'processed_files': 0,
                'message': 'æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†',
                'date': date_str,
                'total_records': 0,
                'duration': 0,
                'processing_speed': 0,
                'cache_hit_rate': 0
            }

        self.logger.info(f"ğŸ“ æ‰¾åˆ° {len(pending_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")

        # å°†æ–‡ä»¶åˆ†æ‰¹åˆ†é…ç»™çº¿ç¨‹
        files_per_thread = max(1, len(pending_files) // self.max_workers)
        file_batches = []

        for i in range(0, len(pending_files), files_per_thread):
            batch = pending_files[i:i + files_per_thread]
            if batch:
                file_batches.append(batch)

        actual_workers = min(self.max_workers, len(file_batches))
        self.logger.info(f"ğŸ§µ åˆ†é… {len(file_batches)} ä¸ªæ‰¹æ¬¡ç»™ {actual_workers} ä¸ªçº¿ç¨‹")

        # å¹¶è¡Œå¤„ç†
        all_results = []
        total_records = 0
        total_errors = []

        try:
            with ThreadPoolExecutor(max_workers=actual_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_batch = {}
                for i, batch in enumerate(file_batches):
                    future = executor.submit(self.process_file_batch, batch, i, test_mode, limit)
                    future_to_batch[future] = i

                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_batch):
                    batch_id = future_to_batch[future]
                    try:
                        result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                        all_results.append(result)

                        if result['success']:
                            total_records += result['total_records']
                            self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ: {result['processed_files']} æ–‡ä»¶, "
                                           f"{result['total_records']} è®°å½•")
                        else:
                            total_errors.extend(result.get('errors', []))
                            self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_id} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    except Exception as e:
                        error_msg = f"æ‰¹æ¬¡ {batch_id} æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                        self.logger.error(f"âŒ {error_msg}")
                        self.record_error('thread_errors', error_msg, {
                            'batch_id': batch_id,
                            'exception_type': type(e).__name__,
                            'traceback': traceback.format_exc()
                        })
                        total_errors.append(error_msg)

        except Exception as e:
            error_msg = f"çº¿ç¨‹æ± æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            self.record_error('critical_errors', error_msg, {
                'date': date_str,
                'exception_type': type(e).__name__,
                'traceback': traceback.format_exc()
            })
            return {
                'success': False,
                'error': error_msg,
                'date': date_str,
                'error_details': {'error_type': 'thread_pool_error'}
            }

        # æ›´æ–°å¤„ç†çŠ¶æ€ï¼ˆéæµ‹è¯•æ¨¡å¼ï¼‰
        if not test_mode:
            for file_path in pending_files:
                # æ ¹æ®å¤„ç†ç»“æœæ›´æ–°çŠ¶æ€
                file_processed = any(
                    any(fr['file'] == file_path.name for fr in r.get('file_results', []))
                    for r in all_results if r.get('success', False)
                )
                if file_processed:
                    self.mark_file_processed(file_path, 0, 0)  # ç®€åŒ–çŠ¶æ€æ›´æ–°
            self.save_state()

        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = 0.0
        if self.cache_hit_stats['total_requests'] > 0:
            total_hits = self.cache_hit_stats['ua_hits'] + self.cache_hit_stats['uri_hits']
            cache_hit_rate = (total_hits / (self.cache_hit_stats['total_requests'] * 2)) * 100

        duration = time.time() - start_time
        self.session_stats['end_time'] = datetime.now()

        # æ€§èƒ½ç»Ÿè®¡
        speed = total_records / duration if duration > 0 else 0
        self.session_stats['avg_processing_speed'] = speed
        self.session_stats['cache_hit_rate'] = cache_hit_rate
        self.session_stats['total_records_written'] = total_records

        success = len(total_errors) == 0

        result = {
            'success': success,
            'date': date_str,
            'processed_files': sum(r.get('processed_files', 0) for r in all_results),
            'total_records': total_records,
            'duration': duration,
            'processing_speed': speed,
            'cache_hit_rate': cache_hit_rate,
            'errors': total_errors,
            'thread_results': all_results,
            'error_details': {
                'total_errors': len(total_errors),
                'thread_count': actual_workers,
                'batch_count': len(file_batches)
            }
        }

        if success:
            self.logger.info(f"ğŸ‰ æ—¥æœŸ {date_str} å¹¶è¡Œå¤„ç†å®Œæˆ!")
            self.logger.info(f"ğŸ“Š {result['processed_files']} æ–‡ä»¶, {total_records:,} è®°å½•")
            self.logger.info(f"â±ï¸  è€—æ—¶ {duration:.2f}s, é€Ÿåº¦ {speed:.1f} è®°å½•/ç§’")
            self.logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")
        else:
            self.logger.error(f"âŒ æ—¥æœŸ {date_str} å¤„ç†å®Œæˆä½†æœ‰é”™è¯¯: {len(total_errors)} ä¸ªé”™è¯¯")

        return result

    # === é”™è¯¯å¤„ç†å’Œç»Ÿè®¡æ–¹æ³• ===

    def record_error(self, error_type: str, error_msg: str, context: Dict[str, Any] = None):
        """è®°å½•é”™è¯¯ä¿¡æ¯å’Œç»Ÿè®¡ - å¢å¼ºç‰ˆæœ¬"""
        timestamp = datetime.now()

        with self.stats_lock:
            # æ›´æ–°é”™è¯¯ç»Ÿè®¡
            if error_type in self.session_stats['error_stats']:
                self.session_stats['error_stats'][error_type] += 1
            else:
                self.session_stats['error_stats'][error_type] = 1

            self.session_stats['total_errors'] += 1

            # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
            error_detail = {
                'timestamp': timestamp.isoformat(),
                'error_type': error_type,
                'error_message': error_msg,
                'context': context or {}
            }

            self.session_stats['error_details'].append(error_detail)

            # æŒ‰æ–‡ä»¶ç»Ÿè®¡é”™è¯¯
            if context and 'file_name' in context:
                file_name = context['file_name']
                if file_name not in self.session_stats['file_error_stats']:
                    self.session_stats['file_error_stats'][file_name] = {
                        'total_errors': 0,
                        'error_types': defaultdict(int),
                        'first_error_time': timestamp.isoformat()
                    }

                self.session_stats['file_error_stats'][file_name]['total_errors'] += 1
                self.session_stats['file_error_stats'][file_name]['error_types'][error_type] += 1
                self.session_stats['file_error_stats'][file_name]['last_error_time'] = timestamp.isoformat()

            # é™åˆ¶é”™è¯¯è¯¦æƒ…æ•°é‡ï¼Œé¿å…å†…å­˜æº¢å‡º
            if len(self.session_stats['error_details']) > 1000:
                self.session_stats['error_details'] = self.session_stats['error_details'][-500:]

        # è®°å½•åˆ°æ—¥å¿—
        if error_type in ['critical_errors', 'database_write_errors', 'connection_errors']:
            self.logger.error(f"âŒ [{error_type}] {error_msg}")
        elif error_type in ['warning_errors', 'parsing_errors']:
            self.logger.warning(f"âš ï¸ [{error_type}] {error_msg}")
        else:
            self.logger.info(f"â„¹ï¸ [{error_type}] {error_msg}")

    def record_performance_warning(self, warning_msg: str, context: Dict[str, Any] = None):
        """è®°å½•æ€§èƒ½è­¦å‘Š"""
        with self.stats_lock:
            warning = {
                'timestamp': datetime.now().isoformat(),
                'message': warning_msg,
                'context': context or {}
            }
            self.session_stats['performance_warnings'].append(warning)

            # é™åˆ¶è­¦å‘Šæ•°é‡
            if len(self.session_stats['performance_warnings']) > 100:
                self.session_stats['performance_warnings'] = self.session_stats['performance_warnings'][-50:]

        self.logger.warning(f"âš ï¸ [æ€§èƒ½è­¦å‘Š] {warning_msg}")

    def get_error_summary(self) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦æŠ¥å‘Š"""
        with self.stats_lock:
            total_errors = self.session_stats['total_errors']
            total_records = self.session_stats['total_records_written']

            if total_records == 0:
                error_rate = 0.0 if total_errors == 0 else 100.0
            else:
                error_rate = (total_errors / (total_records + total_errors)) * 100

            return {
                'total_errors': total_errors,
                'error_rate_percent': round(error_rate, 2),
                'error_breakdown': dict(self.session_stats['error_stats']),
                'files_with_errors': len(self.session_stats['file_error_stats']),
                'performance_warnings_count': len(self.session_stats['performance_warnings']),
                'most_common_errors': self._get_most_common_errors(),
                'error_trend': self._analyze_error_trend(),
                'thread_diagnostic': self.session_stats['diagnostic_info']['thread_status'],
                'connection_diagnostic': self.session_stats['diagnostic_info']['connection_status']
            }

    def _get_most_common_errors(self) -> List[Dict[str, Any]]:
        """è·å–æœ€å¸¸è§çš„é”™è¯¯ç±»å‹"""
        error_counts = self.session_stats['error_stats']
        sorted_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)
        return [{'error_type': k, 'count': v} for k, v in sorted_errors[:5] if v > 0]

    def _analyze_error_trend(self) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯è¶‹åŠ¿"""
        error_details = self.session_stats['error_details']
        if len(error_details) < 2:
            return {'trend': 'insufficient_data', 'recent_errors': len(error_details)}

        # åˆ†ææœ€è¿‘çš„é”™è¯¯
        recent_errors = error_details[-10:]
        error_types = defaultdict(int)
        for error in recent_errors:
            error_types[error['error_type']] += 1

        return {
            'trend': 'stable' if len(set(e['error_type'] for e in recent_errors)) <= 2 else 'varied',
            'recent_errors': len(recent_errors),
            'recent_error_types': dict(error_types)
        }

    def print_error_report(self):
        """æ‰“å°è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š - å¢å¼ºç‰ˆæœ¬"""
        summary = self.get_error_summary()

        print("\n" + "="*80)
        print("ğŸ“Š è¯¦ç»†é”™è¯¯ç»Ÿè®¡æŠ¥å‘Š (æ¶æ„ä¼˜åŒ–ç‰ˆæœ¬)")
        print("="*80)

        if summary['total_errors'] == 0:
            print("âœ… æ²¡æœ‰å‘ç°é”™è¯¯ - å¤„ç†å®Œå…¨æˆåŠŸï¼")
            print("="*80)
            return

        print(f"ğŸ“ˆ æ€»é”™è¯¯æ•°: {summary['total_errors']}")
        print(f"ğŸ“ˆ é”™è¯¯ç‡: {summary['error_rate_percent']}%")
        print(f"ğŸ“ æœ‰é”™è¯¯çš„æ–‡ä»¶æ•°: {summary['files_with_errors']}")
        print(f"âš ï¸  æ€§èƒ½è­¦å‘Šæ•°: {summary['performance_warnings_count']}")

        if summary['most_common_errors']:
            print(f"\nğŸ” æœ€å¸¸è§é”™è¯¯:")
            for error in summary['most_common_errors']:
                print(f"   â€¢ {error['error_type']}: {error['count']} æ¬¡")

        print(f"\nğŸ“‹ é”™è¯¯è¯¦ç»†åˆ†ç±»:")
        for error_type, count in summary['error_breakdown'].items():
            if count > 0:
                print(f"   â€¢ {error_type}: {count}")

        # æ˜¾ç¤ºæ–‡ä»¶é”™è¯¯ç»Ÿè®¡ï¼ˆåªæ˜¾ç¤ºé”™è¯¯æœ€å¤šçš„å‰5ä¸ªæ–‡ä»¶ï¼‰
        if self.session_stats['file_error_stats']:
            print(f"\nğŸ“ æ–‡ä»¶é”™è¯¯ç»Ÿè®¡ï¼ˆå‰5ä¸ªï¼‰:")
            file_errors = sorted(
                self.session_stats['file_error_stats'].items(),
                key=lambda x: x[1]['total_errors'],
                reverse=True
            )[:5]

            for file_name, stats in file_errors:
                print(f"   â€¢ {file_name}: {stats['total_errors']} é”™è¯¯")
                # æ˜¾ç¤ºä¸»è¦é”™è¯¯ç±»å‹
                top_error_types = sorted(
                    stats['error_types'].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:3]
                for error_type, count in top_error_types:
                    print(f"     - {error_type}: {count}")

        # æ˜¾ç¤ºçº¿ç¨‹è¯Šæ–­ä¿¡æ¯
        if summary['thread_diagnostic']:
            print(f"\nğŸ§µ çº¿ç¨‹è¯Šæ–­ä¿¡æ¯:")
            for thread_name, status in summary['thread_diagnostic'].items():
                thread_status = status.get('status', 'unknown')
                error_count = status.get('error_count', 0)
                records = status.get('records_processed', 0)
                print(f"   â€¢ {thread_name}: {thread_status}, é”™è¯¯: {error_count}, è®°å½•: {records}")

        # æ˜¾ç¤ºè¿æ¥è¯Šæ–­ä¿¡æ¯
        if summary['connection_diagnostic']:
            conn_info = summary['connection_diagnostic']
            print(f"\nğŸ”— è¿æ¥è¯Šæ–­ä¿¡æ¯:")
            print(f"   â€¢ æˆåŠŸè¿æ¥: {conn_info.get('successful', 0)}/{conn_info.get('total_requested', 0)}")
            print(f"   â€¢ æˆåŠŸç‡: {conn_info.get('success_rate', 0):.1f}%")

        # æ˜¾ç¤ºæœ€è¿‘çš„æ€§èƒ½è­¦å‘Š
        if self.session_stats['performance_warnings']:
            print(f"\nâš ï¸  æœ€è¿‘çš„æ€§èƒ½è­¦å‘Šï¼ˆæœ€è¿‘5ä¸ªï¼‰:")
            recent_warnings = self.session_stats['performance_warnings'][-5:]
            for warning in recent_warnings:
                print(f"   â€¢ {warning['message']}")

        print("="*80)

    # === ç»§æ‰¿çš„æ–¹æ³•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰===

    def process_all_parallel(self, test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—"""
        self.logger.info("ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—")
        self.session_stats['start_time'] = datetime.now()
        start_time = time.time()

        # æ‰«ææ‰€æœ‰æ—¥æœŸç›®å½•
        log_files_by_date = self.scan_log_directories()
        if not log_files_by_date:
            return {'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶'}

        total_records = 0
        processed_dates = 0
        all_errors = []
        date_results = []

        # æŒ‰æ—¥æœŸé¡ºåºå¤„ç†ï¼ˆä½†æ¯ä¸ªæ—¥æœŸå†…éƒ¨å¹¶è¡Œï¼‰
        for date_str in sorted(log_files_by_date.keys()):
            self.logger.info(f"ğŸ“… å¼€å§‹å¤„ç†æ—¥æœŸ: {date_str}")

            result = self.process_date_parallel(date_str, force_reprocess=False,
                                              test_mode=test_mode, limit=limit)
            date_results.append(result)

            if result['success'] and result.get('processed_files', 0) > 0:
                processed_dates += 1
                total_records += result['total_records']
                self.logger.info(f"âœ… {date_str} å®Œæˆ: {result['total_records']:,} è®°å½•")
            else:
                if result.get('errors'):
                    all_errors.extend(result['errors'])
                self.logger.warning(f"âš ï¸ {date_str} è·³è¿‡æˆ–å¤±è´¥")

        duration = time.time() - start_time
        success = len(all_errors) == 0
        overall_speed = total_records / duration if duration > 0 else 0

        self.session_stats['end_time'] = datetime.now()
        self.session_stats['avg_processing_speed'] = overall_speed

        return {
            'success': success,
            'processed_dates': processed_dates,
            'total_records': total_records,
            'duration': duration,
            'processing_speed': overall_speed,
            'errors': all_errors,
            'date_results': date_results
        }

    def load_state(self) -> Dict[str, Any]:
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

        return {
            'processed_files': {},
            'last_update': None,
            'total_processed_records': 0,
            'processing_history': []
        }

    def save_state(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        try:
            self.processed_state['last_update'] = datetime.now().isoformat()
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_state, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            self.logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def scan_log_directories(self) -> Dict[str, List[Path]]:
        """æ‰«ææ—¥å¿—ç›®å½•"""
        if not self.base_log_dir.exists():
            return {}

        log_files_by_date = {}
        for date_dir in self.base_log_dir.iterdir():
            if date_dir.is_dir() and date_dir.name.isdigit() and len(date_dir.name) == 8:
                try:
                    datetime.strptime(date_dir.name, '%Y%m%d')
                    log_files = list(date_dir.glob("*.log"))
                    if log_files:
                        log_files_by_date[date_dir.name] = sorted(log_files)
                except ValueError:
                    continue

        return log_files_by_date

    def is_file_processed(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        file_key = str(file_path)
        return file_key in self.processed_state.get('processed_files', {})

    def mark_file_processed(self, file_path: Path, record_count: int, processing_time: float):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        file_key = str(file_path)
        try:
            self.processed_state['processed_files'][file_key] = {
                'processed_at': datetime.now().isoformat(),
                'record_count': record_count,
                'processing_time': processing_time,
                'mtime': file_path.stat().st_mtime,
                'file_size': file_path.stat().st_size
            }
        except Exception as e:
            self.logger.error(f"æ ‡è®°æ–‡ä»¶çŠ¶æ€å¤±è´¥ {file_path}: {e}")

    def show_performance_stats(self):
        """æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 80)
        print("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š (æ¶æ„ä¼˜åŒ–ç‰ˆæœ¬)")
        print("=" * 80)

        print(f"âš™ï¸  é…ç½®ä¿¡æ¯:")
        print(f"   æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        print(f"   è¿æ¥æ± å¤§å°: {self.connection_pool_size}")
        print(f"   å½“å‰å¯ç”¨è¿æ¥: {len(self.writer_pool)}")

        print(f"\nğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:")
        print(f"   User-Agentç¼“å­˜: {len(self.ua_cache)} é¡¹")
        print(f"   URIç¼“å­˜: {len(self.uri_cache)} é¡¹")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {self.session_stats.get('cache_hit_rate', 0):.1f}%")

        if self.session_stats.get('avg_processing_speed', 0) > 0:
            print(f"\nğŸƒ æ€§èƒ½æŒ‡æ ‡:")
            print(f"   å¹³å‡å¤„ç†é€Ÿåº¦: {self.session_stats['avg_processing_speed']:.1f} è®°å½•/ç§’")
            print(f"   æ€»å¤„ç†è®°å½•æ•°: {self.session_stats.get('total_records_written', 0):,}")

            if self.session_stats.get('start_time') and self.session_stats.get('end_time'):
                duration = (self.session_stats['end_time'] - self.session_stats['start_time']).total_seconds()
                print(f"   æ€»å¤„ç†æ—¶é—´: {duration:.1f} ç§’")

        # æ˜¾ç¤ºé”™è¯¯æ±‡æ€»
        if self.session_stats['total_errors'] > 0:
            print(f"\nâŒ é”™è¯¯æ±‡æ€»:")
            print(f"   æ€»é”™è¯¯æ•°: {self.session_stats['total_errors']}")
            top_errors = sorted(
                self.session_stats['error_stats'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:3]
            for error_type, count in top_errors:
                if count > 0:
                    print(f"   {error_type}: {count}")

        print("=" * 80)

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")

        # å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥
        for writer in self.writer_pool:
            try:
                writer.close()
            except:
                pass
        self.writer_pool.clear()

        # æ¸…ç†ç¼“å­˜
        self.ua_cache.clear()
        self.uri_cache.clear()

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        self.logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

    # === ç®€åŒ–çš„äº¤äº’å¼èœå•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰===

    def interactive_menu(self):
        """ç®€åŒ–çš„äº¤äº’å¼èœå•"""
        while True:
            print("\n" + "=" * 80)
            print("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ¶æ„ä¼˜åŒ–ç‰ˆæœ¬")
            print("=" * 80)
            print("1. ğŸ”¥ å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—")
            print("2. ğŸ“… å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ—¥å¿—")
            print("3. ğŸ“Š æŸ¥çœ‹çŠ¶æ€å’Œæ€§èƒ½ç»Ÿè®¡")
            print("4. ğŸ§ª æµ‹è¯•æ¨¡å¼å¤„ç†")
            print("5. ğŸ“‹ è¯¦ç»†é”™è¯¯æŠ¥å‘Š")
            print("0. ğŸ‘‹ é€€å‡º")
            print("-" * 80)
            print(f"ğŸ“Š å½“å‰é…ç½®: æ‰¹é‡{self.batch_size} | çº¿ç¨‹{self.max_workers} | è¿æ¥æ± {self.connection_pool_size}")

            try:
                choice = input("è¯·é€‰æ‹©æ“ä½œ [0-5]: ").strip()

                if choice == '0':
                    print("ğŸ‘‹ å†è§ï¼")
                    break

                elif choice == '1':
                    print("\nğŸ”¥ å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—...")
                    limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None

                    start_time = time.time()
                    result = self.process_all_parallel(test_mode=False, limit=limit)
                    total_time = time.time() - start_time

                    if result['success']:
                        print(f"\nâœ… å¤„ç†å®Œæˆ: {result['total_records']:,} è®°å½•, {total_time:.2f}s")
                    else:
                        print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    input("\næŒ‰å›è½¦é”®ç»§ç»­...")

                elif choice == '2':
                    date_str = input("\nè¯·è¾“å…¥æ—¥æœŸ (YYYYMMDDæ ¼å¼): ").strip()
                    if len(date_str) != 8 or not date_str.isdigit():
                        print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯")
                        continue

                    force = input("å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Ÿ(y/N): ").strip().lower() == 'y'
                    limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None

                    print(f"\nğŸ”¥ å¤„ç† {date_str} çš„æ—¥å¿—...")
                    result = self.process_date_parallel(date_str, force, test_mode=False, limit=limit)

                    if result['success']:
                        print(f"\nâœ… å¤„ç†å®Œæˆ: {result['total_records']:,} è®°å½•")
                    else:
                        print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    input("\næŒ‰å›è½¦é”®ç»§ç»­...")

                elif choice == '3':
                    self.show_performance_stats()
                    input("\næŒ‰å›è½¦é”®ç»§ç»­...")

                elif choice == '4':
                    print("\nğŸ§ª æµ‹è¯•æ¨¡å¼å¤„ç†")
                    date_str = input("è¯·è¾“å…¥æ—¥æœŸ (YYYYMMDDæ ¼å¼): ").strip()
                    if len(date_str) != 8 or not date_str.isdigit():
                        print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯")
                        continue

                    limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (å»ºè®®100-1000): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else 100

                    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¤„ç† {date_str} çš„æ—¥å¿—...")
                    result = self.process_date_parallel(date_str, False, test_mode=True, limit=limit)

                    if result['success']:
                        print(f"\nâœ… æµ‹è¯•å®Œæˆ: {result['total_records']:,} è®°å½•")
                    else:
                        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    input("\næŒ‰å›è½¦é”®ç»§ç»­...")

                elif choice == '5':
                    self.print_error_report()
                    input("\næŒ‰å›è½¦é”®ç»§ç»­...")

                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 0-5")
                    input("æŒ‰å›è½¦é”®ç»§ç»­...")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ æ“ä½œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                input("æŒ‰å›è½¦é”®ç»§ç»­...")

def main():
    """ä¸»å‡½æ•°"""
    import logging

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    parser = argparse.ArgumentParser(
        description='é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ¶æ„ä¼˜åŒ–ç‰ˆæœ¬'
    )

    parser.add_argument('--date', help='å¤„ç†æŒ‡å®šæ—¥æœŸ (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰æœªå¤„ç†æ—¥å¿—')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--limit', type=int, help='æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°é™åˆ¶')
    parser.add_argument('--batch-size', type=int, default=2000, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--workers', type=int, default=4, help='å·¥ä½œçº¿ç¨‹æ•°')

    args = parser.parse_args()

    try:
        with HighPerformanceETLController(
            batch_size=args.batch_size,
            max_workers=args.workers
        ) as controller:

            # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºäº¤äº’å¼èœå•
            if not any([args.date, args.all]):
                controller.interactive_menu()
                return

            if args.date:
                result = controller.process_date_parallel(
                    args.date,
                    force_reprocess=args.force,
                    test_mode=args.test,
                    limit=args.limit
                )

                print(f"\nğŸ¯ å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸ: {result['date']}")
                print(f"æ–‡ä»¶: {result.get('processed_files', 0)}")
                print(f"è®°å½•: {result.get('total_records', 0):,}")
                print(f"è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")

            elif args.all:
                result = controller.process_all_parallel(
                    test_mode=args.test,
                    limit=args.limit
                )

                print(f"\nğŸ¯ æ‰¹é‡å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸæ•°: {result.get('processed_dates', 0)}")
                print(f"è®°å½•æ•°: {result.get('total_records', 0):,}")
                print(f"æ€»è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"å¹³å‡é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")

            controller.show_performance_stats()
            controller.print_error_report()

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()