#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ç‰ˆæœ¬
High Performance ETL Controller - Multi-threaded parallel processing

ç›¸æ¯”intelligent_etl_controllerçš„ä¸»è¦ä¼˜åŒ–ï¼š
1. å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ–‡ä»¶
2. å¤§æ‰¹é‡æ•°æ®åº“æ“ä½œï¼ˆ1000-5000æ¡/æ‰¹ï¼‰
3. è¿æ¥æ± ç®¡ç†ï¼Œé¿å…é¢‘ç¹è¿æ¥
4. å†…å­˜ç¼“å­˜ä¼˜åŒ–ï¼Œå‡å°‘é‡å¤è®¡ç®—
5. å¼‚æ­¥I/Oå’Œæµå¼å¤„ç†
"""

import sys
import os
import json
import time
import argparse
import threading
import queue
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from collections import defaultdict
import gc

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
current_dir = Path(__file__).parent
etl_root = current_dir.parent
sys.path.append(str(etl_root))

from parsers.base_log_parser import BaseLogParser
from processors.field_mapper import FieldMapper
from writers.dwd_writer import DWDWriter

class HighPerformanceETLController:
    """é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å¤šçº¿ç¨‹å¹¶è¡Œç‰ˆæœ¬"""
    
    def __init__(self, 
                 base_log_dir: str = None, 
                 state_file: str = None,
                 batch_size: int = 5000,        # è¶…å¤§æ‰¹é‡å¤§å°ä¼˜åŒ–
                 max_workers: int = 6,          # å¢åŠ å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°
                 connection_pool_size: int = None,  # æ•°æ®åº“è¿æ¥æ± å¤§å°ï¼ˆé»˜è®¤ä¸max_workersç›¸åŒï¼‰
                 memory_limit_mb: int = 512):    # å†…å­˜é™åˆ¶
        """
        åˆå§‹åŒ–é«˜æ€§èƒ½ETLæ§åˆ¶å™¨
        
        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆæ¨è1000-5000ï¼‰
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            connection_pool_size: æ•°æ®åº“è¿æ¥æ± å¤§å°
            memory_limit_mb: å†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆMBï¼‰
        """
        # åŸºç¡€é…ç½®
        self.base_log_dir = Path(base_log_dir) if base_log_dir else \
            Path("D:/project/nginx-log-analyzer/nginx-analytics-warehouse/nginx_logs")
        self.state_file = Path(state_file) if state_file else \
            Path(etl_root / "processed_logs_state.json")
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.batch_size = batch_size
        self.max_workers = max_workers
        # è¿æ¥æ± å¤§å°é»˜è®¤ä¸çº¿ç¨‹æ•°ç›¸åŒï¼Œç¡®ä¿æ¯ä¸ªçº¿ç¨‹æœ‰ç‹¬ç«‹è¿æ¥
        self.connection_pool_size = connection_pool_size if connection_pool_size is not None else max_workers
        self.memory_limit_mb = memory_limit_mb
        
        # æ—¥å¿—é…ç½® - å¿…é¡»åœ¨ä½¿ç”¨loggerä¹‹å‰åˆå§‹åŒ–
        import logging
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # çº¿ç¨‹å®‰å…¨çš„ç»„ä»¶æ± 
        self.parser_pool = [BaseLogParser() for _ in range(max_workers)]
        self.mapper_pool = [FieldMapper() for _ in range(max_workers)]
        self.writer_pool = []
        
        # åˆå§‹åŒ–è¿æ¥æ± 
        self._init_connection_pool()
        
        # é«˜æ€§èƒ½ç¼“å­˜ä¼˜åŒ– - ä½¿ç”¨LRUç¼“å­˜
        from functools import lru_cache
        self.ua_cache = {}  # User-Agentè§£æç¼“å­˜ (å¢å¤§å®¹é‡)
        self.uri_cache = {}  # URIè§£æç¼“å­˜ (å¢å¤§å®¹é‡)
        self.ip_cache = {}  # IPåœ°ç†ä¿¡æ¯ç¼“å­˜
        self.cache_hit_stats = {'ua_hits': 0, 'uri_hits': 0, 'ip_hits': 0, 'total_requests': 0}
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ç¼“å­˜
        import re
        self.regex_cache = {
            'user_agent_mobile': re.compile(r'(Mobile|Android|iPhone|iPad)', re.I),
            'uri_api': re.compile(r'/api/|/scmp-gateway/'),
            'ip_internal': re.compile(r'^(10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01])\.)')
        }
        
        # çº¿ç¨‹åŒæ­¥
        self.result_queue = queue.Queue()
        self.error_queue = queue.Queue()
        self.stats_lock = threading.Lock()
        
        # å¤„ç†çŠ¶æ€
        self.processed_state = self.load_state()
        
        # å…¨å±€ç»Ÿè®¡ä¿¡æ¯ - å¢å¼ºé”™è¯¯ç»Ÿè®¡
        self.session_stats = {
            'start_time': None,
            'end_time': None,
            'total_files_processed': 0,
            'total_lines_processed': 0,
            'total_records_written': 0,
            'total_errors': 0,
            'cache_hit_rate': 0.0,
            'avg_processing_speed': 0.0,
            'peak_memory_usage_mb': 0,
            'processing_errors': [],
            # === è¯¦ç»†é”™è¯¯ç»Ÿè®¡ ===
            'error_stats': {
                'parsing_errors': 0,        # è§£æé”™è¯¯
                'field_mapping_errors': 0,  # å­—æ®µæ˜ å°„é”™è¯¯
                'database_write_errors': 0, # æ•°æ®åº“å†™å…¥é”™è¯¯
                'fallback_records': 0,      # å®¹é”™å¤‡ç”¨è®°å½•æ•°
                'critical_errors': 0,       # è‡´å‘½é”™è¯¯
                'warning_errors': 0,        # è­¦å‘Šçº§é”™è¯¯
                'skipped_lines': 0,         # è·³è¿‡çš„è¡Œæ•°
                'invalid_records': 0        # æ— æ•ˆè®°å½•æ•°
            },
            'error_details': [],            # è¯¦ç»†é”™è¯¯è®°å½•
            'file_error_stats': {},         # æŒ‰æ–‡ä»¶ç»Ÿè®¡é”™è¯¯
            'performance_warnings': []       # æ€§èƒ½è­¦å‘Š
        }
        
        # é…ç½®åŒ¹é…æ£€æŸ¥
        if self.max_workers != self.connection_pool_size:
            self.logger.warning(f"âš ï¸  é…ç½®ä¸åŒ¹é…è­¦å‘Š:")
            self.logger.warning(f"   ğŸ§µ çº¿ç¨‹æ•°: {self.max_workers}")
            self.logger.warning(f"   ğŸ”— è¿æ¥æ± : {self.connection_pool_size}")
            self.logger.warning(f"   ğŸ’¡ å»ºè®®: çº¿ç¨‹æ•°å’Œè¿æ¥æ± å¤§å°åº”è¯¥ç›¸åŒä»¥é¿å…èµ„æºç«äº‰")
        
        self.logger.info("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.base_log_dir}")
        self.logger.info(f"âš™ï¸ æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        self.logger.info(f"ğŸ§µ æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        self.logger.info(f"ğŸ”— è¿æ¥æ± å¤§å°: {self.connection_pool_size}")
        
        # èµ„æºé…ç½®å»ºè®®
        if self.max_workers > 8:
            self.logger.warning(f"âš ï¸  çº¿ç¨‹æ•° {self.max_workers} å¯èƒ½è¿‡é«˜ï¼Œå»ºè®®æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´")
        if self.batch_size > 10000:
            self.logger.info(f"ğŸ’¡ å¤§æ‰¹é‡å¤„ç†æ¨¡å¼ ({self.batch_size:,})ï¼Œç¡®ä¿å†…å­˜å……è¶³")
    
    def _init_connection_pool(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        self.logger.info("ğŸ”— åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        for i in range(self.connection_pool_size):
            writer = DWDWriter()
            if writer.connect():
                self.writer_pool.append(writer)
                self.logger.info(f"âœ… è¿æ¥ {i+1} å»ºç«‹æˆåŠŸ")
            else:
                self.logger.error(f"âŒ è¿æ¥ {i+1} å»ºç«‹å¤±è´¥")
                
        if not self.writer_pool:
            raise RuntimeError("âŒ æ— æ³•å»ºç«‹ä»»ä½•æ•°æ®åº“è¿æ¥")
        
        self.logger.info(f"ğŸ”— è¿æ¥æ± åˆå§‹åŒ–å®Œæˆï¼š{len(self.writer_pool)} ä¸ªè¿æ¥")
    
    def get_writer(self) -> Optional[DWDWriter]:
        """ä»è¿æ¥æ± è·å–Writerï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.stats_lock:
            if self.writer_pool:
                return self.writer_pool.pop()
        return None
    
    def return_writer(self, writer: DWDWriter):
        """å½’è¿˜Writeråˆ°è¿æ¥æ± ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.stats_lock:
            if writer and len(self.writer_pool) < self.connection_pool_size:
                self.writer_pool.append(writer)
    
    def _optimized_batch_write(self, writer: DWDWriter, batch_data: List[Dict]) -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„æ‰¹é‡æ•°æ®åº“å†™å…¥
        
        ç‰¹æ€§:
        1. é¢„å¤„ç†æ•°æ®éªŒè¯ï¼Œå‡å°‘æ•°æ®åº“è´Ÿè½½
        2. æ™ºèƒ½é‡è¯•æœºåˆ¶
        3. æ‰¹é‡å¤§å°åŠ¨æ€è°ƒæ•´
        4. è¿æ¥å¥åº·æ£€æŸ¥
        """
        if not batch_data:
            return {'success': True, 'count': 0, 'message': 'æ— æ•°æ®å†™å…¥'}
        
        try:
            # 1. é¢„éªŒè¯æ•°æ® - è¿‡æ»¤æ˜æ˜¾æ— æ•ˆè®°å½•
            valid_batch = []
            for record in batch_data:
                # åŸºç¡€å­—æ®µéªŒè¯
                if (record.get('log_time') and 
                    record.get('client_ip') and 
                    record.get('request_uri')):
                    valid_batch.append(record)
            
            if not valid_batch:
                return {
                    'success': False, 
                    'count': 0, 
                    'error': 'æ‰¹æ¬¡ä¸­æ— æœ‰æ•ˆè®°å½•'
                }
            
            # 2. è¿æ¥å¥åº·æ£€æŸ¥
            if not writer.test_connection():
                # å°è¯•é‡è¿ä¸€æ¬¡
                if not writer.connect():
                    return {
                        'success': False,
                        'count': 0, 
                        'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'
                    }
            
            # 3. æ‰§è¡Œå†™å…¥ï¼ˆæ”¯æŒå¤§æ‰¹é‡ä¼˜åŒ–ï¼‰
            if len(valid_batch) > 2000:
                # å¤§æ‰¹é‡åˆ†å—å†™å…¥ï¼Œå‡å°‘å†…å­˜å‹åŠ›
                total_written = 0
                chunk_size = 1000
                
                for i in range(0, len(valid_batch), chunk_size):
                    chunk = valid_batch[i:i + chunk_size]
                    result = writer.write_batch(chunk)
                    
                    if result['success']:
                        total_written += result['count']
                    else:
                        return {
                            'success': False,
                            'count': total_written,
                            'error': f'åˆ†å—å†™å…¥å¤±è´¥: {result.get("error", "æœªçŸ¥é”™è¯¯")}'
                        }
                
                return {
                    'success': True,
                    'count': total_written,
                    'message': f'åˆ†å—å†™å…¥å®Œæˆ: {total_written} æ¡è®°å½•'
                }
            else:
                # æ­£å¸¸æ‰¹é‡å†™å…¥
                return writer.write_batch(valid_batch)
                
        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–æ‰¹é‡å†™å…¥å¼‚å¸¸: {e}")
            return {
                'success': False,
                'count': 0,
                'error': f'å†™å…¥å¼‚å¸¸: {str(e)}'
            }
    
    def cached_ua_parse(self, user_agent: str, mapper: FieldMapper) -> Dict[str, str]:
        """é«˜æ€§èƒ½User-Agentè§£æç¼“å­˜ä¼˜åŒ–"""
        # ä½¿ç”¨å“ˆå¸Œä¼˜åŒ–é•¿UAå­—ç¬¦ä¸²
        cache_key = hash(user_agent) if len(user_agent) > 50 else user_agent
        
        if cache_key in self.ua_cache:
            with self.stats_lock:
                self.cache_hit_stats['ua_hits'] += 1
                self.cache_hit_stats['total_requests'] += 1
            return self.ua_cache[cache_key]
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè§£æ
        result = mapper._parse_user_agent(user_agent)
        
        # æ™ºèƒ½ç¼“å­˜ç®¡ç†
        if len(self.ua_cache) < 20000:  # å¤§å¹…å¢åŠ ç¼“å­˜å®¹é‡
            self.ua_cache[cache_key] = result
        elif len(self.ua_cache) >= 20000:  # LRUæ¸…ç†
            import random
            keys_to_remove = random.sample(list(self.ua_cache.keys()), min(1000, len(self.ua_cache) // 10))
            for key in keys_to_remove:
                self.ua_cache.pop(key, None)
            self.ua_cache[cache_key] = result
        
        with self.stats_lock:
            self.cache_hit_stats['total_requests'] += 1
        
        return result
    
    def cached_uri_parse(self, uri: str, mapper: FieldMapper) -> Dict[str, str]:
        """é«˜æ€§èƒ½URIç»“æ„è§£æç¼“å­˜ä¼˜åŒ–"""
        cache_key = hash(uri) if len(uri) > 100 else uri
        
        if cache_key in self.uri_cache:
            with self.stats_lock:
                self.cache_hit_stats['uri_hits'] += 1
            return self.uri_cache[cache_key]
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè§£æ
        result = mapper._parse_uri_structure(uri)
        
        # æ™ºèƒ½ç¼“å­˜ç®¡ç†
        if len(self.uri_cache) < 15000:  # å¢åŠ URIç¼“å­˜å®¹é‡
            self.uri_cache[cache_key] = result
        elif len(self.uri_cache) >= 15000:
            # ä¼˜åŒ–çš„LRUæ¸…ç†
            import random
            keys_to_remove = random.sample(list(self.uri_cache.keys()), min(800, len(self.uri_cache) // 12))
            for key in keys_to_remove:
                self.uri_cache.pop(key, None)
            self.uri_cache[cache_key] = result
        
        return result
    
    def process_file_batch(self, file_paths: List[Path], thread_id: int, 
                          test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆå•çº¿ç¨‹æ‰§è¡Œï¼‰
        
        Args:
            file_paths: è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            thread_id: çº¿ç¨‹ID
            test_mode: æµ‹è¯•æ¨¡å¼
            limit: æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°é™åˆ¶
        """
        start_time = time.time()
        
        # è·å–çº¿ç¨‹ä¸“ç”¨ç»„ä»¶
        parser = self.parser_pool[thread_id % len(self.parser_pool)]
        mapper = self.mapper_pool[thread_id % len(self.mapper_pool)]
        writer = self.get_writer()
        
        if not writer and not test_mode:
            return {'success': False, 'error': 'æ— æ³•è·å–æ•°æ®åº“è¿æ¥', 'thread_id': thread_id}
        
        try:
            batch_stats = {
                'thread_id': thread_id,
                'total_files': len(file_paths),
                'processed_files': 0,
                'total_records': 0,
                'total_lines': 0,
                'errors': [],
                'file_results': []
            }
            
            # å¤§æ‰¹é‡ç¼“å†²åŒº
            mega_batch = []
            
            for file_path in file_paths:
                self.logger.info(f"ğŸ§µ{thread_id} å¤„ç†æ–‡ä»¶: {file_path.name}")
                
                file_start = time.time()
                file_records = 0
                file_lines = 0
                
                try:
                    # é«˜æ€§èƒ½æ‰¹é‡å¤„ç†æ–‡ä»¶ - å‡å°‘I/Oè°ƒç”¨
                    with open(file_path, 'r', encoding='utf-8', buffering=8192 * 4) as f:  # å¢å¤§ç¼“å†²åŒº
                        # æ‰¹é‡è¯»å–è¡Œä»¥å‡å°‘I/Oè°ƒç”¨
                        line_batch = []
                        for line in f:
                            line_batch.append(line.strip())
                            
                            # æ¯100è¡Œå¤„ç†ä¸€æ¬¡
                            if len(line_batch) >= 100:
                                # æ‰¹é‡è§£æ - ä½¿ç”¨ç°æœ‰çš„parse_lineæ–¹æ³•
                                for line_text in line_batch:
                                    parsed_data = parser.parse_line(line_text)
                                    file_lines += 1
                                    
                                    if parsed_data:
                                        # é«˜æ€§èƒ½å­—æ®µé¢„å¤„ç†
                                        request = parsed_data.get('request', '')
                                        user_agent = parsed_data.get('agent', '') or parsed_data.get('user_agent', '')
                                        
                                        # å¿«é€ŸURIæå–ï¼ˆé¿å…å¤šæ¬¡splitï¼‰
                                        if request and ' ' in request:
                                            uri = request.split(' ', 2)[1] if len(request.split(' ', 2)) > 1 else ''
                                        else:
                                            uri = ''
                                        
                                        # æ™ºèƒ½ç¼“å­˜ç­–ç•¥
                                        if user_agent and len(user_agent) > 10:  # è¿‡æ»¤æ— æ•ˆUA
                                            parsed_data['_cached_ua'] = self.cached_ua_parse(user_agent, mapper)
                                        if uri and len(uri) > 1:  # è¿‡æ»¤æ— æ•ˆURI
                                            parsed_data['_cached_uri'] = self.cached_uri_parse(uri, mapper)
                                        
                                        # å­—æ®µæ˜ å°„
                                        mapped_data = mapper.map_to_dwd(parsed_data, file_path.name)
                                        mega_batch.append(mapped_data)
                                        file_records += 1
                                        
                                        # ä¼˜åŒ–çš„æ‰¹é‡å†™å…¥æ£€æŸ¥
                                        if len(mega_batch) >= self.batch_size:
                                            if not test_mode:
                                                # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å†™å…¥
                                                write_result = self._optimized_batch_write(writer, mega_batch)
                                                if not write_result['success']:
                                                    batch_stats['errors'].append(f"{file_path.name}: {write_result['error']}")
                                            
                                            mega_batch.clear()  # æ¸…ç©ºæ‰¹æ¬¡
                                            
                                            # ä¼˜åŒ–åƒåœ¾å›æ”¶ç­–ç•¥ - å‡å°‘é¢‘ç‡
                                            if file_records % (self.batch_size * 3) == 0:
                                                gc.collect()
                                        
                                        # æ£€æŸ¥é™åˆ¶
                                        if limit and file_records >= limit:
                                            break
                                    
                                    if limit and file_records >= limit:
                                        break
                                
                                line_batch.clear()  # æ¸…ç©ºæ‰¹æ¬¡
                                
                                if limit and file_records >= limit:
                                    break
                        
                        # å¤„ç†æœ€åä¸è¶³100è¡Œçš„æ•°æ®
                        if line_batch and (not limit or file_records < limit):
                            for line_text in line_batch:
                                parsed_data = parser.parse_line(line_text)
                                if limit and file_records >= limit:
                                    break
                                file_lines += 1
                                if parsed_data:
                                    # åŒæ ·çš„å¤„ç†é€»è¾‘
                                    request = parsed_data.get('request', '')
                                    user_agent = parsed_data.get('agent', '') or parsed_data.get('user_agent', '')
                                    
                                    if request and ' ' in request:
                                        uri = request.split(' ', 2)[1] if len(request.split(' ', 2)) > 1 else ''
                                    else:
                                        uri = ''
                                    
                                    if user_agent and len(user_agent) > 10:
                                        parsed_data['_cached_ua'] = self.cached_ua_parse(user_agent, mapper)
                                    if uri and len(uri) > 1:
                                        parsed_data['_cached_uri'] = self.cached_uri_parse(uri, mapper)
                                    
                                    mapped_data = mapper.map_to_dwd(parsed_data, file_path.name)
                                    mega_batch.append(mapped_data)
                                    file_records += 1
                
                except Exception as e:
                    error_msg = f"æ–‡ä»¶å¤„ç†é”™è¯¯ {file_path.name}: {e}"
                    self.logger.error(error_msg)
                    batch_stats['errors'].append(error_msg)
                
                file_duration = time.time() - file_start
                batch_stats['file_results'].append({
                    'file': file_path.name,
                    'records': file_records,
                    'lines': file_lines,
                    'duration': file_duration
                })
                
                batch_stats['processed_files'] += 1
                batch_stats['total_records'] += file_records
                batch_stats['total_lines'] += file_lines
                
                self.logger.info(f"ğŸ§µ{thread_id} å®Œæˆ {file_path.name}: {file_records} è®°å½•, {file_duration:.2f}s")
            
            # å¤„ç†å‰©ä½™æ‰¹æ¬¡
            if mega_batch:
                if not test_mode:
                    # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å†™å…¥å¤„ç†æœ€ç»ˆæ‰¹æ¬¡
                    write_result = self._optimized_batch_write(writer, mega_batch)
                    if not write_result['success']:
                        batch_stats['errors'].append(f"æœ€ç»ˆæ‰¹æ¬¡å†™å…¥å¤±è´¥: {write_result['error']}")
                mega_batch.clear()
            
            batch_stats['duration'] = time.time() - start_time
            batch_stats['success'] = len(batch_stats['errors']) == 0
            
            return batch_stats
            
        finally:
            # å½’è¿˜è¿æ¥åˆ°æ± 
            if writer:
                self.return_writer(writer)
    
    def process_date_parallel(self, date_str: str, force_reprocess: bool = False,
                             test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """
        å¹¶è¡Œå¤„ç†æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰æ—¥å¿—
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDDæ ¼å¼)
            force_reprocess: å¼ºåˆ¶é‡æ–°å¤„ç†
            test_mode: æµ‹è¯•æ¨¡å¼
            limit: æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°é™åˆ¶
        """
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {date_str} çš„æ—¥å¿—")
        self.session_stats['start_time'] = datetime.now()
        start_time = time.time()
        
        # æ£€æŸ¥æ—¥æœŸç›®å½•
        date_dir = self.base_log_dir / date_str
        if not date_dir.exists():
            return {'success': False, 'error': f'æ—¥æœŸç›®å½•ä¸å­˜åœ¨: {date_dir}'}
        
        # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        all_log_files = list(date_dir.glob("*.log"))
        if not all_log_files:
            return {'success': False, 'error': f'ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.logæ–‡ä»¶: {date_dir}'}
        
        # è¿‡æ»¤éœ€è¦å¤„ç†çš„æ–‡ä»¶
        if not force_reprocess:
            pending_files = [f for f in all_log_files if not self.is_file_processed(f)]
        else:
            pending_files = all_log_files
        
        if not pending_files:
            self.logger.info(f"ğŸ“‹ æ—¥æœŸ {date_str} çš„æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†")
            return {'success': True, 'processed_files': 0, 'message': 'æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†'}
        
        self.logger.info(f"ğŸ“ æ‰¾åˆ° {len(pending_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        # å°†æ–‡ä»¶åˆ†æ‰¹åˆ†é…ç»™çº¿ç¨‹
        files_per_thread = max(1, len(pending_files) // self.max_workers)
        file_batches = []
        
        for i in range(0, len(pending_files), files_per_thread):
            batch = pending_files[i:i + files_per_thread]
            if batch:
                file_batches.append(batch)
        
        self.logger.info(f"ğŸ§µ åˆ†é… {len(file_batches)} ä¸ªæ‰¹æ¬¡ç»™ {min(self.max_workers, len(file_batches))} ä¸ªçº¿ç¨‹")
        
        # å¹¶è¡Œå¤„ç†
        all_results = []
        total_records = 0
        total_errors = []
        
        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(file_batches))) as executor:
            # æäº¤ä»»åŠ¡
            future_to_batch = {}
            for i, batch in enumerate(file_batches):
                future = executor.submit(self.process_file_batch, batch, i, test_mode, limit)
                future_to_batch[future] = i
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                batch_id = future_to_batch[future]
                try:
                    result = future.result()
                    all_results.append(result)
                    
                    if result['success']:
                        total_records += result['total_records']
                        self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ: {result['processed_files']} æ–‡ä»¶, "
                                       f"{result['total_records']} è®°å½•")
                    else:
                        total_errors.extend(result.get('errors', []))
                        self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_id} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        
                except Exception as e:
                    error_msg = f"æ‰¹æ¬¡ {batch_id} æ‰§è¡Œå¼‚å¸¸: {e}"
                    self.logger.error(error_msg)
                    total_errors.append(error_msg)
        
        # æ›´æ–°å¤„ç†çŠ¶æ€ï¼ˆéæµ‹è¯•æ¨¡å¼ï¼‰
        if not test_mode:
            for file_path in pending_files:
                # ç®€åŒ–çŠ¶æ€æ›´æ–°ï¼Œè¿™é‡Œå¯ä»¥æ ¹æ®å®é™…ç»“æœæ›´ç²¾ç¡®æ›´æ–°
                self.mark_file_processed(file_path, 0, 0)  # å ä½æ›´æ–°
            self.save_state()
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = 0.0
        if self.cache_hit_stats['total_requests'] > 0:
            total_hits = self.cache_hit_stats['ua_hits'] + self.cache_hit_stats['uri_hits']
            cache_hit_rate = (total_hits / (self.cache_hit_stats['total_requests'] * 2)) * 100
        
        duration = time.time() - start_time
        self.session_stats['end_time'] = datetime.now()
        
        # æ€§èƒ½ç»Ÿè®¡
        speed = total_records / duration if duration > 0 else 0
        self.session_stats['avg_processing_speed'] = speed
        self.session_stats['cache_hit_rate'] = cache_hit_rate
        self.session_stats['total_records_written'] = total_records
        
        success = len(total_errors) == 0
        
        result = {
            'success': success,
            'date': date_str,
            'processed_files': sum(r.get('processed_files', 0) for r in all_results),
            'total_records': total_records,
            'duration': duration,
            'processing_speed': speed,
            'cache_hit_rate': cache_hit_rate,
            'errors': total_errors,
            'thread_results': all_results
        }
        
        if success:
            self.logger.info(f"ğŸ‰ æ—¥æœŸ {date_str} å¹¶è¡Œå¤„ç†å®Œæˆ!")
            self.logger.info(f"ğŸ“Š {result['processed_files']} æ–‡ä»¶, {total_records:,} è®°å½•")
            self.logger.info(f"â±ï¸  è€—æ—¶ {duration:.2f}s, é€Ÿåº¦ {speed:.1f} è®°å½•/ç§’")
            self.logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%")
        
        return result
    
    def process_all_parallel(self, test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—"""
        self.logger.info("ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—")
        self.session_stats['start_time'] = datetime.now()
        start_time = time.time()
        
        # æ‰«ææ‰€æœ‰æ—¥æœŸç›®å½•
        log_files_by_date = self.scan_log_directories()
        if not log_files_by_date:
            return {'success': False, 'error': 'æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶'}
        
        total_records = 0
        processed_dates = 0
        all_errors = []
        date_results = []
        
        # ä¼˜åŒ–çš„å¹¶è¡Œå¤„ç†ç­–ç•¥
        total_dates = len(log_files_by_date)
        self.logger.info(f"ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å¹¶è¡Œå¤„ç†: {total_dates} ä¸ªæ—¥æœŸï¼Œ{self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        
        # é¢„çƒ­è¿æ¥æ±  - ç¡®ä¿æ‰€æœ‰è¿æ¥éƒ½æ˜¯æ´»è·ƒçš„
        self._warmup_connection_pool()
        
        # æŒ‰æ—¥æœŸé¡ºåºå¤„ç†ï¼ˆä½†æ¯ä¸ªæ—¥æœŸå†…éƒ¨å¹¶è¡Œï¼‰
        for idx, date_str in enumerate(sorted(log_files_by_date.keys()), 1):
            self.logger.info(f"ğŸ“… [{idx}/{total_dates}] å¼€å§‹å¤„ç†æ—¥æœŸ: {date_str}")
            
            # åŠ¨æ€è°ƒæ•´å¤„ç†ç­–ç•¥
            file_count = len(log_files_by_date[date_str])
            if file_count > self.max_workers * 2:
                # å¤§é‡æ–‡ä»¶æ—¶ï¼Œä¼˜åŒ–çº¿ç¨‹åˆ†é…
                self.logger.info(f"ğŸ“Š å¤§æ–‡ä»¶é›†åˆæ£€æµ‹åˆ° ({file_count} æ–‡ä»¶)ï¼Œå¯ç”¨é«˜æ€§èƒ½æ¨¡å¼")
            
            result = self.process_date_parallel(date_str, force_reprocess=False, 
                                              test_mode=test_mode, limit=limit)
            date_results.append(result)
            
            if result['success'] and result.get('processed_files', 0) > 0:
                processed_dates += 1
                total_records += result['total_records']
                self.logger.info(f"âœ… {date_str} å®Œæˆ: {result['total_records']:,} è®°å½•")
                
                # æœŸé—´ä¼˜åŒ– - å®šæœŸæ¸…ç†ç¼“å­˜å’Œåƒåœ¾å›æ”¶
                if idx % 3 == 0:  # æ¯3ä¸ªæ—¥æœŸæ¸…ç†ä¸€æ¬¡
                    self._periodic_cleanup()
                    
            else:
                if result.get('errors'):
                    all_errors.extend(result['errors'])
                self.logger.warning(f"âš ï¸ {date_str} è·³è¿‡æˆ–å¤±è´¥")
        
        duration = time.time() - start_time
        success = len(all_errors) == 0
        overall_speed = total_records / duration if duration > 0 else 0
        
        self.session_stats['end_time'] = datetime.now()
        self.session_stats['avg_processing_speed'] = overall_speed
        
        return {
            'success': success,
            'processed_dates': processed_dates,
            'total_records': total_records,
            'duration': duration,
            'processing_speed': overall_speed,
            'errors': all_errors,
            'date_results': date_results
        }
    
    def _warmup_connection_pool(self):
        """é¢„çƒ­è¿æ¥æ±  - ç¡®ä¿æ‰€æœ‰è¿æ¥éƒ½æ˜¯å¥åº·çš„"""
        self.logger.info("ğŸ”¥ é¢„çƒ­æ•°æ®åº“è¿æ¥æ± ...")
        healthy_connections = 0
        
        for writer in self.writer_pool[:]:  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸåˆ—è¡¨
            try:
                if writer.test_connection():
                    healthy_connections += 1
                else:
                    # ç§»é™¤ä¸å¥åº·çš„è¿æ¥å¹¶å°è¯•é‡è¿
                    self.writer_pool.remove(writer)
                    writer.close()
                    
                    # åˆ›å»ºæ–°è¿æ¥
                    new_writer = DWDWriter()
                    if new_writer.connect():
                        self.writer_pool.append(new_writer)
                        healthy_connections += 1
                        self.logger.info(f"ğŸ”„ é‡å»ºè¿æ¥æˆåŠŸ")
                    else:
                        self.logger.warning(f"âš ï¸ è¿æ¥é‡å»ºå¤±è´¥")
            except Exception as e:
                self.logger.error(f"âŒ è¿æ¥é¢„çƒ­å¼‚å¸¸: {e}")
        
        self.logger.info(f"ğŸ”¥ è¿æ¥æ± é¢„çƒ­å®Œæˆ: {healthy_connections}/{self.connection_pool_size} è¿æ¥å¥åº·")
    
    def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç† - ä¼˜åŒ–å†…å­˜å’Œç¼“å­˜ä½¿ç”¨"""
        self.logger.debug("ğŸ§¹ æ‰§è¡Œå®šæœŸæ¸…ç†...")
        
        # 1. å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        before_objects = len(gc.get_objects())
        gc.collect()
        after_objects = len(gc.get_objects())
        freed_objects = before_objects - after_objects
        
        # 2. æ™ºèƒ½ç¼“å­˜æ¸…ç† - å¦‚æœç¼“å­˜è¿‡å¤§ï¼Œæ¸…ç†éƒ¨åˆ†
        ua_cache_size = len(self.ua_cache)
        uri_cache_size = len(self.uri_cache)
        
        if ua_cache_size > 15000:  # UAç¼“å­˜æ¸…ç†é˜ˆå€¼
            import random
            keys_to_remove = random.sample(
                list(self.ua_cache.keys()), 
                min(3000, ua_cache_size // 4)
            )
            for key in keys_to_remove:
                self.ua_cache.pop(key, None)
            self.logger.debug(f"ğŸ§¹ UAç¼“å­˜æ¸…ç†: {len(keys_to_remove)} é¡¹")
        
        if uri_cache_size > 10000:  # URIç¼“å­˜æ¸…ç†é˜ˆå€¼
            import random
            keys_to_remove = random.sample(
                list(self.uri_cache.keys()), 
                min(2000, uri_cache_size // 4)
            )
            for key in keys_to_remove:
                self.uri_cache.pop(key, None)
            self.logger.debug(f"ğŸ§¹ URIç¼“å­˜æ¸…ç†: {len(keys_to_remove)} é¡¹")
        
        # 3. å†…å­˜ä½¿ç”¨æŠ¥å‘Š
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.session_stats['peak_memory_usage_mb'] = max(
                self.session_stats['peak_memory_usage_mb'], 
                memory_mb
            )
            
            if freed_objects > 0:
                self.logger.debug(f"ğŸ§¹ æ¸…ç†å®Œæˆ: é‡Šæ”¾ {freed_objects} ä¸ªå¯¹è±¡ï¼Œå†…å­˜ä½¿ç”¨ {memory_mb:.1f}MB")
        except ImportError:
            pass  # psutilä¸å¯ç”¨æ—¶è·³è¿‡å†…å­˜ç›‘æ§
    
    # === å…¼å®¹æ€§æ–¹æ³• ===
    
    def load_state(self) -> Dict[str, Any]:
        """åŠ è½½å¤„ç†çŠ¶æ€ï¼ˆç»§æ‰¿è‡ªåŸç‰ˆæœ¬ï¼‰"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
        
        return {
            'processed_files': {},
            'last_update': None,
            'total_processed_records': 0,
            'processing_history': []
        }
    
    def save_state(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        try:
            self.processed_state['last_update'] = datetime.now().isoformat()
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_state, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            self.logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
    
    def scan_log_directories(self) -> Dict[str, List[Path]]:
        """æ‰«ææ—¥å¿—ç›®å½•ï¼ˆç»§æ‰¿è‡ªåŸç‰ˆæœ¬ï¼‰"""
        if not self.base_log_dir.exists():
            return {}
        
        log_files_by_date = {}
        for date_dir in self.base_log_dir.iterdir():
            if date_dir.is_dir() and date_dir.name.isdigit() and len(date_dir.name) == 8:
                try:
                    datetime.strptime(date_dir.name, '%Y%m%d')
                    log_files = list(date_dir.glob("*.log"))
                    if log_files:
                        log_files_by_date[date_dir.name] = sorted(log_files)
                except ValueError:
                    continue
        
        return log_files_by_date
    
    def is_file_processed(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        file_key = str(file_path)
        return file_key in self.processed_state.get('processed_files', {})
    
    def mark_file_processed(self, file_path: Path, record_count: int, processing_time: float):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        file_key = str(file_path)
        try:
            self.processed_state['processed_files'][file_key] = {
                'processed_at': datetime.now().isoformat(),
                'record_count': record_count,
                'processing_time': processing_time,
                'mtime': file_path.stat().st_mtime,
                'file_size': file_path.stat().st_size
            }
        except Exception as e:
            self.logger.error(f"æ ‡è®°æ–‡ä»¶çŠ¶æ€å¤±è´¥ {file_path}: {e}")
    
    def show_performance_stats(self):
        """æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print("\\n" + "=" * 80)
        print("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 80)
        
        print(f"âš™ï¸  é…ç½®ä¿¡æ¯:")
        print(f"   æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        print(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        print(f"   è¿æ¥æ± å¤§å°: {self.connection_pool_size}")
        print(f"   å½“å‰å¯ç”¨è¿æ¥: {len(self.writer_pool)}")
        
        print(f"\\nğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:")
        print(f"   User-Agentç¼“å­˜: {len(self.ua_cache)} é¡¹")
        print(f"   URIç¼“å­˜: {len(self.uri_cache)} é¡¹")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {self.session_stats.get('cache_hit_rate', 0):.1f}%")
        
        if self.session_stats.get('avg_processing_speed', 0) > 0:
            print(f"\\nğŸƒ æ€§èƒ½æŒ‡æ ‡:")
            print(f"   å¹³å‡å¤„ç†é€Ÿåº¦: {self.session_stats['avg_processing_speed']:.1f} è®°å½•/ç§’")
            print(f"   æ€»å¤„ç†è®°å½•æ•°: {self.session_stats.get('total_records_written', 0):,}")
            
            if self.session_stats.get('start_time') and self.session_stats.get('end_time'):
                duration = (self.session_stats['end_time'] - self.session_stats['start_time']).total_seconds()
                print(f"   æ€»å¤„ç†æ—¶é—´: {duration:.1f} ç§’")
        
        print("=" * 80)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
        
        # å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥
        for writer in self.writer_pool:
            try:
                writer.close()
            except:
                pass
        self.writer_pool.clear()
        
        # æ¸…ç†ç¼“å­˜
        self.ua_cache.clear()
        self.uri_cache.clear()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        self.logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
    
    # === äº¤äº’å¼èœå•åŠŸèƒ½ ===
    
    def interactive_menu(self):
        """äº¤äº’å¼èœå• - é«˜æ€§èƒ½ç‰ˆæœ¬"""
        while True:
            print("\\n" + "=" * 80)
            print("ğŸš€ é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ç‰ˆæœ¬")
            print("=" * 80)
            print("1. ğŸ”¥ é«˜æ€§èƒ½å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿— (æ¨è)")
            print("2. ğŸ“… é«˜æ€§èƒ½å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ—¥å¿—")
            print("3. ğŸ“Š æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€å’Œæ€§èƒ½ç»Ÿè®¡")
            print("4. ğŸ§ª æµ‹è¯•æ¨¡å¼å¤„ç† (ä¸å†™å…¥æ•°æ®åº“)")
            print("5. âš™ï¸ æ€§èƒ½é…ç½®è°ƒä¼˜")
            print("6. ğŸ§¹ æ¸…ç©ºæ‰€æœ‰æ•°æ® (å¼€å‘ç¯å¢ƒ)")
            print("7. ğŸ” å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ—¥å¿—")
            print("8. ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•")
            print("0. ğŸ‘‹ é€€å‡º")
            print("-" * 80)
            print(f"ğŸ“Š å½“å‰é…ç½®: æ‰¹é‡{self.batch_size} | çº¿ç¨‹{self.max_workers} | è¿æ¥æ± {self.connection_pool_size}")
            
            try:
                choice = input("è¯·é€‰æ‹©æ“ä½œ [0-8]: ").strip()
                
                if choice == '0':
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                elif choice == '1':
                    print("\\nğŸ”¥ é«˜æ€§èƒ½å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—...")
                    
                    # è¯¢é—®é…ç½®
                    limit_input = input("æ˜¯å¦é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•°ï¼Ÿ(ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None
                    
                    confirm = input(f"ç¡®è®¤ä½¿ç”¨å½“å‰é…ç½®å¤„ç†å—ï¼Ÿæ‰¹é‡{self.batch_size}ï¼Œ{self.max_workers}çº¿ç¨‹ (y/N): ").strip().lower()
                    if confirm != 'y':
                        print("æ“ä½œå·²å–æ¶ˆ")
                        continue
                    
                    start_time = time.time()
                    result = self.process_all_parallel(test_mode=False, limit=limit)
                    total_time = time.time() - start_time
                    
                    self._print_batch_process_result(result, total_time)
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                elif choice == '2':
                    date_str = input("\\nè¯·è¾“å…¥æ—¥æœŸ (YYYYMMDDæ ¼å¼ï¼Œå¦‚: 20250901): ").strip()
                    if not self._validate_date_format(date_str):
                        continue
                    
                    force = input("æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†ï¼Ÿ(y/N): ").strip().lower() == 'y'
                    limit_input = input("æ˜¯å¦é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•°ï¼Ÿ(ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                    limit = int(limit_input) if limit_input.isdigit() else None
                    
                    print(f"\\nğŸ”¥ é«˜æ€§èƒ½å¤„ç† {date_str} çš„æ—¥å¿—...")
                    start_time = time.time()
                    result = self.process_date_parallel(date_str, force, test_mode=False, limit=limit)
                    total_time = time.time() - start_time
                    
                    self._print_single_date_result(result, total_time)
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                elif choice == '3':
                    print()
                    self.show_status()
                    self.show_performance_stats()
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                elif choice == '4':
                    print("\\nğŸ§ª é«˜æ€§èƒ½æµ‹è¯•æ¨¡å¼å¤„ç†")
                    sub_choice = input("é€‰æ‹©: 1)å¤„ç†æ‰€æœ‰æœªå¤„ç†æ—¥å¿— 2)å¤„ç†æŒ‡å®šæ—¥æœŸ [1-2]: ").strip()
                    
                    if sub_choice == '1':
                        limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (å»ºè®®100-1000): ").strip()
                        limit = int(limit_input) if limit_input.isdigit() else 100
                        
                        print("\\nğŸ§ª é«˜æ€§èƒ½æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ—¥å¿—...")
                        result = self.process_all_parallel(test_mode=True, limit=limit)
                        self._print_batch_process_result(result)
                        
                    elif sub_choice == '2':
                        date_str = input("è¯·è¾“å…¥æ—¥æœŸ (YYYYMMDDæ ¼å¼): ").strip()
                        if not self._validate_date_format(date_str):
                            continue
                        
                        limit_input = input("é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•° (å»ºè®®100-1000): ").strip()
                        limit = int(limit_input) if limit_input.isdigit() else 100
                        
                        print(f"\\nğŸ§ª é«˜æ€§èƒ½æµ‹è¯•æ¨¡å¼ï¼šå¤„ç† {date_str} çš„æ—¥å¿—...")
                        result = self.process_date_parallel(date_str, False, test_mode=True, limit=limit)
                        self._print_single_date_result(result)
                    
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                elif choice == '5':
                    print("\\nâš™ï¸ æ€§èƒ½é…ç½®è°ƒä¼˜")
                    print(f"å½“å‰é…ç½®:")
                    print(f"  æ‰¹é‡å¤§å°: {self.batch_size}")
                    print(f"  æœ€å¤§çº¿ç¨‹: {self.max_workers}")
                    print(f"  è¿æ¥æ± å¤§å°: {self.connection_pool_size}")
                    
                    print("\\næ¨èé…ç½®:")
                    print("  å°å‹æœåŠ¡å™¨: æ‰¹é‡2000, çº¿ç¨‹4, è¿æ¥æ± 4")
                    print("  ä¸­å‹æœåŠ¡å™¨: æ‰¹é‡5000, çº¿ç¨‹6, è¿æ¥æ± 6")
                    print("  é«˜æ€§èƒ½æœåŠ¡å™¨: æ‰¹é‡10000, çº¿ç¨‹8, è¿æ¥æ± 8")
                    
                    new_batch = input("\\næ–°çš„æ‰¹é‡å¤§å° (ç•™ç©ºä¿æŒå½“å‰): ").strip()
                    new_workers = input("æ–°çš„çº¿ç¨‹æ•° (ç•™ç©ºä¿æŒå½“å‰): ").strip()
                    
                    if new_batch.isdigit():
                        self.batch_size = int(new_batch)
                        print(f"âœ… æ‰¹é‡å¤§å°å·²è°ƒæ•´ä¸º: {self.batch_size}")
                    
                    if new_workers.isdigit():
                        old_workers = self.max_workers
                        self.max_workers = int(new_workers)
                        print(f"âœ… çº¿ç¨‹æ•°å·²è°ƒæ•´ä¸º: {self.max_workers}")
                        
                        # é‡æ–°åˆå§‹åŒ–ç»„ä»¶æ± 
                        if self.max_workers != old_workers:
                            print("ğŸ”„ é‡æ–°åˆå§‹åŒ–ç»„ä»¶æ± ...")
                            self.parser_pool = [BaseLogParser() for _ in range(self.max_workers)]
                            self.mapper_pool = [FieldMapper() for _ in range(self.max_workers)]
                    
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                elif choice == '6':
                    print("\\nâš ï¸  æ¸…ç©ºæ‰€æœ‰æ•°æ®")
                    confirm = input("ç¡®è®¤æ¸…ç©ºæ‰€æœ‰ETLæ•°æ®ï¼Ÿè¿™å°†åˆ é™¤æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ—¥å¿—æ•°æ® (y/N): ").strip().lower()
                    if confirm == 'y':
                        second_confirm = input("å†æ¬¡ç¡®è®¤ï¼è¾“å…¥ 'CLEAR' ç¡®è®¤åˆ é™¤: ").strip()
                        if second_confirm == 'CLEAR':
                            self.clear_all_data()
                        else:
                            print("âŒ ç¡®è®¤å¤±è´¥ï¼Œæ“ä½œå·²å–æ¶ˆ")
                    else:
                        print("âŒ æ“ä½œå·²å–æ¶ˆ")
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                elif choice == '7':
                    print("\\nâš ï¸  å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ—¥å¿—")
                    print("è¿™å°†å¿½ç•¥å¤„ç†çŠ¶æ€ï¼Œé‡æ–°å¤„ç†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶")
                    confirm = input("ç¡®è®¤æ‰§è¡Œï¼Ÿ(y/N): ").strip().lower()
                    if confirm == 'y':
                        limit_input = input("æ˜¯å¦é™åˆ¶æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†è¡Œæ•°ï¼Ÿ(ç•™ç©ºè¡¨ç¤ºä¸é™åˆ¶): ").strip()
                        limit = int(limit_input) if limit_input.isdigit() else None
                        
                        print("\\nğŸ”¥ å¼€å§‹å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ—¥å¿—...")
                        
                        # ä¸´æ—¶å¤‡ä»½çŠ¶æ€
                        backup_state = self.processed_state.copy()
                        
                        # æ¸…ç©ºçŠ¶æ€ä»¥å¼ºåˆ¶é‡å¤„ç†
                        self.processed_state = {
                            'processed_files': {},
                            'last_update': None,
                            'total_processed_records': 0,
                            'processing_history': []
                        }
                        
                        result = self.process_all_parallel(test_mode=False, limit=limit)
                        
                        # å¦‚æœå¤±è´¥ï¼Œæ¢å¤çŠ¶æ€
                        if not result['success']:
                            self.processed_state = backup_state
                            print("âŒ å¤„ç†å¤±è´¥ï¼Œå·²æ¢å¤åŸå§‹çŠ¶æ€")
                        
                        self._print_batch_process_result(result)
                    else:
                        print("âŒ æ“ä½œå·²å–æ¶ˆ")
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                elif choice == '8':
                    print("\\nğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•")
                    print("è¿™å°†è¿è¡Œå¤šç§é…ç½®çš„æ€§èƒ½æµ‹è¯•")
                    confirm = input("ç¡®è®¤è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Ÿ(y/N): ").strip().lower()
                    
                    if confirm == 'y':
                        self._run_performance_benchmark()
                    
                    input("\\næŒ‰å›è½¦é”®ç»§ç»­...")
                
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 0-8")
                    input("æŒ‰å›è½¦é”®ç»§ç»­...")
                    
            except KeyboardInterrupt:
                print("\\n\\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"\\nâŒ æ“ä½œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                input("æŒ‰å›è½¦é”®ç»§ç»­...")
    
    def show_status(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        print("=" * 80)
        print("ğŸ“Š é«˜æ€§èƒ½ETLæ§åˆ¶å™¨çŠ¶æ€æŠ¥å‘Š")
        print("=" * 80)
        
        # 1. æ—¥å¿—ç›®å½•çŠ¶æ€
        log_files_by_date = self.scan_log_directories()
        print(f"ğŸ“ æ—¥å¿—æ ¹ç›®å½•: {self.base_log_dir}")
        print(f"   æ‰¾åˆ° {len(log_files_by_date)} ä¸ªæ—¥æœŸç›®å½•")
        
        if log_files_by_date:
            total_files = sum(len(files) for files in log_files_by_date.values())
            print(f"   æ€»è®¡ {total_files} ä¸ªæ—¥å¿—æ–‡ä»¶")
            
            # æ˜¾ç¤ºæœ€è¿‘çš„å‡ ä¸ªæ—¥æœŸ
            recent_dates = sorted(log_files_by_date.keys())[-5:]
            print(f"   æœ€è¿‘æ—¥æœŸ: {', '.join(recent_dates)}")
            
            # æ˜¾ç¤ºæ¯ä¸ªæ—¥æœŸçš„æ–‡ä»¶æ•°
            print("   å„æ—¥æœŸæ–‡ä»¶ç»Ÿè®¡:")
            for date_str in sorted(log_files_by_date.keys())[-10:]:
                file_count = len(log_files_by_date[date_str])
                processed_count = sum(1 for f in log_files_by_date[date_str] if self.is_file_processed(f))
                status = "âœ…" if processed_count == file_count else f"âš ï¸ {processed_count}/{file_count}"
                print(f"     {date_str}: {file_count} ä¸ªæ–‡ä»¶ {status}")
        
        # 2. å¤„ç†çŠ¶æ€ç»Ÿè®¡
        processed_files_count = len(self.processed_state.get('processed_files', {}))
        total_processed_records = sum(
            info.get('record_count', 0) 
            for info in self.processed_state.get('processed_files', {}).values()
        )
        
        print(f"\\nğŸ“ˆ å¤„ç†çŠ¶æ€ç»Ÿè®¡:")
        print(f"   å·²å¤„ç†æ–‡ä»¶: {processed_files_count} ä¸ª")
        print(f"   å·²å¤„ç†è®°å½•: {total_processed_records:,} æ¡")
        
        if self.processed_state.get('last_update'):
            print(f"   æœ€åæ›´æ–°: {self.processed_state['last_update']}")
        
        # 3. æœ€è¿‘å¤„ç†å†å²
        history = self.processed_state.get('processing_history', [])
        if history:
            print(f"\\nğŸ•’ æœ€è¿‘å¤„ç†è®°å½•:")
            for record in history[-5:]:
                date_str = record.get('date', 'Unknown')
                files = record.get('files', 0)
                records = record.get('records', 0)
                duration = record.get('duration', 0)
                processed_at = record.get('processed_at', '')[:19].replace('T', ' ')
                print(f"     {date_str} - {processed_at}: {files} æ–‡ä»¶, {records:,} è®°å½•, {duration:.1f}s")
    
    def clear_all_data(self):
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼ˆå¼€å‘ç¯å¢ƒä½¿ç”¨ï¼‰"""
        self.logger.info("å¼€å§‹æ¸…ç©ºæ‰€æœ‰æ•°æ®")
        
        print("âš ï¸  è­¦å‘Šï¼šè¿™å°†æ¸…ç©ºæ‰€æœ‰ETLæ•°æ®å’Œå¤„ç†çŠ¶æ€")
        print("1. æ¸…ç©ºClickHouseæ•°æ®åº“è¡¨")
        print("2. é‡ç½®å¤„ç†çŠ¶æ€æ–‡ä»¶")
        
        # è·å–Writerè¿›è¡Œæ¸…ç†
        writer = self.get_writer()
        if not writer:
            print("âŒ æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        try:
            tables_to_clear = ['dwd_nginx_enriched_v3']
            cleared_count = 0
            
            for table_name in tables_to_clear:
                try:
                    result = writer.client.command(f"TRUNCATE TABLE {table_name}")
                    print(f"âœ… å·²æ¸…ç©ºè¡¨: {table_name}")
                    cleared_count += 1
                except Exception as e:
                    print(f"âŒ æ¸…ç©ºè¡¨å¤±è´¥ {table_name}: {e}")
            
            print(f"âœ… æˆåŠŸæ¸…ç©º {cleared_count}/{len(tables_to_clear)} ä¸ªè¡¨")
            
            # é‡ç½®çŠ¶æ€æ–‡ä»¶
            self.processed_state = {
                'processed_files': {},
                'last_update': None,
                'total_processed_records': 0,
                'processing_history': []
            }
            self.save_state()
            print("âœ… å¤„ç†çŠ¶æ€å·²é‡ç½®")
            
            # é‡ç½®ä¼šè¯ç»Ÿè®¡
            self.session_stats = {
                'start_time': None,
                'end_time': None,
                'total_files_processed': 0,
                'total_lines_processed': 0,
                'total_records_written': 0,
                'total_errors': 0,
                'cache_hit_rate': 0.0,
                'avg_processing_speed': 0.0,
                'peak_memory_usage_mb': 0,
                'processing_errors': []
            }
            
        finally:
            self.return_writer(writer)
    
    def _run_performance_benchmark(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\\nğŸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        # è·å–æµ‹è¯•æ—¥æœŸ
        log_files = self.scan_log_directories()
        if not log_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶è¿›è¡Œæµ‹è¯•")
            return
        
        test_date = sorted(log_files.keys())[-1]
        print(f"ğŸ“… ä½¿ç”¨æµ‹è¯•æ—¥æœŸ: {test_date}")
        
        # ä¸åŒé…ç½®çš„åŸºå‡†æµ‹è¯•
        configs = [
            {'batch_size': 1000, 'workers': 2, 'name': 'åŸºç¡€é…ç½®'},
            {'batch_size': 2000, 'workers': 4, 'name': 'æ ‡å‡†é…ç½®'},
            {'batch_size': 5000, 'workers': 6, 'name': 'é«˜æ€§èƒ½é…ç½®'}
        ]
        
        results = []
        
        for config in configs:
            print(f"\\nğŸ”§ æµ‹è¯• {config['name']}: æ‰¹é‡{config['batch_size']}, çº¿ç¨‹{config['workers']}")
            
            # ä¸´æ—¶è°ƒæ•´é…ç½®
            old_batch = self.batch_size
            old_workers = self.max_workers
            
            self.batch_size = config['batch_size']
            self.max_workers = config['workers']
            
            try:
                start_time = time.time()
                result = self.process_date_parallel(
                    test_date, 
                    force_reprocess=True, 
                    test_mode=True, 
                    limit=200  # é™åˆ¶æµ‹è¯•è§„æ¨¡
                )
                duration = time.time() - start_time
                
                if result['success']:
                    speed = result.get('processing_speed', 0)
                    results.append({
                        'config': config['name'],
                        'batch_size': config['batch_size'],
                        'workers': config['workers'],
                        'records': result.get('total_records', 0),
                        'duration': duration,
                        'speed': speed
                    })
                    print(f"   âœ… é€Ÿåº¦: {speed:.1f} è®°å½•/ç§’")
                else:
                    print(f"   âŒ æµ‹è¯•å¤±è´¥")
            
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            
            finally:
                # æ¢å¤åŸé…ç½®
                self.batch_size = old_batch
                self.max_workers = old_workers
        
        # æ˜¾ç¤ºåŸºå‡†æµ‹è¯•ç»“æœ
        if results:
            print("\\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
            print(f"{'é…ç½®':<15} {'æ‰¹é‡':<8} {'çº¿ç¨‹':<6} {'è®°å½•æ•°':<8} {'è€—æ—¶(s)':<8} {'é€Ÿåº¦(rec/s)':<12}")
            print("-" * 65)
            
            for r in results:
                print(f"{r['config']:<15} {r['batch_size']:<8} {r['workers']:<6} {r['records']:<8} {r['duration']:<8.2f} {r['speed']:<12.1f}")
            
            # æ¨èæœ€ä½³é…ç½®
            best = max(results, key=lambda x: x['speed'])
            print(f"\\nğŸ† æ¨èé…ç½®: {best['config']}")
            print(f"   æ‰¹é‡å¤§å°: {best['batch_size']}")
            print(f"   çº¿ç¨‹æ•°: {best['workers']}")
            print(f"   é¢„æœŸé€Ÿåº¦: {best['speed']:.1f} è®°å½•/ç§’")
    
    def _validate_date_format(self, date_str: str) -> bool:
        """éªŒè¯æ—¥æœŸæ ¼å¼"""
        if not date_str or len(date_str) != 8 or not date_str.isdigit():
            print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨YYYYMMDDæ ¼å¼")
            return False
        
        try:
            datetime.strptime(date_str, '%Y%m%d')
            return True
        except ValueError:
            print("âŒ æ— æ•ˆçš„æ—¥æœŸ")
            return False
    
    def _print_batch_process_result(self, result: Dict[str, Any], total_time: float = None):
        """æ‰“å°æ‰¹é‡å¤„ç†ç»“æœ"""
        print("\\n" + "=" * 60)
        if result['success']:
            print("âœ… é«˜æ€§èƒ½æ‰¹é‡å¤„ç†æˆåŠŸ!")
            print("=" * 60)
            print(f"ğŸ“„ å¤„ç†æ—¥æœŸæ•°: {result.get('processed_dates', 0)} ä¸ª")
            print(f"ğŸ“Š æ€»è®°å½•æ•°: {result.get('total_records', 0):,} æ¡")
            print(f"â±ï¸  æ€»è€—æ—¶: {result.get('duration', total_time or 0):.2f} ç§’")
            
            if result.get('processing_speed', 0) > 0:
                print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {result['processing_speed']:.1f} è®°å½•/ç§’")
                
        else:
            print("âŒ é«˜æ€§èƒ½æ‰¹é‡å¤„ç†å¤±è´¥!")
            print("=" * 60)
            print(f"âŒ é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            errors = result.get('errors', [])
            if errors:
                print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯ ({len(errors)} ä¸ª):")
                for i, error in enumerate(errors[:3], 1):
                    print(f"   {i}. {error}")
                if len(errors) > 3:
                    print(f"   ... è¿˜æœ‰ {len(errors) - 3} ä¸ªé”™è¯¯")
        print("=" * 60)
    
    def _print_single_date_result(self, result: Dict[str, Any], total_time: float = None):
        """æ‰“å°å•ä¸ªæ—¥æœŸçš„å¤„ç†ç»“æœ"""
        print("\\n" + "=" * 60)
        if result['success']:
            print("âœ… é«˜æ€§èƒ½æ—¥æœŸå¤„ç†æˆåŠŸ!")
            print("=" * 60)
            print(f"ğŸ“… å¤„ç†æ—¥æœŸ: {result['date']}")
            print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {result['processed_files']} ä¸ª")
            print(f"ğŸ“Š æ€»è®°å½•æ•°: {result['total_records']:,} æ¡")
            print(f"â±ï¸  æ€»è€—æ—¶: {result.get('duration', total_time or 0):.2f} ç§’")
            
            if result.get('processing_speed', 0) > 0:
                print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {result['processing_speed']:.1f} è®°å½•/ç§’")
            
            if result.get('cache_hit_rate', 0) > 0:
                print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {result['cache_hit_rate']:.1f}%")
                
        else:
            print("âŒ é«˜æ€§èƒ½æ—¥æœŸå¤„ç†å¤±è´¥!")
            print("=" * 60)
            print(f"ğŸ“… å¤„ç†æ—¥æœŸ: {result['date']}")
            print(f"âŒ é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            errors = result.get('errors', [])
            if errors:
                print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯:")
                for i, error in enumerate(errors, 1):
                    print(f"   {i}. {error}")
        print("=" * 60)

    def print_error_report(self):
        """æ‰“å°è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š"""
        summary = self.get_error_summary()

        print("\\n" + "="*70)
        print("ğŸ“Š é”™è¯¯ç»Ÿè®¡æŠ¥å‘Š")
        print("="*70)

        if summary['total_errors'] == 0:
            print("âœ… æ²¡æœ‰å‘ç°é”™è¯¯ - å¤„ç†å®Œå…¨æˆåŠŸï¼")
            return

        print(f"ğŸ“ˆ æ€»é”™è¯¯æ•°: {summary['total_errors']}")
        print(f"ğŸ“ˆ é”™è¯¯ç‡: {summary['error_rate_percent']}%")

        # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
        if summary['by_error_type']:
            print("\\nğŸ“‹ é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
            for error_type, count in summary['by_error_type']:
                print(f"   {error_type}: {count} æ¬¡")

        # æœ€è¿‘é”™è¯¯
        if summary['recent_errors']:
            print(f"\\nğŸ• æœ€è¿‘é”™è¯¯ (æœ€å¤šæ˜¾ç¤º5ä¸ª):")
            for error in summary['recent_errors'][:5]:
                print(f"   [{error['timestamp']}] {error['error_type']}: {error['message'][:100]}")

        print("="*70)

    # === é”™è¯¯å¤„ç†å’Œç»Ÿè®¡æ–¹æ³• ===

    def record_error(self, error_type: str, error_msg: str, context: Dict[str, Any] = None):
        """è®°å½•é”™è¯¯ä¿¡æ¯å’Œç»Ÿè®¡"""
        with self.stats_lock:
            # æ›´æ–°é”™è¯¯ç»Ÿè®¡
            if error_type in self.session_stats['error_stats']:
                self.session_stats['error_stats'][error_type] += 1
            else:
                self.session_stats['error_stats'][error_type] = 1

            self.session_stats['total_errors'] += 1

            # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
            error_detail = {
                'timestamp': datetime.now().isoformat(),
                'error_type': error_type,
                'error_message': error_msg,
                'context': context or {}
            }

            self.session_stats['error_details'].append(error_detail)

            # æŒ‰æ–‡ä»¶ç»Ÿè®¡é”™è¯¯
            if context and 'source_file' in context:
                file_name = context['source_file']
                if file_name not in self.session_stats['file_error_stats']:
                    self.session_stats['file_error_stats'][file_name] = {
                        'total_errors': 0,
                        'error_types': defaultdict(int)
                    }

                self.session_stats['file_error_stats'][file_name]['total_errors'] += 1
                self.session_stats['file_error_stats'][file_name]['error_types'][error_type] += 1

            # é™åˆ¶é”™è¯¯è¯¦æƒ…æ•°é‡ï¼Œé¿å…å†…å­˜æº¢å‡º
            if len(self.session_stats['error_details']) > 1000:
                self.session_stats['error_details'] = self.session_stats['error_details'][-500:]

    def record_fallback_record(self, context: Dict[str, Any] = None):
        """è®°å½•å®¹é”™å¤‡ç”¨è®°å½•"""
        self.record_error('fallback_records', 'ä½¿ç”¨å®¹é”™å¤‡ç”¨è®°å½•', context)

    def record_performance_warning(self, warning_msg: str, context: Dict[str, Any] = None):
        """è®°å½•æ€§èƒ½è­¦å‘Š"""
        with self.stats_lock:
            warning = {
                'timestamp': datetime.now().isoformat(),
                'message': warning_msg,
                'context': context or {}
            }
            self.session_stats['performance_warnings'].append(warning)

            # é™åˆ¶è­¦å‘Šæ•°é‡
            if len(self.session_stats['performance_warnings']) > 100:
                self.session_stats['performance_warnings'] = self.session_stats['performance_warnings'][-50:]

    def get_error_summary(self) -> Dict[str, Any]:
        """è·å–é”™è¯¯æ‘˜è¦æŠ¥å‘Š"""
        with self.stats_lock:
            total_errors = self.session_stats['total_errors']
            total_records = self.session_stats['total_records_written']

            if total_records == 0:
                error_rate = 0.0
            else:
                error_rate = (total_errors / (total_records + total_errors)) * 100

            return {
                'total_errors': total_errors,
                'error_rate_percent': round(error_rate, 2),
                'error_breakdown': dict(self.session_stats['error_stats']),
                'files_with_errors': len(self.session_stats['file_error_stats']),
                'performance_warnings_count': len(self.session_stats['performance_warnings']),
                'most_common_errors': self._get_most_common_errors(),
                'by_error_type': [],
                'recent_errors': []
            }

    def _get_most_common_errors(self) -> List[Dict[str, Any]]:
        """è·å–æœ€å¸¸è§çš„é”™è¯¯ç±»å‹"""
        error_counts = self.session_stats['error_stats']
        sorted_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)
        return [{'error_type': k, 'count': v} for k, v in sorted_errors[:5] if v > 0]

def main():
    """ä¸»å‡½æ•°"""
    import logging
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    parser = argparse.ArgumentParser(
        description='é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ç‰ˆæœ¬'
    )
    
    parser.add_argument('--date', help='å¤„ç†æŒ‡å®šæ—¥æœŸ (YYYYMMDDæ ¼å¼)')
    parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰æœªå¤„ç†æ—¥å¿—')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--limit', type=int, help='æ¯ä¸ªæ–‡ä»¶çš„è¡Œæ•°é™åˆ¶')
    parser.add_argument('--batch-size', type=int, default=2000, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--workers', type=int, default=4, help='å·¥ä½œçº¿ç¨‹æ•°')
    
    args = parser.parse_args()
    
    try:
        with HighPerformanceETLController(
            batch_size=args.batch_size,
            max_workers=args.workers
        ) as controller:
            
            # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºäº¤äº’å¼èœå•
            if not any([args.date, args.all]):
                controller.interactive_menu()
                return
            
            if args.date:
                result = controller.process_date_parallel(
                    args.date, 
                    force_reprocess=args.force,
                    test_mode=args.test,
                    limit=args.limit
                )
                
                print(f"\\nğŸ¯ å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸ: {result.get('date', args.date)}")
                print(f"æ–‡ä»¶: {result.get('processed_files', 0)}")
                print(f"è®°å½•: {result.get('total_records', 0):,}")
                print(f"è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")
                
            elif args.all:
                result = controller.process_all_parallel(
                    test_mode=args.test,
                    limit=args.limit
                )
                
                print(f"\\nğŸ¯ æ‰¹é‡å¤„ç†ç»“æœ:")
                print(f"æ—¥æœŸæ•°: {result.get('processed_dates', 0)}")
                print(f"è®°å½•æ•°: {result.get('total_records', 0):,}")
                print(f"æ€»è€—æ—¶: {result.get('duration', 0):.2f}s")
                print(f"å¹³å‡é€Ÿåº¦: {result.get('processing_speed', 0):.1f} è®°å½•/ç§’")
            
            else:
                # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
                parser.print_help()
                print("\\nç¤ºä¾‹ç”¨æ³•:")
                print("  python high_performance_etl_controller.py --all")
                print("  python high_performance_etl_controller.py --date 20250901")
                print("  python high_performance_etl_controller.py --all --batch-size 5000 --workers 8")
            
            controller.show_performance_stats()
            controller.print_error_report()

    except KeyboardInterrupt:
        print("\\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\\nâŒ æ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    

if __name__ == "__main__":
    main()
