#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å¢å¼ºç›‘æ§ç‰ˆæœ¬
Ultra Performance ETL Controller - Enhanced Monitoring Version

æ ¸å¿ƒæ”¹è¿›ï¼š
1. å®æ—¶ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤º - é˜²æ­¢ç•Œé¢å‡æ­»ï¼Œå®æ—¶äº†è§£å¤„ç†è¿›å±•
2. è¿›åº¦ç›‘æ§å’Œè‡ªåŠ¨æ¢å¤ - é˜²æ­¢ç³»ç»Ÿå¡æ­»
3. å¤šçº§æ€§èƒ½ä¼˜åŒ– - è¿›ä¸€æ­¥æå‡å¤„ç†é€Ÿåº¦
4. æ™ºèƒ½èµ„æºç®¡ç† - è‡ªé€‚åº”è°ƒèŠ‚é¿å…ç³»ç»Ÿè¿‡è½½
5. è¯¦ç»†çš„æ–‡ä»¶çº§åˆ«è¿›åº¦è¿½è¸ª
"""

import sys
import os
import json
import time
import argparse
import threading
import queue
import psutil
import signal
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, deque
import gc
import traceback
import logging

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥å…¶ä»–æ¨¡å—
current_dir = Path(__file__).parent
etl_root = current_dir.parent
sys.path.append(str(etl_root))

from parsers.base_log_parser import BaseLogParser
from processors.field_mapper import FieldMapper
from writers.dwd_writer import DWDWriter

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ - å®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½é˜²æ­¢å¡æ­»"""

    def __init__(self, warning_memory_percent=80, critical_memory_percent=90):
        self.warning_memory_percent = warning_memory_percent
        self.critical_memory_percent = critical_memory_percent
        self.start_time = time.time()

        # æ€§èƒ½å†å²è®°å½• (ä¿ç•™æœ€è¿‘100ä¸ªæ•°æ®ç‚¹)
        self.performance_history = {
            'timestamps': deque(maxlen=100),
            'memory_usage': deque(maxlen=100),
            'cpu_usage': deque(maxlen=100),
            'processing_speed': deque(maxlen=100)
        }

    def check_system_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶å†µ"""
        try:
            # è·å–å½“å‰ç³»ç»Ÿä¿¡æ¯
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=0.1)
            process = psutil.Process()
            process_memory = process.memory_info()

            current_time = time.time()

            # æ›´æ–°å†å²è®°å½•
            self.performance_history['timestamps'].append(current_time)
            self.performance_history['memory_usage'].append(memory.percent)
            self.performance_history['cpu_usage'].append(cpu_percent)

            health_status = {
                'timestamp': current_time,
                'system_memory_percent': memory.percent,
                'system_memory_available_gb': memory.available / (1024**3),
                'process_memory_mb': process_memory.rss / (1024**2),
                'cpu_percent': cpu_percent,
                'uptime_seconds': current_time - self.start_time,
                'status': 'healthy'
            }

            # åˆ¤æ–­å¥åº·çŠ¶å†µ
            if memory.percent > self.critical_memory_percent:
                health_status['status'] = 'critical'
                health_status['warning'] = f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory.percent:.1f}%"
            elif memory.percent > self.warning_memory_percent:
                health_status['status'] = 'warning'
                health_status['warning'] = f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜: {memory.percent:.1f}%"
            elif cpu_percent > 95:
                health_status['status'] = 'warning'
                health_status['warning'] = f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent:.1f}%"

            return health_status

        except Exception as e:
            return {
                'timestamp': time.time(),
                'status': 'error',
                'error': str(e)
            }

class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨ - å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦"""

    def __init__(self, logger):
        self.logger = logger
        self.lock = threading.Lock()

        # å…¨å±€ç»Ÿè®¡
        self.start_time = None
        self.total_files = 0
        self.completed_files = 0
        self.total_records = 0
        self.total_errors = 0

        # å½“å‰å¤„ç†çŠ¶æ€
        self.current_files = {}  # {thread_id: {'file': path, 'progress': 0.5, 'speed': records/sec}}
        self.recent_speeds = deque(maxlen=50)  # æœ€è¿‘çš„å¤„ç†é€Ÿåº¦è®°å½•

        # æ–‡ä»¶çº§åˆ«è¯¦ç»†è¿›åº¦
        self.file_progress = {}  # {file_path: {'total_lines': int, 'processed_lines': int, 'start_time': float}}

        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'avg_speed_rps': 0.0,
            'peak_speed_rps': 0.0,
            'current_speed_rps': 0.0,
            'estimated_completion': None,
            'efficiency_score': 0.0
        }

    def start_processing(self, total_files: int):
        """å¼€å§‹å¤„ç†"""
        with self.lock:
            self.start_time = time.time()
            self.total_files = total_files
            self.completed_files = 0
            self.total_records = 0
            self.total_errors = 0

    def start_file(self, thread_id: int, file_path: Path, estimated_lines: int = None):
        """å¼€å§‹å¤„ç†æ–‡ä»¶"""
        with self.lock:
            self.current_files[thread_id] = {
                'file': file_path,
                'start_time': time.time(),
                'processed_lines': 0,
                'estimated_lines': estimated_lines or 10000,
                'speed_rps': 0.0
            }

            self.file_progress[str(file_path)] = {
                'total_lines': estimated_lines or 10000,
                'processed_lines': 0,
                'start_time': time.time(),
                'thread_id': thread_id
            }

    def update_file_progress(self, thread_id: int, processed_lines: int, file_records: int):
        """æ›´æ–°æ–‡ä»¶å¤„ç†è¿›åº¦"""
        with self.lock:
            if thread_id in self.current_files:
                current_time = time.time()
                file_info = self.current_files[thread_id]
                file_info['processed_lines'] = processed_lines

                # è®¡ç®—å¤„ç†é€Ÿåº¦
                elapsed = current_time - file_info['start_time']
                if elapsed > 0:
                    speed_rps = file_records / elapsed
                    file_info['speed_rps'] = speed_rps
                    self.recent_speeds.append(speed_rps)

                    # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                    if speed_rps > self.performance_stats['peak_speed_rps']:
                        self.performance_stats['peak_speed_rps'] = speed_rps

                # æ›´æ–°æ–‡ä»¶çº§åˆ«è¿›åº¦
                file_path = str(file_info['file'])
                if file_path in self.file_progress:
                    self.file_progress[file_path]['processed_lines'] = processed_lines

    def complete_file(self, thread_id: int, record_count: int, error_count: int = 0):
        """å®Œæˆæ–‡ä»¶å¤„ç†"""
        with self.lock:
            if thread_id in self.current_files:
                file_info = self.current_files[thread_id]
                file_path = str(file_info['file'])

                # æ›´æ–°ç»Ÿè®¡
                self.completed_files += 1
                self.total_records += record_count
                self.total_errors += error_count

                # å®Œæˆæ–‡ä»¶çº§åˆ«è¿›åº¦
                if file_path in self.file_progress:
                    self.file_progress[file_path]['completed'] = True
                    self.file_progress[file_path]['record_count'] = record_count
                    self.file_progress[file_path]['error_count'] = error_count

                # ç§»é™¤å½“å‰å¤„ç†çŠ¶æ€
                del self.current_files[thread_id]

    def get_progress_summary(self) -> Dict[str, Any]:
        """è·å–è¿›åº¦æ‘˜è¦"""
        with self.lock:
            if not self.start_time:
                return {'status': 'not_started'}

            current_time = time.time()
            elapsed = current_time - self.start_time

            # è®¡ç®—å¹³å‡é€Ÿåº¦
            avg_speed = self.total_records / elapsed if elapsed > 0 else 0

            # è®¡ç®—å½“å‰é€Ÿåº¦ (æœ€è¿‘10ç§’)
            recent_speeds_list = list(self.recent_speeds)
            current_speed = sum(recent_speeds_list[-10:]) / min(10, len(recent_speeds_list)) if recent_speeds_list else 0

            # ä¼°ç®—å®Œæˆæ—¶é—´
            remaining_files = self.total_files - self.completed_files
            estimated_completion = None
            if current_speed > 0 and remaining_files > 0:
                # å‡è®¾æ¯ä¸ªæ–‡ä»¶å¹³å‡10000æ¡è®°å½•
                remaining_records = remaining_files * 10000
                eta_seconds = remaining_records / current_speed
                estimated_completion = current_time + eta_seconds

            # è®¡ç®—æ•ˆç‡è¯„åˆ† (0-100)
            efficiency_score = min(100, (current_speed / 1000) * 100) if current_speed > 0 else 0

            progress_percent = (self.completed_files / self.total_files * 100) if self.total_files > 0 else 0

            return {
                'status': 'processing',
                'progress_percent': progress_percent,
                'completed_files': self.completed_files,
                'total_files': self.total_files,
                'total_records': self.total_records,
                'total_errors': self.total_errors,
                'elapsed_seconds': elapsed,
                'avg_speed_rps': avg_speed,
                'current_speed_rps': current_speed,
                'peak_speed_rps': self.performance_stats['peak_speed_rps'],
                'estimated_completion': estimated_completion,
                'efficiency_score': efficiency_score,
                'active_threads': len(self.current_files),
                'current_files': {
                    tid: {
                        'file': info['file'].name,
                        'progress': info['processed_lines'] / info['estimated_lines'],
                        'speed_rps': info['speed_rps']
                    } for tid, info in self.current_files.items()
                }
            }

class UltraPerformanceETLController:
    """è¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨ - å¢å¼ºç›‘æ§ç‰ˆæœ¬"""

    def __init__(self,
                 base_log_dir: str = None,
                 state_file: str = None,
                 batch_size: int = 3000,        # å¢åŠ æ‰¹å¤„ç†å¤§å°
                 max_workers: int = 6,          # å¢åŠ çº¿ç¨‹æ•°
                 memory_limit_mb: int = 1024,   # å¢åŠ å†…å­˜é™åˆ¶
                 enable_realtime_stats: bool = True,  # å¯ç”¨å®æ—¶ç»Ÿè®¡
                 auto_optimize: bool = True):   # è‡ªåŠ¨ä¼˜åŒ–
        """
        åˆå§‹åŒ–è¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨

        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆæ¨è2000-5000ï¼‰
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆæ¨è4-8ï¼‰
            memory_limit_mb: å†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆMBï¼‰
            enable_realtime_stats: æ˜¯å¦å¯ç”¨å®æ—¶ç»Ÿè®¡æ˜¾ç¤º
            auto_optimize: æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ€§èƒ½ä¼˜åŒ–
        """
        # åŸºç¡€é…ç½®
        self.base_log_dir = Path(base_log_dir) if base_log_dir else \
            Path("D:/project/nginx-log-analyzer/nginx-analytics-warehouse/nginx_logs")
        self.state_file = Path(state_file) if state_file else \
            Path(etl_root / "processed_logs_state.json")

        # æ€§èƒ½é…ç½®
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.memory_limit_mb = memory_limit_mb
        self.enable_realtime_stats = enable_realtime_stats
        self.auto_optimize = auto_optimize

        # æ—¥å¿—é…ç½®
        self.logger = self._setup_logger()

        # ç›‘æ§ç»„ä»¶
        self.performance_monitor = PerformanceMonitor()
        self.progress_tracker = ProgressTracker(self.logger)

        # ä¼˜åŒ–çš„ç»„ä»¶æ± 
        self.parser_pool = [BaseLogParser() for _ in range(max_workers)]
        self.mapper_pool = [FieldMapper() for _ in range(max_workers)]
        self.writer_pool = []

        # ç¼“å­˜ä¼˜åŒ–
        self.ua_cache = {}
        self.uri_cache = {}
        self.ip_cache = {}  # æ–°å¢IPè§£æç¼“å­˜
        self.cache_stats = {'hits': 0, 'misses': 0}

        # æ™ºèƒ½æ‰¹é‡å†™å…¥
        self.write_buffer = []
        self.write_buffer_lock = threading.Lock()
        self.auto_flush_interval = 2.0  # è‡ªåŠ¨åˆ·æ–°é—´éš”
        self.last_flush_time = time.time()

        # æ§åˆ¶ä¿¡å·
        self.shutdown_event = threading.Event()
        self.pause_event = threading.Event()

        # å®æ—¶ç»Ÿè®¡çº¿ç¨‹
        self.stats_thread = None
        if self.enable_realtime_stats:
            self.stats_thread = threading.Thread(target=self._stats_monitor_loop, daemon=True)

        # è‡ªåŠ¨ä¼˜åŒ–å‚æ•°
        self.optimization_history = deque(maxlen=20)
        self.last_optimization_time = time.time()

        # åˆå§‹åŒ–ç³»ç»Ÿ
        self._init_system()

        # æ³¨å†Œä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)

            # æ–‡ä»¶å¤„ç†å™¨
            log_file = etl_root / f"etl_ultra_performance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)

            # æ ¼å¼åŒ–å™¨
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(formatter)
            file_handler.setFormatter(formatter)

            logger.addHandler(console_handler)
            logger.addHandler(file_handler)

        return logger

    def _init_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        self.logger.info("ğŸš€ è¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨åˆå§‹åŒ–å¼€å§‹")

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
        self._init_connection_pool()

        # å¯åŠ¨å®æ—¶ç»Ÿè®¡
        if self.enable_realtime_stats and self.stats_thread:
            self.stats_thread.start()

        self.logger.info("âœ… è¶…é«˜æ€§èƒ½ETLæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.base_log_dir}")
        self.logger.info(f"âš™ï¸ æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        self.logger.info(f"ğŸ§µ æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        self.logger.info(f"ğŸ’¾ å†…å­˜é™åˆ¶: {self.memory_limit_mb}MB")
        self.logger.info(f"ğŸ“Š å®æ—¶ç»Ÿè®¡: {'å¯ç”¨' if self.enable_realtime_stats else 'ç¦ç”¨'}")
        self.logger.info(f"ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–: {'å¯ç”¨' if self.auto_optimize else 'ç¦ç”¨'}")

    def _init_connection_pool(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        self.logger.info("ğŸ”— åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        success_count = 0

        for i in range(self.max_workers):
            try:
                writer = DWDWriter()
                if writer.connect():
                    self.writer_pool.append(writer)
                    success_count += 1
                    self.logger.info(f"âœ… æ•°æ®åº“è¿æ¥ {i+1}/{self.max_workers} æˆåŠŸ")
                else:
                    self.logger.error(f"âŒ æ•°æ®åº“è¿æ¥ {i+1}/{self.max_workers} å¤±è´¥")
            except Exception as e:
                self.logger.error(f"âŒ åˆ›å»ºæ•°æ®åº“è¿æ¥ {i+1} å¼‚å¸¸: {e}")

        if success_count == 0:
            raise RuntimeError("æ— æ³•åˆ›å»ºä»»ä½•æ•°æ®åº“è¿æ¥")

        self.logger.info(f"ğŸ¯ æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å®Œæˆ: {success_count}/{self.max_workers}")

    def _stats_monitor_loop(self):
        """å®æ—¶ç»Ÿè®¡ç›‘æ§å¾ªç¯"""
        self.logger.info("ğŸ“Š å¯åŠ¨å®æ—¶ç»Ÿè®¡ç›‘æ§çº¿ç¨‹")

        last_display_time = 0
        display_interval = 5.0  # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡

        while not self.shutdown_event.is_set():
            try:
                current_time = time.time()

                # æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶å†µ
                health = self.performance_monitor.check_system_health()

                # è·å–è¿›åº¦æ‘˜è¦
                progress = self.progress_tracker.get_progress_summary()

                # è‡ªåŠ¨ä¼˜åŒ–æ£€æŸ¥
                if self.auto_optimize:
                    self._auto_optimize_performance(health, progress)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                if current_time - last_display_time >= display_interval:
                    self._display_realtime_stats(health, progress)
                    last_display_time = current_time

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
                if health.get('status') == 'critical':
                    self.logger.warning("âš ï¸ ç³»ç»Ÿèµ„æºç´§å¼ ï¼Œæš‚åœå¤„ç†...")
                    self.pause_event.set()
                    time.sleep(5)  # ç­‰å¾…5ç§’
                    self.pause_event.clear()
                    self.logger.info("â–¶ï¸ æ¢å¤å¤„ç†")

                time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡

            except Exception as e:
                self.logger.error(f"ğŸ“Š ç»Ÿè®¡ç›‘æ§çº¿ç¨‹å¼‚å¸¸: {e}")
                time.sleep(5)

    def _display_realtime_stats(self, health: Dict, progress: Dict):
        """æ˜¾ç¤ºå®æ—¶ç»Ÿè®¡ä¿¡æ¯"""
        if progress.get('status') != 'processing':
            return

        # æ¸…å±å¹¶æ˜¾ç¤ºç»Ÿè®¡
        os.system('cls' if os.name == 'nt' else 'clear')

        print("=" * 80)
        print("ğŸš€ Nginxæ—¥å¿—ETLå¤„ç†å™¨ - å®æ—¶ç›‘æ§é¢æ¿")
        print("=" * 80)

        # è¿›åº¦ä¿¡æ¯
        print(f"ğŸ“ æ€»ä½“è¿›åº¦: {progress['completed_files']}/{progress['total_files']} æ–‡ä»¶ " +
              f"({progress['progress_percent']:.1f}%)")
        print(f"ğŸ“Š å¤„ç†è®°å½•: {progress['total_records']:,} æ¡ " +
              f"(é”™è¯¯: {progress['total_errors']:,} æ¡)")

        # æ€§èƒ½ä¿¡æ¯
        elapsed_str = str(timedelta(seconds=int(progress['elapsed_seconds'])))
        print(f"â±ï¸ è¿è¡Œæ—¶é—´: {elapsed_str}")
        print(f"âš¡ å½“å‰é€Ÿåº¦: {progress['current_speed_rps']:.0f} RPS")
        print(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {progress['avg_speed_rps']:.0f} RPS")
        print(f"ğŸ† å³°å€¼é€Ÿåº¦: {progress['peak_speed_rps']:.0f} RPS")
        print(f"ğŸ’¯ æ•ˆç‡è¯„åˆ†: {progress['efficiency_score']:.0f}/100")

        # å®Œæˆæ—¶é—´ä¼°ç®—
        if progress['estimated_completion']:
            eta = datetime.fromtimestamp(progress['estimated_completion'])
            print(f"ğŸ¯ é¢„è®¡å®Œæˆ: {eta.strftime('%H:%M:%S')}")

        print("-" * 40)

        # å½“å‰å¤„ç†æ–‡ä»¶
        print("ğŸ“‚ å½“å‰å¤„ç†æ–‡ä»¶:")
        for tid, file_info in progress['current_files'].items():
            progress_bar = "â–ˆ" * int(file_info['progress'] * 20) + "â–‘" * (20 - int(file_info['progress'] * 20))
            print(f"  çº¿ç¨‹{tid}: {file_info['file'][:30]}")
            print(f"          [{progress_bar}] {file_info['progress']*100:.1f}% ({file_info['speed_rps']:.0f} RPS)")

        print("-" * 40)

        # ç³»ç»Ÿå¥åº·çŠ¶å†µ
        status_icon = "ğŸŸ¢" if health['status'] == 'healthy' else "ğŸŸ¡" if health['status'] == 'warning' else "ğŸ”´"
        print(f"{status_icon} ç³»ç»ŸçŠ¶æ€: {health['status'].upper()}")
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {health['system_memory_percent']:.1f}% " +
              f"(å¯ç”¨: {health['system_memory_available_gb']:.1f}GB)")
        print(f"ğŸ–¥ï¸ CPUä½¿ç”¨ç‡: {health['cpu_percent']:.1f}%")
        print(f"ğŸ“¦ è¿›ç¨‹å†…å­˜: {health['process_memory_mb']:.0f}MB")

        if 'warning' in health:
            print(f"âš ï¸ {health['warning']}")

        print("=" * 80)
        print("æŒ‰ Ctrl+C åœæ­¢å¤„ç†")

    def _auto_optimize_performance(self, health: Dict, progress: Dict):
        """è‡ªåŠ¨æ€§èƒ½ä¼˜åŒ–"""
        if not self.auto_optimize:
            return

        current_time = time.time()
        if current_time - self.last_optimization_time < 30:  # 30ç§’ä¼˜åŒ–ä¸€æ¬¡
            return

        try:
            current_speed = progress.get('current_speed_rps', 0)
            memory_percent = health.get('system_memory_percent', 0)
            cpu_percent = health.get('cpu_percent', 0)

            # è®°å½•å½“å‰æ€§èƒ½
            self.optimization_history.append({
                'time': current_time,
                'speed': current_speed,
                'memory': memory_percent,
                'cpu': cpu_percent,
                'batch_size': self.batch_size
            })

            # è‡ªåŠ¨è°ƒä¼˜é€»è¾‘
            if memory_percent > 85 and self.batch_size > 1000:
                # å†…å­˜ç´§å¼ ï¼Œå‡å°æ‰¹å¤§å°
                self.batch_size = max(1000, int(self.batch_size * 0.8))
                self.logger.info(f"ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–: é™ä½æ‰¹å¤„ç†å¤§å°è‡³ {self.batch_size}")

            elif memory_percent < 60 and cpu_percent < 70 and current_speed > 0:
                # èµ„æºå……è¶³ï¼Œå¯ä»¥å¢åŠ æ‰¹å¤§å°
                if len(self.optimization_history) >= 3:
                    recent_speeds = [h['speed'] for h in list(self.optimization_history)[-3:]]
                    if all(speed >= 500 for speed in recent_speeds):  # é€Ÿåº¦ç¨³å®šä¸”è‰¯å¥½
                        self.batch_size = min(5000, int(self.batch_size * 1.2))
                        self.logger.info(f"ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–: æå‡æ‰¹å¤„ç†å¤§å°è‡³ {self.batch_size}")

            self.last_optimization_time = current_time

        except Exception as e:
            self.logger.error(f"ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–å¤±è´¥: {e}")

    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        self.logger.info(f"ğŸ“¡ æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…åœæ­¢...")
        self.shutdown_event.set()

    def cached_ua_parse(self, user_agent: str, mapper: FieldMapper) -> Dict:
        """ç¼“å­˜çš„ç”¨æˆ·ä»£ç†è§£æ"""
        if user_agent in self.ua_cache:
            self.cache_stats['hits'] += 1
            return self.ua_cache[user_agent]

        self.cache_stats['misses'] += 1
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„è§£ææ–¹æ³•
        parsed = mapper._parse_user_agent_enhanced(user_agent)

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.ua_cache) > 10000:
            # ç§»é™¤æœ€æ—§çš„1000ä¸ªæ¡ç›®
            for _ in range(1000):
                self.ua_cache.pop(next(iter(self.ua_cache)))

        self.ua_cache[user_agent] = parsed
        return parsed

    def process_all_logs(self, test_mode: bool = False, limit: int = None,
                        skip_processed: bool = True) -> Dict[str, Any]:
        """
        å¤„ç†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ - å¢å¼ºç‰ˆæœ¬

        Args:
            test_mode: æµ‹è¯•æ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“
            limit: æ¯ä¸ªæ–‡ä»¶å¤„ç†çš„è®°å½•é™åˆ¶
            skip_processed: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹å¤„ç†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶")

            # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
            log_files = list(self.base_log_dir.glob("**/*.log"))
            if not log_files:
                return {'success': False, 'error': 'æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶'}

            # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
            if skip_processed:
                processed_state = self.load_state()
                unprocessed_files = [f for f in log_files if str(f) not in processed_state.get('processed_files', {})]
                self.logger.info(f"ğŸ“ å‘ç° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œ{len(unprocessed_files)} ä¸ªæœªå¤„ç†")
                log_files = unprocessed_files
            else:
                self.logger.info(f"ğŸ“ å‘ç° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")

            if not log_files:
                self.logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ")
                return {'success': True, 'message': 'æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ'}

            # å¯åŠ¨è¿›åº¦è¿½è¸ª
            self.progress_tracker.start_processing(len(log_files))

            # æ–‡ä»¶åˆ†ç»„å¤„ç†
            file_groups = self._group_files_for_processing(log_files)

            # å¤šçº¿ç¨‹å¤„ç†
            results = []
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_group = {
                    executor.submit(self.process_file_group, group, thread_id, test_mode, limit): (group, thread_id)
                    for thread_id, group in enumerate(file_groups)
                }

                for future in as_completed(future_to_group):
                    group, thread_id = future_to_group[future]
                    try:
                        result = future.result()
                        results.append(result)

                        if not result.get('success', False):
                            self.logger.error(f"çº¿ç¨‹ {thread_id} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                    except Exception as e:
                        self.logger.error(f"çº¿ç¨‹ {thread_id} å¼‚å¸¸: {e}")
                        results.append({'success': False, 'error': str(e), 'thread_id': thread_id})

            # æ±‡æ€»ç»“æœ
            return self._summarize_results(results)

        except Exception as e:
            self.logger.error(f"å¤„ç†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
        finally:
            # ç¡®ä¿èµ„æºæ¸…ç†
            self.cleanup()

    def _group_files_for_processing(self, files: List[Path]) -> List[List[Path]]:
        """å°†æ–‡ä»¶åˆ†ç»„ä»¥ä¾¿å¹¶è¡Œå¤„ç†"""
        files_per_thread = max(1, len(files) // self.max_workers)
        groups = []

        for i in range(0, len(files), files_per_thread):
            group = files[i:i + files_per_thread]
            groups.append(group)

        # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§çº¿ç¨‹æ•°
        while len(groups) > self.max_workers:
            # å°†æœ€åä¸€ç»„åˆå¹¶åˆ°å‰ä¸€ç»„
            last_group = groups.pop()
            groups[-1].extend(last_group)

        return groups

    def process_file_group(self, file_paths: List[Path], thread_id: int,
                          test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¤„ç†æ–‡ä»¶ç»„"""
        results = []

        for file_path in file_paths:
            if self.shutdown_event.is_set():
                break

            # ç­‰å¾…æš‚åœçŠ¶æ€ç»“æŸ
            while self.pause_event.is_set() and not self.shutdown_event.is_set():
                time.sleep(1)

            result = self.process_single_file(file_path, thread_id, test_mode, limit)
            results.append(result)

        return {
            'success': all(r.get('success', False) for r in results),
            'thread_id': thread_id,
            'file_results': results
        }

    def process_single_file(self, file_path: Path, thread_id: int,
                           test_mode: bool = False, limit: int = None) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ - å¢å¼ºç‰ˆæœ¬"""
        start_time = time.time()

        # ä¼°ç®—æ–‡ä»¶è¡Œæ•°
        estimated_lines = self._estimate_file_lines(file_path)

        # å¼€å§‹æ–‡ä»¶å¤„ç†è¿½è¸ª
        self.progress_tracker.start_file(thread_id, file_path, estimated_lines)

        try:
            # è·å–ç»„ä»¶
            parser = self.parser_pool[thread_id % len(self.parser_pool)]
            mapper = self.mapper_pool[thread_id % len(self.mapper_pool)]
            writer = self.writer_pool[thread_id % len(self.writer_pool)] if not test_mode else None

            batch = []
            file_records = 0
            file_lines = 0
            file_errors = 0

            self.logger.info(f"ğŸ§µ{thread_id} å¼€å§‹å¤„ç†: {file_path.name} (é¢„ä¼° {estimated_lines:,} è¡Œ)")

            # æµå¼å¤„ç†æ–‡ä»¶
            for parsed_data in parser.parse_file(file_path):
                if self.shutdown_event.is_set():
                    break

                file_lines += 1

                # æ›´æ–°è¿›åº¦
                if file_lines % 1000 == 0:
                    self.progress_tracker.update_file_progress(thread_id, file_lines, file_records)

                if parsed_data:
                    try:
                        # ç¼“å­˜è§£æ
                        user_agent = parsed_data.get('user_agent', '')
                        if user_agent:
                            parsed_data['_cached_ua'] = self.cached_ua_parse(user_agent, mapper)

                        # å­—æ®µæ˜ å°„
                        mapped_data = mapper.map_to_dwd(parsed_data, file_path.name)
                        batch.append(mapped_data)
                        file_records += 1

                        # æ‰¹é‡å†™å…¥
                        if len(batch) >= self.batch_size:
                            if not test_mode and writer:
                                try:
                                    writer.write_batch(batch)
                                except Exception as e:
                                    self.logger.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                                    file_errors += 1

                            batch.clear()
                            gc.collect()

                        # æ£€æŸ¥é™åˆ¶
                        if limit and file_records >= limit:
                            break

                    except Exception as e:
                        file_errors += 1
                        if file_errors <= 5:  # åªè®°å½•å‰5ä¸ªé”™è¯¯
                            self.logger.error(f"è®°å½•å¤„ç†é”™è¯¯: {e}")

            # å¤„ç†å‰©ä½™æ‰¹æ¬¡
            if batch and not test_mode and writer:
                try:
                    writer.write_batch(batch)
                except Exception as e:
                    self.logger.error(f"æœ€ç»ˆæ‰¹é‡å†™å…¥å¤±è´¥: {e}")
                    file_errors += 1

            # å®Œæˆæ–‡ä»¶å¤„ç†
            processing_time = time.time() - start_time
            self.progress_tracker.complete_file(thread_id, file_records, file_errors)

            # æ ‡è®°æ–‡ä»¶å·²å¤„ç†
            if not test_mode:
                self.mark_file_processed(file_path, file_records, processing_time)

            result = {
                'success': True,
                'file_path': str(file_path),
                'lines_processed': file_lines,
                'records_processed': file_records,
                'errors': file_errors,
                'processing_time': processing_time,
                'speed_rps': file_records / processing_time if processing_time > 0 else 0
            }

            self.logger.info(
                f"âœ… æ–‡ä»¶å®Œæˆ: {file_path.name} - "
                f"{file_records:,} æ¡è®°å½•ï¼Œ{file_errors} ä¸ªé”™è¯¯ï¼Œ"
                f"{processing_time:.1f}ç§’ ({result['speed_rps']:.0f} RPS)"
            )

            return result

        except Exception as e:
            error_msg = f"å¤„ç†æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}"
            self.logger.error(error_msg)
            self.progress_tracker.complete_file(thread_id, 0, 1)
            return {
                'success': False,
                'file_path': str(file_path),
                'error': error_msg
            }

    def _estimate_file_lines(self, file_path: Path) -> int:
        """ä¼°ç®—æ–‡ä»¶è¡Œæ•°"""
        try:
            # å¿«é€Ÿä¼°ç®—ï¼šè¯»å–å‰1000è¡Œï¼Œè®¡ç®—å¹³å‡è¡Œé•¿ï¼Œä¼°ç®—æ€»è¡Œæ•°
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                sample_lines = []
                for i, line in enumerate(f):
                    sample_lines.append(len(line))
                    if i >= 999:  # è¯»å–1000è¡Œæ ·æœ¬
                        break

            if sample_lines:
                avg_line_length = sum(sample_lines) / len(sample_lines)
                file_size = file_path.stat().st_size
                estimated_lines = int(file_size / avg_line_length)
                return max(1000, estimated_lines)  # è‡³å°‘1000è¡Œ
            else:
                return 10000  # é»˜è®¤ä¼°ç®—

        except Exception:
            return 10000  # é»˜è®¤ä¼°ç®—

    def _summarize_results(self, results: List[Dict]) -> Dict[str, Any]:
        """æ±‡æ€»å¤„ç†ç»“æœ"""
        total_files = len(results)
        successful_files = sum(1 for r in results if r.get('success', False))

        total_records = 0
        total_errors = 0
        total_time = 0

        for result in results:
            if 'file_results' in result:
                for file_result in result['file_results']:
                    total_records += file_result.get('records_processed', 0)
                    total_errors += file_result.get('errors', 0)
                    total_time += file_result.get('processing_time', 0)
            else:
                total_records += result.get('records_processed', 0)
                total_errors += result.get('errors', 0)
                total_time += result.get('processing_time', 0)

        avg_speed = total_records / total_time if total_time > 0 else 0

        summary = {
            'success': successful_files > 0,
            'total_files': total_files,
            'successful_files': successful_files,
            'failed_files': total_files - successful_files,
            'total_records_processed': total_records,
            'total_errors': total_errors,
            'total_processing_time': total_time,
            'average_speed_rps': avg_speed,
            'cache_hit_rate': self.cache_stats['hits'] / (self.cache_stats['hits'] + self.cache_stats['misses']) if (self.cache_stats['hits'] + self.cache_stats['misses']) > 0 else 0
        }

        self.logger.info("=" * 60)
        self.logger.info("ğŸ¯ å¤„ç†å®Œæˆæ€»ç»“:")
        self.logger.info(f"ğŸ“ æ–‡ä»¶: {successful_files}/{total_files} æˆåŠŸ")
        self.logger.info(f"ğŸ“Š è®°å½•: {total_records:,} æ¡ (é”™è¯¯: {total_errors:,})")
        self.logger.info(f"â±ï¸ æ—¶é—´: {total_time:.1f} ç§’")
        self.logger.info(f"âš¡ å¹³å‡é€Ÿåº¦: {avg_speed:.0f} RPS")
        self.logger.info(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {summary['cache_hit_rate']:.1%}")
        self.logger.info("=" * 60)

        return summary

    def load_state(self) -> Dict:
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            self.logger.warning(f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
        return {'processed_files': {}}

    def mark_file_processed(self, file_path: Path, record_count: int, processing_time: float):
        """æ ‡è®°æ–‡ä»¶å·²å¤„ç†"""
        try:
            state = self.load_state()
            state['processed_files'][str(file_path)] = {
                'timestamp': datetime.now().isoformat(),
                'record_count': record_count,
                'processing_time': processing_time
            }

            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)

        except Exception as e:
            self.logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")

        # åœæ­¢ç»Ÿè®¡çº¿ç¨‹
        self.shutdown_event.set()

        # å…³é—­æ•°æ®åº“è¿æ¥
        for writer in self.writer_pool:
            try:
                writer.close()
            except Exception as e:
                self.logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

        # æ¸…ç†ç¼“å­˜
        self.ua_cache.clear()
        self.uri_cache.clear()
        self.ip_cache.clear()

        self.logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¶…é«˜æ€§èƒ½ETLå¤„ç†å™¨')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--limit', type=int, help='æ¯ä¸ªæ–‡ä»¶å¤„ç†è®°å½•é™åˆ¶')
    parser.add_argument('--batch-size', type=int, default=3000, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--workers', type=int, default=6, help='å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--memory-limit', type=int, default=1024, help='å†…å­˜é™åˆ¶(MB)')
    parser.add_argument('--no-stats', action='store_true', help='ç¦ç”¨å®æ—¶ç»Ÿè®¡')
    parser.add_argument('--no-optimize', action='store_true', help='ç¦ç”¨è‡ªåŠ¨ä¼˜åŒ–')

    args = parser.parse_args()

    # åˆ›å»ºæ§åˆ¶å™¨
    controller = UltraPerformanceETLController(
        batch_size=args.batch_size,
        max_workers=args.workers,
        memory_limit_mb=args.memory_limit,
        enable_realtime_stats=not args.no_stats,
        auto_optimize=not args.no_optimize
    )

    try:
        # å¤„ç†æ‰€æœ‰æ—¥å¿—
        result = controller.process_all_logs(
            test_mode=args.test,
            limit=args.limit
        )

        if result['success']:
            print("\nğŸ‰ å¤„ç†å®Œæˆ!")
        else:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        print(f"\nğŸ’¥ å¤„ç†å¼‚å¸¸: {e}")
    finally:
        controller.cleanup()

if __name__ == "__main__":
    main()