services:
  # ========== 核心数据存储(优先启动) ==========

  # ClickHouse 数据库 - 修复网络连接问题
  clickhouse:
    image: clickhouse/clickhouse-server:24.3-alpine
    container_name: nginx-analytics-clickhouse
    restart: unless-stopped
    ports:
      - "8123:8123"
      - "9000:9000"
    environment:
      CLICKHOUSE_DB: nginx_analytics
      CLICKHOUSE_USER: analytics_user
      CLICKHOUSE_PASSWORD: analytics_password_change_in_prod
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
    networks:
      - nginx-analytics
    # 使用简单的启动命令覆盖默认配置
    command: >
      sh -c "
        echo '<clickhouse><listen_host>0.0.0.0</listen_host><listen_host>::</listen_host></clickhouse>' > /etc/clickhouse-server/config.d/listen.xml &&
        /entrypoint.sh
      "
    healthcheck:
      test: ["CMD-SHELL", "nc -z 127.0.0.1 8123 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Redis 缓存 - 共享缓存服务
  redis:
    image: redis:7-alpine
    container_name: nginx-analytics-redis
    restart: unless-stopped
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    networks:
      - nginx-analytics
    command: redis-server --appendonly yes --requirepass redis_password
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "redis_password", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ========== BI数据可视化工具 ==========

  # Grafana 数据可视化
  grafana:
    image: grafana/grafana:latest
    container_name: nginx-analytics-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin123
      GF_INSTALL_PLUGINS: grafana-clickhouse-datasource
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./services/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./services/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - nginx-analytics
    depends_on:
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # PostgreSQL for Superset - 修复权限问题
  postgres:
    image: postgres:13
    container_name: nginx-analytics-postgres
    restart: unless-stopped
    ports:
      - "5433:5432"
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset_password
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - nginx-analytics
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset -d superset"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Apache Superset - 高级数据分析(修复配置)
  superset:
    image: apache/superset:latest
    container_name: nginx-analytics-superset
    restart: unless-stopped
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=nginx_analytics_secret_key_change_in_production
      - SUPERSET_CONFIG_PATH=/app/pythonpath/superset_config.py
      - DATABASE_HOST=postgres
      - DATABASE_DB=superset
      - DATABASE_USER=superset
      - DATABASE_PASSWORD=superset_password
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=redis_password
    volumes:
      - superset_data:/app/superset_home
      - ./services/superset/config:/app/pythonpath
    networks:
      - nginx-analytics
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8088/health || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 120s

  # ========== DataEase BI工具 v2.10.12 (安全版本) ==========

  # DataEase MySQL数据库 v2.10.12
  dataease-mysql:
    image: registry.cn-qingdao.aliyuncs.com/dataease/mysql:8.4.5
    container_name: nginx-analytics-dataease-mysql
    restart: unless-stopped
    ports:
      - "3307:3306"
    environment:
      MYSQL_ROOT_PASSWORD: Password123@mysql
      MYSQL_DATABASE: dataease
      TZ: Asia/Shanghai
    env_file:
      - ./services/dataease/config/mysql.env
    volumes:
      - dataease_mysql_data:/var/lib/mysql
      - ./services/dataease/config/my.cnf:/etc/mysql/conf.d/my.cnf
    networks:
      - nginx-analytics
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-uroot", "-pPassword123@mysql", "--protocol", "tcp"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 30s

  # DataEase主服务 v2.10.12
  dataease:
    #image: registry.cn-qingdao.aliyuncs.com/dataease/dataease:v2.10.12
    image: registry.cn-qingdao.aliyuncs.com/dataease/dataease:v2.10.13
    container_name: nginx-analytics-dataease
    restart: unless-stopped
    ports:
      - "8810:8100"
    environment:
      - TZ=Asia/Shanghai
    volumes:
      - ./services/dataease/config:/opt/apps/config
      - dataease_logs:/opt/dataease2.0/logs
      - dataease_data:/opt/dataease2.0/data
      - dataease_cache:/opt/dataease2.0/cache
    networks:
      - nginx-analytics
    depends_on:
      dataease-mysql:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8100/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # ========== 夜莺监控栈 ==========

  # MySQL for 夜莺元数据存储
  n9e-mysql:
    image: mysql:8
    container_name: n9e-mysql
    hostname: n9e-mysql
    restart: unless-stopped
    ports:
      - "3308:3306"
    environment:
      TZ: Asia/Shanghai
      MYSQL_ROOT_PASSWORD: 1234
      MYSQL_DATABASE: n9e_v6
    volumes:
      - ./data/n9e-mysql:/var/lib/mysql
      - ./services/n9e/init-scripts:/docker-entrypoint-initdb.d
    networks:
      - nginx-analytics
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-uroot", "-p1234"]
      interval: 30s
      timeout: 10s
      retries: 3

  # VictoriaMetrics 时序数据库
  victoriametrics:
    image: victoriametrics/victoria-metrics:v1.79.12
    container_name: nginx-analytics-victoriametrics
    hostname: victoriametrics
    restart: unless-stopped
    ports:
      - "8428:8428"
    environment:
      TZ: Asia/Shanghai
    volumes:
      - ./data/victoriametrics:/victoria-metrics-data
    networks:
      - nginx-analytics
    command:
      - "--storageDataPath=/victoria-metrics-data"
      - "--loggerTimezone=Asia/Shanghai"
      - "--retentionPeriod=1y"
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8428/health || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 30s

  # 夜莺核心服务 - 修复启动配置
  nightingale:
    image: flashcatcloud/nightingale:latest
    container_name: nginx-analytics-nightingale
    hostname: nightingale
    restart: unless-stopped
    ports:
      - "17000:17000"
      - "20090:20090"
    environment:
      GIN_MODE: release
      TZ: Asia/Shanghai
      WAIT_HOSTS: n9e-mysql:3306, nginx-analytics-redis:6379
      N9E_CONFIGS: /app/etc
    volumes:
      - ./services/n9e/config/nightingale:/app/etc
      - ./data/n9e:/app/data
    networks:
      - nginx-analytics
    depends_on:
      n9e-mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
      victoriametrics:
        condition: service_healthy
    # 修复启动命令 - 使用环境变量指定配置目录
    command: >
      sh -c "/app/n9e"
    healthcheck:
      test: ["CMD", "pgrep", "n9e"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Categraf 数据采集器
  categraf:
    image: flashcatcloud/categraf:latest
    container_name: nginx-analytics-categraf
    hostname: categraf
    restart: unless-stopped
    environment:
      TZ: Asia/Shanghai
      HOST_PROC: /hostfs/proc
      HOST_SYS: /hostfs/sys
      HOST_MOUNT_PREFIX: /hostfs
    volumes:
      - ./services/n9e/config/categraf:/etc/categraf/conf
      - /:/hostfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc:/hostfs/proc:ro
      - /sys:/hostfs/sys:ro
    networks:
      - nginx-analytics
    depends_on:
      nightingale:
        condition: service_healthy
    privileged: true
    # 解决容器无法访问宿主机问题
    pid: host

  # ========== ETL 数据处理引擎 ==========

  # Spark Master - ETL计算引擎
  spark-master:
    image: apache/spark:3.5.1
    container_name: nginx-analytics-spark-master
    hostname: spark-master
    restart: unless-stopped
    ports:
      - "8080:8080"  # Spark Master UI
      - "7077:7077"  # Spark Master Port
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - TZ=Asia/Shanghai
    volumes:
      - spark_data:/opt/spark-data
      - ./services/spark/conf:/opt/spark/conf
      - ./services/seatunnel/jobs:/opt/seatunnel/jobs
    networks:
      - nginx-analytics
    command: >
      sh -c "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Spark Worker - ETL计算节点
  spark-worker:
    image: apache/spark:3.5.1
    container_name: nginx-analytics-spark-worker
    hostname: spark-worker
    restart: unless-stopped
    ports:
      - "8081:8081"  # Spark Worker UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - TZ=Asia/Shanghai
    volumes:
      - spark_data:/opt/spark-data
      - ./services/spark/conf:/opt/spark/conf
      - ./services/seatunnel/jobs:/opt/seatunnel/jobs
    networks:
      - nginx-analytics
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      sh -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Flink JobManager - 流处理引擎管理节点
  flink-jobmanager:
    image: flink:1.18-java11
    container_name: nginx-analytics-flink-jobmanager
    hostname: flink-jobmanager
    restart: unless-stopped
    ports:
      - "8082:8081"  # Flink JobManager UI
    environment:
      - FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager
      - TZ=Asia/Shanghai
    volumes:
      - flink_data:/opt/flink-data
      - ./services/flink/conf:/opt/flink/conf
      - ./services/seatunnel/jobs:/opt/seatunnel/jobs
    networks:
      - nginx-analytics
    command: jobmanager
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Flink TaskManager - 流处理引擎工作节点
  flink-taskmanager:
    image: flink:1.18-java11
    container_name: nginx-analytics-flink-taskmanager
    hostname: flink-taskmanager
    restart: unless-stopped
    environment:
      - FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager taskmanager.numberOfTaskSlots:2
      - TZ=Asia/Shanghai
    volumes:
      - flink_data:/opt/flink-data
      - ./services/flink/conf:/opt/flink/conf
      - ./services/seatunnel/jobs:/opt/seatunnel/jobs
    networks:
      - nginx-analytics
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    command: taskmanager

  # SeaTunnel Master - 数据集成平台主节点
  seatunnel-master:
    image: apache/seatunnel:latest
    container_name: nginx-analytics-seatunnel-master
    hostname: seatunnel-master
    restart: unless-stopped
    ports:
      - "5801:5801"  # SeaTunnel Engine REST API
    environment:
      # 官方推荐的环境变量
      - SEATUNNEL_HOME=/opt/seatunnel
      - TZ=Asia/Shanghai
      # 集群配置 - 按照官方文档
      - ST_DOCKER_MEMBER_LIST=seatunnel-master:5801
      # 计算引擎配置
      - SPARK_HOME=/opt/spark
      - SPARK_MASTER_URL=spark://spark-master:7077
      - FLINK_HOME=/opt/flink
      # 数据源连接配置
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_DATABASE=nginx_analytics
      - CLICKHOUSE_USER=analytics_user
      - CLICKHOUSE_PASSWORD=analytics_password_change_in_prod
    volumes:
      # 挂载ETL作业配置文件
      - ./services/seatunnel/jobs:/opt/seatunnel/jobs:ro
      # 挂载数据源
      - ../nginx_logs-a:/opt/data/nginx_logs:ro
      # 挂载SeaTunnel数据目录
      - seatunnel_data:/opt/seatunnel/data
      - seatunnel_logs:/opt/seatunnel/logs
      - ./services/seatunnel/config:/opt/seatunnel/config
    networks:
      - nginx-analytics
    depends_on:
      clickhouse:
        condition: service_healthy
      spark-master:
        condition: service_healthy
    command:
      - sh
      - -c
      - |
        echo "启动SeaTunnel Master节点..."
        echo "环境变量: ST_DOCKER_MEMBER_LIST=$ST_DOCKER_MEMBER_LIST"

        # 确保目录存在
        mkdir -p /opt/seatunnel/logs /opt/seatunnel/data

        # 等待依赖服务就绪
        sleep 10

        # 启动master节点 - 按照官方文档
        cd /opt/seatunnel
        exec ./bin/seatunnel-cluster.sh -r master
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5801/hazelcast/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ========== DolphinScheduler 3.3.1 Standalone模式 ==========

  # DolphinScheduler Standalone - All-in-one 模式
  dolphinscheduler-standalone:
    image: apache/dolphinscheduler-standalone-server:3.3.1
    container_name: nginx-analytics-dolphinscheduler
    hostname: dolphinscheduler-standalone
    restart: unless-stopped
    ports:
      - "12345:12345"  # DolphinScheduler Web UI
    environment:
      DATABASE_TYPE: h2
      SPRING_PROFILES_ACTIVE: h2
      TZ: Asia/Shanghai
      # 资源存储配置
      RESOURCE_STORAGE_TYPE: LOCAL
      RESOURCE_UPLOAD_PATH: /opt/dolphinscheduler/resources
      # SeaTunnel 集成配置
      SEATUNNEL_HOME: /opt/seatunnel
      # 数据源连接配置
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_DATABASE: nginx_analytics
      CLICKHOUSE_USER: analytics_user
      CLICKHOUSE_PASSWORD: analytics_password_change_in_prod
    volumes:
      - ds_standalone_data:/opt/dolphinscheduler
      - ds_logs:/opt/dolphinscheduler/logs
      # 挂载SeaTunnel相关目录用于ETL作业
      - seatunnel_data:/opt/seatunnel/data:ro
      - ./services/seatunnel/jobs:/opt/seatunnel/jobs:ro
      # 挂载数据源文件
      - ../nginx_logs-a:/opt/data/nginx_logs:ro
    networks:
      - nginx-analytics
    depends_on:
      seatunnel-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12345/dolphinscheduler/"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 300s

  # ========== 可选: 系统监控 ==========

  # Prometheus Node Exporter - 系统指标收集
  node-exporter:
    image: prom/node-exporter:latest
    container_name: nginx-analytics-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - nginx-analytics

networks:
  nginx-analytics:
    driver: bridge
    name: nginx-analytics

volumes:
  # 核心数据存储卷
  clickhouse_data:
    driver: local
  clickhouse_logs:
    driver: local
  redis_data:
    driver: local

  # BI工具数据卷
  grafana_data:
    driver: local
  postgres_data:
    driver: local
  superset_data:
    driver: local

  # DataEase v2.10.12数据卷
  dataease_mysql_data:
    driver: local
  dataease_data:
    driver: local
  dataease_logs:
    driver: local
  dataease_cache:
    driver: local

  # 夜莺监控数据卷
  n9e_mysql_data:
    driver: local
  victoriametrics_data:
    driver: local
  n9e_data:
    driver: local

  # ETL数据处理引擎数据卷
  spark_data:
    driver: local
  flink_data:
    driver: local
  seatunnel_data:
    driver: local
  seatunnel_logs:
    driver: local

  # DolphinScheduler Standalone数据卷
  ds_standalone_data:
    driver: local
  ds_logs:
    driver: local