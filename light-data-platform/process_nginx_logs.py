# -*- coding: utf-8 -*-
"""
Nginxæ—¥å¿—å¤„ç†ç»Ÿä¸€å…¥å£
æ”¯æŒå…¨é‡å’Œå¢é‡å¤„ç†ï¼ŒçŠ¶æ€ç®¡ç†å’Œé”™è¯¯æ¢å¤
"""

import os
import sys
import json
import argparse
from datetime import datetime, date
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent))

from scripts.clickhouse_pipeline import ClickHousePipeline
from scripts.incremental_manager import IncrementalManager
from self.self_00_02_utils import log_info

class NginxLogManager:
    """Nginxæ—¥å¿—å¤„ç†ç®¡ç†å™¨"""
    
    def __init__(self):
        self.pipeline = ClickHousePipeline()
        self.increment_manager = IncrementalManager()
    
    def process_logs(self, log_dir: str, target_date: date = None, mode: str = 'incremental') -> dict:
        """å¤„ç†nginxæ—¥å¿—"""
        log_info("="*60)
        log_info(f"ğŸš€ å¼€å§‹Nginxæ—¥å¿—å¤„ç†")
        log_info(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
        log_info(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date or 'å…¨éƒ¨'}")
        log_info(f"ğŸ”„ å¤„ç†æ¨¡å¼: {mode}")
        log_info("="*60)
        
        try:
            # æ£€æŸ¥æ—¥å¿—ç›®å½•
            if not os.path.exists(log_dir):
                raise Exception(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
            
            # æ‰§è¡Œæ•°æ®ç®¡é“
            result = self.pipeline.process_nginx_logs_to_clickhouse(log_dir, target_date, mode)
            
            # è¾“å‡ºç»“æœæ‘˜è¦
            self._print_processing_summary(result)
            
            return result
            
        except Exception as e:
            log_info(f"âŒ å¤„ç†å¤±è´¥: {e}", level="ERROR")
            return {'status': 'error', 'message': str(e)}
    
    def show_status(self, target_date: date = None) -> dict:
        """æ˜¾ç¤ºå¤„ç†çŠ¶æ€"""
        log_info("ğŸ“Š è·å–å¤„ç†çŠ¶æ€...")
        
        try:
            if target_date:
                # æ˜¾ç¤ºæŒ‡å®šæ—¥æœŸçŠ¶æ€
                status = self.increment_manager.get_date_status(target_date)
                self._print_date_status(status)
            else:
                # æ˜¾ç¤ºæ€»ä½“çŠ¶æ€
                summary = self.increment_manager.get_processing_summary()
                pipeline_status = self.pipeline.get_pipeline_status()
                
                self._print_overall_status(summary, pipeline_status)
            
            return {'status': 'success'}
            
        except Exception as e:
            log_info(f"âŒ è·å–çŠ¶æ€å¤±è´¥: {e}", level="ERROR")
            return {'status': 'error', 'message': str(e)}
    
    def reset_failed(self, target_date: date = None) -> dict:
        """é‡ç½®å¤±è´¥çš„æ–‡ä»¶"""
        log_info("ğŸ”„ é‡ç½®å¤±è´¥æ–‡ä»¶...")
        
        try:
            self.increment_manager.reset_failed_files(target_date)
            log_info("âœ… å¤±è´¥æ–‡ä»¶é‡ç½®å®Œæˆ")
            return {'status': 'success'}
            
        except Exception as e:
            log_info(f"âŒ é‡ç½®å¤±è´¥: {e}", level="ERROR")
            return {'status': 'error', 'message': str(e)}
    
    def _print_processing_summary(self, result: dict):
        """æ‰“å°å¤„ç†ç»“æœæ‘˜è¦"""
        log_info("="*60)
        log_info("ğŸ“‹ å¤„ç†ç»“æœæ‘˜è¦")
        log_info("="*60)
        
        status = result.get('status', 'unknown')
        if status == 'completed':
            log_info(f"âœ… å¤„ç†çŠ¶æ€: å®Œæˆ")
            log_info(f"ğŸ“ˆ å¤„ç†è®°å½•æ•°: {result.get('processed', 0):,}")
            log_info(f"ğŸ“ æˆåŠŸæ–‡ä»¶: {result.get('success_files', 0)}")
            log_info(f"âŒ å¤±è´¥æ–‡ä»¶: {result.get('failed_files', 0)}")
            log_info(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {result.get('total_files', 0)}")
            
            success_rate = 0
            if result.get('total_files', 0) > 0:
                success_rate = result.get('success_files', 0) / result.get('total_files', 0) * 100
            log_info(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.1f}%")
            
        elif status == 'no_files':
            log_info("âš ï¸  æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„æ—¥å¿—æ–‡ä»¶")
        elif status == 'up_to_date':
            log_info("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€å¤„ç†")
        elif status == 'error':
            log_info(f"âŒ å¤„ç†é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        log_info("="*60)
    
    def _print_date_status(self, status: dict):
        """æ‰“å°æŒ‡å®šæ—¥æœŸçŠ¶æ€"""
        log_info("="*50)
        log_info(f"ğŸ“… æ—¥æœŸ {status['date']} å¤„ç†çŠ¶æ€")
        log_info("="*50)
        log_info(f"ğŸ“Š çŠ¶æ€: {status['status']}")
        log_info(f"ğŸ“ æ–‡ä»¶æ€»æ•°: {status['files_count']}")
        log_info(f"âœ… å·²å®Œæˆ: {status['completed_count']}")
        log_info(f"âŒ å¤±è´¥: {status['failed_count']}")
        log_info(f"ğŸ”„ å¤„ç†ä¸­: {status['processing_count']}")
        log_info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {status['total_records']:,}")
        
        # æ˜¾ç¤ºæ–‡ä»¶è¯¦æƒ…
        if status.get('files'):
            log_info("\nğŸ“‹ æ–‡ä»¶è¯¦æƒ…:")
            for file_info in status['files']:
                status_icon = "âœ…" if file_info.get('status') == 'completed' else ("âŒ" if file_info.get('status') == 'failed' else "ğŸ”„")
                log_info(f"  {status_icon} {file_info.get('file', 'unknown')} - {file_info.get('records_count', 0)} æ¡è®°å½•")
        
        log_info("="*50)
    
    def _print_overall_status(self, summary: dict, pipeline_status: dict):
        """æ‰“å°æ€»ä½“çŠ¶æ€"""
        log_info("="*60)
        log_info("ğŸŒ ç³»ç»Ÿæ€»ä½“çŠ¶æ€")
        log_info("="*60)
        
        # å¤„ç†çŠ¶æ€æ‘˜è¦
        log_info("ğŸ“Š å¤„ç†çŠ¶æ€æ‘˜è¦:")
        log_info(f"  ğŸ“… æ€»å¤„ç†å¤©æ•°: {summary.get('total_dates', 0)}")
        log_info(f"  âœ… å®Œæˆå¤©æ•°: {summary.get('completed_dates', 0)}")
        log_info(f"  ğŸ¯ å®Œæˆç‡: {summary.get('completion_rate', 0):.1f}%")
        log_info(f"  ğŸ“ æ€»æ–‡ä»¶æ•°: {summary.get('total_files', 0)}")
        log_info(f"  âœ… æˆåŠŸæ–‡ä»¶: {summary.get('completed_files', 0)}")
        log_info(f"  âŒ å¤±è´¥æ–‡ä»¶: {summary.get('failed_files', 0)}")
        log_info(f"  ğŸ¯ æ–‡ä»¶æˆåŠŸç‡: {summary.get('file_success_rate', 0):.1f}%")
        log_info(f"  ğŸ“ˆ æ€»è®°å½•æ•°: {summary.get('total_records', 0):,}")
        
        # ClickHouseçŠ¶æ€
        if 'clickhouse_stats' in pipeline_status:
            stats = pipeline_status['clickhouse_stats']
            log_info("\nğŸª ClickHouseæ•°æ®åº“çŠ¶æ€:")
            log_info(f"  ğŸ“ˆ è®°å½•æ€»æ•°: {stats.get('total_records', 0):,}")
            log_info(f"  âœ… æˆåŠŸç‡: {stats.get('success_rate', 0):.1f}%")
            log_info(f"  ğŸŒ æ…¢è¯·æ±‚ç‡: {stats.get('slow_rate', 0):.1f}%")
            log_info(f"  âš ï¸  å¼‚å¸¸ç‡: {stats.get('anomaly_rate', 0):.1f}%")
            log_info(f"  ğŸ“Š å¹³å‡è´¨é‡åˆ†: {stats.get('avg_quality_score', 0):.2f}")
            
            # å¹³å°åˆ†å¸ƒ
            if stats.get('platform_distribution'):
                log_info("\nğŸ“± å¹³å°åˆ†å¸ƒ:")
                for platform, count in stats['platform_distribution'].items():
                    log_info(f"  {platform}: {count:,} æ¡è®°å½•")
        
        # å¥åº·çŠ¶æ€
        health = pipeline_status.get('pipeline_health', 'unknown')
        health_icon = "ğŸŸ¢" if health == 'healthy' else ("ğŸŸ¡" if health == 'no_data' else "ğŸ”´")
        log_info(f"\nğŸ¥ ç®¡é“å¥åº·çŠ¶æ€: {health_icon} {health}")
        
        log_info("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Nginxæ—¥å¿—å¤„ç†ç®¡ç†å™¨ v1.0.0',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¢é‡å¤„ç†æ‰€æœ‰æ—¥å¿—
  python process_nginx_logs.py --log-dir /path/to/nginx-logs
  
  # å…¨é‡å¤„ç†æŒ‡å®šæ—¥æœŸ
  python process_nginx_logs.py --log-dir /path/to/nginx-logs --date 2025-08-29 --mode full
  
  # æŸ¥çœ‹å¤„ç†çŠ¶æ€
  python process_nginx_logs.py --status
  
  # æŸ¥çœ‹æŒ‡å®šæ—¥æœŸçŠ¶æ€
  python process_nginx_logs.py --status --date 2025-08-29
  
  # é‡ç½®å¤±è´¥æ–‡ä»¶
  python process_nginx_logs.py --reset-failed
        """
    )
    
    # ä¸»è¦æ“ä½œå‚æ•°
    parser.add_argument('--log-dir', '-d', type=str, help='nginxæ—¥å¿—æ ¹ç›®å½•')
    parser.add_argument('--date', type=str, help='å¤„ç†æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--mode', type=str, choices=['full', 'incremental'], 
                       default='incremental', help='å¤„ç†æ¨¡å¼ (default: incremental)')
    
    # çŠ¶æ€å’Œç®¡ç†å‚æ•°
    parser.add_argument('--status', action='store_true', help='æ˜¾ç¤ºå¤„ç†çŠ¶æ€')
    parser.add_argument('--reset-failed', action='store_true', help='é‡ç½®å¤±è´¥çš„æ–‡ä»¶')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--json', action='store_true', help='ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = NginxLogManager()
    
    # è§£ææ—¥æœŸ
    target_date = None
    if args.date:
        try:
            target_date = datetime.strptime(args.date, '%Y-%m-%d').date()
        except ValueError:
            print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {args.date}, åº”è¯¥æ˜¯ YYYY-MM-DD")
            sys.exit(1)
    
    # æ‰§è¡Œæ“ä½œ
    result = None
    
    if args.status:
        # æ˜¾ç¤ºçŠ¶æ€
        result = manager.show_status(target_date)
        
    elif args.reset_failed:
        # é‡ç½®å¤±è´¥æ–‡ä»¶
        result = manager.reset_failed(target_date)
        
    else:
        # å¤„ç†æ—¥å¿—
        if not args.log_dir:
            print("âŒ è¯·æŒ‡å®šæ—¥å¿—ç›®å½• --log-dir")
            parser.print_help()
            sys.exit(1)
            
        result = manager.process_logs(args.log_dir, target_date, args.mode)
    
    # è¾“å‡ºç»“æœ
    if args.json and result:
        print(json.dumps(result, ensure_ascii=False, indent=2, default=str))
    
    # é€€å‡ºç 
    if result and result.get('status') in ['success', 'completed', 'up_to_date']:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()